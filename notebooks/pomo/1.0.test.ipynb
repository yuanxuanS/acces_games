{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# POMO Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "\n",
        "from ncobench.data.dataset import TorchDictDataset\n",
        "\n",
        "from ncobench.envs.tsp import TSPEnv\n",
        "from ncobench.models.rl.reinforce import *\n",
        "from ncobench.models.components.am.context import env_context\n",
        "from ncobench.models.components.am.embeddings import env_init_embedding, env_dynamic_embedding\n",
        "from ncobench.models.components.am.encoder import GraphAttentionEncoder\n",
        "from ncobench.models.components.am.decoder import Decoder, decode_probs, PrecomputedCache, LogitAttention\n",
        "from ncobench.models.components.am.base import get_log_likelihood\n",
        "from ncobench.models.nn.attention import NativeFlashMHA, flash_attn_wrapper\n",
        "from ncobench.utils.lightning import get_lightning_device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Novelty compared to `AttentionModel`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pseudo-code of differences for training\n",
        "\n",
        "### Attention Model\n",
        "```python\n",
        "def train(policy network p_Œ∏, training set S, batch size B, significance Œ±):\n",
        "    for i in 1...B:\n",
        "        s_i = RandomInstance(S)\n",
        "        œÄ_i = SampleRollout(s_i, p_Œ∏)\n",
        "        vb = UpdateBaseline(s, œÄ)\n",
        "        ‚àáùêø = (1/B) * Œ£(L(œÄ_i|s_i) - b_i) * ‚àá_Œ∏ log(p_Œ∏(œÄ_i|s_i))\n",
        "        Œ∏ = GradientDescent(Œ∏, ‚àáùêø)\n",
        "        if OneSidedPairedTest(p_Œ∏, p_Œ∏^BL) < Œ±: # p_Œ∏ is better than p_Œ∏^BL\n",
        "            Œ∏^BL = Œ∏\n",
        "```\n",
        "\n",
        "### POMO\n",
        "```python\n",
        "def train(policy network p_Œ∏, training set S, batch size B, number of start nodes N):\n",
        "    for i in 1...B:\n",
        "        s_i = RandomInstance(S)\n",
        "        # New: select starting nodes, and rolout with them\n",
        "        Œ±_i1,...,Œ±_iN = SelectStartNodes(s_i)\n",
        "        œÄ_i1,...,œÄ_iN = SampleRollout(s_i, p_Œ∏, {Œ±_i,j})\n",
        "        vb = UpdateBaseline(s, œÄ)\n",
        "        # New: baseline is simply the average baseline\n",
        "        ‚àáùêø = (1/NB) * Œ£(L(œÄ_ij|s_i) - b_i) * ‚àá_Œ∏ log(p_Œ∏(œÄ_ij|s_i))\n",
        "        Œ∏ = GradientDescent(Œ∏, ‚àáùêø)\n",
        "```\n",
        "\n",
        "So the novelty is:\n",
        "1. We select a set of starting nodes for each instance and rollout with them\n",
        "2. Replace baseline with average baseline\n",
        "\n",
        "---\n",
        "\n",
        "## Other novelty\n",
        "1. Use `multi-greedy` decoding (e.g.) simply take the best out of the starting points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, env, embedding_dim, num_heads, num_pomo=20, **logit_attn_kwargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_heads = num_heads\n",
        "\n",
        "        assert embedding_dim % num_heads == 0\n",
        "\n",
        "        step_context_dim = 2 * embedding_dim  # Embedding of first and last node\n",
        "        self.context = env_context(self.env.name, {\"context_dim\": step_context_dim})\n",
        "        self.dynamic_embedding = env_dynamic_embedding(\n",
        "            self.env.name, {\"embedding_dim\": embedding_dim}\n",
        "        )\n",
        "\n",
        "        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n",
        "        self.project_node_embeddings = nn.Linear(\n",
        "            embedding_dim, 3 * embedding_dim, bias=False\n",
        "        )\n",
        "        self.project_fixed_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.project_step_context = nn.Linear(\n",
        "            step_context_dim, embedding_dim, bias=False\n",
        "        )\n",
        "\n",
        "        # MHA\n",
        "        self.logit_attention = LogitAttention(\n",
        "            embedding_dim, num_heads, **logit_attn_kwargs\n",
        "        )\n",
        "\n",
        "        # POMO\n",
        "        self.num_pomo = num_pomo\n",
        "\n",
        "    def forward(self, td, embeddings, decode_type=\"sampling\"):\n",
        "        outputs = []\n",
        "        actions = []\n",
        "\n",
        "        # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n",
        "        cached_embeds = self._precompute(embeddings)\n",
        "\n",
        "        # Here we suppose all the batch is done at the same time\n",
        "        while not td[\"done\"].any():  \n",
        "\n",
        "            # POMO: first action is decided via select_start_nodes\n",
        "            if next(td[\"i\"]) == 0:\n",
        "                action = self.select_start_nodes(td)\n",
        "                td.set(\"action\", action[:, None])\n",
        "                td = self.env.step(td)[\"next\"]\n",
        "                log_p = torch.zeros_like(action) # first log_p is 0\n",
        "\n",
        "            else:   \n",
        "                # Compute the logits for the next node\n",
        "                log_p, mask = self._get_log_p(cached_embeds, td)\n",
        "\n",
        "                # Select the indices of the next nodes in the sequences, result (batch_size) long\n",
        "                action = decode_probs(\n",
        "                    log_p.exp().squeeze(1), mask.squeeze(1), decode_type=decode_type\n",
        "                )\n",
        "\n",
        "            td.set(\"action\", action[:, None])\n",
        "            td = self.env.step(td)[\"next\"]\n",
        "\n",
        "\n",
        "            # Collect output of step\n",
        "            outputs.append(log_p.squeeze(1))\n",
        "            actions.append(action)\n",
        "\n",
        "        outputs, actions = torch.stack(outputs, 1), torch.stack(actions, 1)\n",
        "        td.set(\"reward\", self.env.get_reward(td[\"observation\"], actions))\n",
        "        return outputs, actions, td\n",
        "\n",
        "    def select_start_nodes(self, td):\n",
        "        \"\"\"Select POMO\"\"\"\n",
        "        batch_size = td[\"observation\"].shape[0]\n",
        "        num_pomo = self.num_pomo\n",
        "        # selected = torch.arange(pomo_size)[None, :].expand(batch_size, pomo_size)\n",
        "        selected = torch.arange(num_pomo)[None, :].expand(batch_size, num_pomo)\n",
        "        return selected\n",
        "\n",
        "    \n",
        "    def _precompute(self, embeddings):\n",
        "        # The fixed context projection of the graph embedding is calculated only once for efficiency\n",
        "        graph_embed = embeddings.mean(1)\n",
        "\n",
        "        # The projection of the node embeddings for the attention is calculated once up front\n",
        "        (\n",
        "            glimpse_key_fixed,\n",
        "            glimpse_val_fixed,\n",
        "            logit_key_fixed,\n",
        "        ) = self.project_node_embeddings(embeddings[:, None, :, :]).chunk(3, dim=-1)\n",
        "\n",
        "        # Organize in a TensorDict for easy access\n",
        "        cached_embeds = PrecomputedCache(\n",
        "            node_embeddings=embeddings,\n",
        "            graph_context=self.project_fixed_context(graph_embed)[:, None, :],\n",
        "            glimpse_key=self.logit_attention._make_heads(glimpse_key_fixed),\n",
        "            glimpse_val=self.logit_attention._make_heads(glimpse_val_fixed),\n",
        "            logit_key=logit_key_fixed,\n",
        "        )\n",
        "\n",
        "        return cached_embeds\n",
        "\n",
        "    def _get_log_p(self, cached, td):\n",
        "        context = self.context(cached.node_embeddings, td)\n",
        "        step_context = self.project_step_context(context)  # [batch, 1, embed_dim]\n",
        "\n",
        "        query = cached.graph_context + step_context  # [batch, 1, embed_dim]\n",
        "\n",
        "        # Compute keys and values for the nodes\n",
        "        # glimpse_K, glimpse_V, logit_K = self._get_attention_node_data(cached, td['observation'])\n",
        "        (\n",
        "            glimpse_key_dynamic,\n",
        "            glimpse_val_dynamic,\n",
        "            logit_key_dynamic,\n",
        "        ) = self.dynamic_embedding(td[\"observation\"])\n",
        "        glimpse_key = cached.glimpse_key + glimpse_key_dynamic\n",
        "        glimpse_key = cached.glimpse_val + glimpse_val_dynamic\n",
        "        logit_key = cached.logit_key + logit_key_dynamic\n",
        "\n",
        "        # Get the mask\n",
        "        mask = ~td[\"action_mask\"]\n",
        "\n",
        "        # Compute logits\n",
        "        log_p = self.logit_attention(query, glimpse_key, glimpse_key, logit_key, mask)\n",
        "\n",
        "        return log_p, mask"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test `POMOBase`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = TSPEnv(num_loc=10).transform()\n",
        "\n",
        "# data = env.gen_params(batch_size=[10000]) # NOTE: need to put batch_size in a list!!\n",
        "# init_td = env.reset(data)\n",
        "# env.batch_size = [10000]\n",
        "init_td = env.reset(batch_size=[10000])\n",
        "dataset = TorchDictDataset(init_td)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=128,\n",
        "                shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "                num_workers=0,\n",
        "                collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            )\n",
        "\n",
        "model = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    # force_flash_attn=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model)\n",
        "\n",
        "x = next(iter(dataloader)).to(\"cuda\")\n",
        "\n",
        "out = model(x, decode_type=\"sampling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lightning_device(lit_module: L.LightningModule) -> torch.device:\n",
        "    \"\"\"Get the device of the lightning module\n",
        "    See device setting issue in setup https://github.com/Lightning-AI/lightning/issues/2638\n",
        "    \"\"\"\n",
        "    if lit_module.trainer.strategy.root_device != lit_module.device:\n",
        "        return lit_module.trainer.strategy.root_device\n",
        "    return lit_module.device\n",
        "\n",
        "\n",
        "class AttentionModel(nn.Module):\n",
        "    def __init__(self, env, policy):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        # self.policy = instantiate(cfg.policy)\n",
        "        # self.baseline = instantiate(cfg.baseline) TODO\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str=\"train\", decode_type: str=None) -> TensorDict:\n",
        "        \"\"\"Evaluate model, get costs and log probabilities and compare with baseline\"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "        out_policy = self.policy(td)\n",
        "        bl_val, bl_loss = self.baseline.eval(td, -out_policy['reward'])\n",
        "\n",
        "        # print(bl_val, bl_loss)\n",
        "        # Calculate loss\n",
        "        advantage = -out_policy['reward'] - bl_val\n",
        "        reinforce_loss = (advantage * out_policy['log_likelihood']).mean()\n",
        "        loss = reinforce_loss + bl_loss\n",
        "\n",
        "        return {'loss': loss, 'reinforce_loss': reinforce_loss, 'bl_loss': bl_loss, 'bl_val': bl_val, **out_policy}\n",
        "    \n",
        "    def setup(self, lit_module):\n",
        "        # Make baseline taking model itself and train_dataloader from model as input\n",
        "        # TODO make this as taken from config\n",
        "        self.baseline = instantiate({\"_target_\": \"__main__.WarmupBaseline\",\n",
        "                                    \"baseline\": {\"_target_\": \"__main__.RolloutBaseline\",                                             }\n",
        "                                    })  \n",
        "\n",
        "        self.baseline.setup(self.policy, lit_module.val_dataloader(), self.env, device=get_lightning_device(lit_module))         \n",
        "        # self.baseline = NoBaseline()\n",
        "\n",
        "    def on_train_epoch_end(self, lit_module):\n",
        "        # self.baseline.epoch_callback(self.policy, self.env, pl_module)\n",
        "        self.baseline.epoch_callback(self.policy, lit_module.val_dataloader(), lit_module.current_epoch, self.env, device=get_lightning_device(lit_module))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCOLightningModule(L.LightningModule):\n",
        "    def __init__(self, env, model, lr=1e-4, batch_size=128, train_size=1000, val_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def setup(self, stage=\"fit\"):\n",
        "        self.train_dataset = self.get_observation_dataset(self.train_size)\n",
        "        self.val_dataset = self.get_observation_dataset(self.val_size)\n",
        "        if hasattr(self.model, \"setup\"):\n",
        "            self.model.setup(self)\n",
        "\n",
        "    def shared_step(self, batch: Any, batch_idx: int, phase: str):\n",
        "        td = self.env.reset(init_observation=batch)\n",
        "        output = self.model(td, phase)\n",
        "        \n",
        "        # output = self.model(batch, phase)\n",
        "        self.log(f\"{phase}/cost\", -output[\"reward\"].mean(), prog_bar=True)\n",
        "        return {\"loss\": output['loss']}\n",
        "\n",
        "    def training_step(self, batch: Any, batch_idx: int):   \n",
        "        return self.shared_step(batch, batch_idx, phase='train')\n",
        "\n",
        "    def validation_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='val')\n",
        "\n",
        "    def test_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        # optim = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "        # TODO: scheduler\n",
        "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_steps)\n",
        "        return [optim] #, [scheduler]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_dataset)\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if hasattr(self.model, \"on_train_epoch_end\"):\n",
        "            self.model.on_train_epoch_end(self)\n",
        "        self.train_dataset = self.get_observation_dataset(self.train_size) \n",
        "\n",
        "    # def get_observation_dataset(self, size):\n",
        "    #     # online data generation: we generate a new batch online\n",
        "    #     data = self.env.gen_params(batch_size=size)\n",
        "    #     return TorchDictDataset(self.env.reset(data))\n",
        "\n",
        "    def get_observation_dataset(self, size):\n",
        "        # online data generation: we generate a new batch online\n",
        "        # data = self.env.gen_params(batch_size=size)\n",
        "        return TorchDictDataset(self.env.reset(batch_size=[size])['observation'])\n",
        "       \n",
        "    def _dataloader(self, dataset):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "            num_workers=0,\n",
        "            collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            pin_memory=self.on_gpu,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable profiling executor. This reduces memory and increases speed.\n",
        "try:\n",
        "    torch._C._jit_set_profiling_executor(False)\n",
        "    torch._C._jit_set_profiling_mode(False)\n",
        "except AttributeError:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Flash Attention does not support masking, force_flash_attn will only be used for fp16\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "env = TSPEnv(num_loc=20).transform()\n",
        "\n",
        "# env = env.transform()\n",
        "policy = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    force_flash_attn=True,\n",
        ")\n",
        "\n",
        "\n",
        "model = instantiate({\"_target_\": \"__main__.AttentionModel\", \"env\": env}, policy=policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "ConfigAttributeError",
          "evalue": "Missing key strategy\n    full_key: strategy\n    object_type=dict",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConfigAttributeError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cfg\u001b[39m.\u001b[39;49mstrategy\n",
            "File \u001b[0;32m~/Dev/ncobench/env/lib/python3.9/site-packages/omegaconf/dictconfig.py:355\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_impl(\n\u001b[1;32m    352\u001b[0m         key\u001b[39m=\u001b[39mkey, default_value\u001b[39m=\u001b[39m_DEFAULT_MARKER_, validate_key\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m ConfigKeyError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_format_and_raise(\n\u001b[1;32m    356\u001b[0m         key\u001b[39m=\u001b[39;49mkey, value\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, cause\u001b[39m=\u001b[39;49me, type_override\u001b[39m=\u001b[39;49mConfigAttributeError\n\u001b[1;32m    357\u001b[0m     )\n\u001b[1;32m    358\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_and_raise(key\u001b[39m=\u001b[39mkey, value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cause\u001b[39m=\u001b[39me)\n",
            "File \u001b[0;32m~/Dev/ncobench/env/lib/python3.9/site-packages/omegaconf/base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[0;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_format_and_raise\u001b[39m(\n\u001b[1;32m    224\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    225\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     type_override: Any \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     format_and_raise(\n\u001b[1;32m    232\u001b[0m         node\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    233\u001b[0m         key\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m    234\u001b[0m         value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m    235\u001b[0m         msg\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(cause) \u001b[39mif\u001b[39;49;00m msg \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m msg,\n\u001b[1;32m    236\u001b[0m         cause\u001b[39m=\u001b[39;49mcause,\n\u001b[1;32m    237\u001b[0m         type_override\u001b[39m=\u001b[39;49mtype_override,\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/Dev/ncobench/env/lib/python3.9/site-packages/omegaconf/_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    896\u001b[0m     ex\u001b[39m.\u001b[39mref_type \u001b[39m=\u001b[39m ref_type\n\u001b[1;32m    897\u001b[0m     ex\u001b[39m.\u001b[39mref_type_str \u001b[39m=\u001b[39m ref_type_str\n\u001b[0;32m--> 899\u001b[0m _raise(ex, cause)\n",
            "File \u001b[0;32m~/Dev/ncobench/env/lib/python3.9/site-packages/omegaconf/_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m     ex\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[39mraise\u001b[39;00m ex\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n",
            "File \u001b[0;32m~/Dev/ncobench/env/lib/python3.9/site-packages/omegaconf/dictconfig.py:351\u001b[0m, in \u001b[0;36mDictConfig.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m()\n\u001b[1;32m    350\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_impl(\n\u001b[1;32m    352\u001b[0m         key\u001b[39m=\u001b[39;49mkey, default_value\u001b[39m=\u001b[39;49m_DEFAULT_MARKER_, validate_key\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m ConfigKeyError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    355\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_and_raise(\n\u001b[1;32m    356\u001b[0m         key\u001b[39m=\u001b[39mkey, value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cause\u001b[39m=\u001b[39me, type_override\u001b[39m=\u001b[39mConfigAttributeError\n\u001b[1;32m    357\u001b[0m     )\n",
            "File \u001b[0;32m~/Dev/ncobench/env/lib/python3.9/site-packages/omegaconf/dictconfig.py:442\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[0;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_impl\u001b[39m(\n\u001b[1;32m    439\u001b[0m     \u001b[39mself\u001b[39m, key: DictKeyType, default_value: Any, validate_key: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    440\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    441\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m         node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_child(\n\u001b[1;32m    443\u001b[0m             key\u001b[39m=\u001b[39;49mkey, throw_on_missing_key\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, validate_key\u001b[39m=\u001b[39;49mvalidate_key\n\u001b[1;32m    444\u001b[0m         )\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m (ConfigAttributeError, ConfigKeyError):\n\u001b[1;32m    446\u001b[0m         \u001b[39mif\u001b[39;00m default_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _DEFAULT_MARKER_:\n",
            "File \u001b[0;32m~/Dev/ncobench/env/lib/python3.9/site-packages/omegaconf/basecontainer.py:73\u001b[0m, in \u001b[0;36mBaseContainer._get_child\u001b[0;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_child\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     throw_on_missing_key: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Optional[Node], List[Optional[Node]]]:\n\u001b[1;32m     72\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Like _get_node, passing through to the nearest concrete Node.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     child \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_node(\n\u001b[1;32m     74\u001b[0m         key\u001b[39m=\u001b[39;49mkey,\n\u001b[1;32m     75\u001b[0m         validate_access\u001b[39m=\u001b[39;49mvalidate_access,\n\u001b[1;32m     76\u001b[0m         validate_key\u001b[39m=\u001b[39;49mvalidate_key,\n\u001b[1;32m     77\u001b[0m         throw_on_missing_value\u001b[39m=\u001b[39;49mthrow_on_missing_value,\n\u001b[1;32m     78\u001b[0m         throw_on_missing_key\u001b[39m=\u001b[39;49mthrow_on_missing_key,\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(child, UnionNode) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_special(child):\n\u001b[1;32m     81\u001b[0m         value \u001b[39m=\u001b[39m child\u001b[39m.\u001b[39m_value()\n",
            "File \u001b[0;32m~/Dev/ncobench/env/lib/python3.9/site-packages/omegaconf/dictconfig.py:480\u001b[0m, in \u001b[0;36mDictConfig._get_node\u001b[0;34m(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m     \u001b[39mif\u001b[39;00m throw_on_missing_key:\n\u001b[0;32m--> 480\u001b[0m         \u001b[39mraise\u001b[39;00m ConfigKeyError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing key \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!s}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[39melif\u001b[39;00m throw_on_missing_value \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39m_is_missing():\n\u001b[1;32m    482\u001b[0m     \u001b[39mraise\u001b[39;00m MissingMandatoryValue(\u001b[39m\"\u001b[39m\u001b[39mMissing mandatory value: $KEY\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mConfigAttributeError\u001b[0m: Missing key strategy\n    full_key: strategy\n    object_type=dict"
          ]
        }
      ],
      "source": [
        "cfg.strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lightning.strategies'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstrategies\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightning.strategies'"
          ]
        }
      ],
      "source": [
        "import lightning.strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m cfg \u001b[39m=\u001b[39m OmegaConf\u001b[39m.\u001b[39mcreate({\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mciao\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[1;32m      6\u001b[0m \u001b[39m# Add key to config\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m cfg\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Create omegaconf config\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "cfg = OmegaConf.create({\"model\": \"ciao\"})\n",
        "\n",
        "# Add key to config\n",
        "cfg.est\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "  rank_zero_warn(\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Flash Attention does not support masking, force_flash_attn will only be used for fp16\n",
            "Evaluating baseline model on evaluation dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | env   | TSPEnv         | 0     \n",
            "1 | model | AttentionModel | 1.4 M \n",
            "-----------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.681     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2306/2500 [02:23<00:12, 16.06it/s, v_num=177, train/cost=4.020]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "env = TSPEnv(num_loc=20).transform()\n",
        "\n",
        "# env = env.transform()\n",
        "policy = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    force_flash_attn=True,\n",
        ")\n",
        "\n",
        "model_final = AttentionModel(env, policy)\n",
        "\n",
        "# # TODO CHANGE THIS\n",
        "batch_size = 512 #1024 #512\n",
        "\n",
        "model = NCOLightningModule(env, model_final, batch_size=batch_size, train_size=1280000, lr=1e-4)\n",
        "\n",
        "# Trick to make calculations faster\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# Wandb Logger - we can use others as well as simply `None`\n",
        "# logger = pl.loggers.WandbLogger(project=\"torchrl\", name=\"pendulum\")\n",
        "# logger = L.loggers.CSVLogger(\"logs\", name=\"tsp\")\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "\n",
        "# from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
        "# callbacks = [DeviceStatsMonitor()]\n",
        "\n",
        "from lightning.pytorch.profilers import AdvancedProfiler\n",
        "\n",
        "profiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logsv2\")\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[1],\n",
        "    # callbacks=callbacks,\n",
        "    # profiler=profiler,\n",
        "    # strategy=\"deepspeed_stage_3_offload\",\n",
        "    precision=16,\n",
        "    log_every_n_steps=100,   \n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
