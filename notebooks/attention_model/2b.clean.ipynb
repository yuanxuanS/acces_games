{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention Model Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "\n",
        "from ncobench.envs.tsp import TSPEnv\n",
        "from ncobench.data.dataset import TensorDictDataset\n",
        "from ncobench.models.rl.reinforce import *\n",
        "from ncobench.models.co.am.context import env_context\n",
        "from ncobench.models.co.am.embeddings import env_init_embedding, env_dynamic_embedding\n",
        "from ncobench.models.co.am.encoder import GraphAttentionEncoder\n",
        "from ncobench.models.co.am.decoder import Decoder, decode_probs, PrecomputedCache, LogitAttention\n",
        "from ncobench.models.co.am.policy import get_log_likelihood\n",
        "from ncobench.models.nn.attention import NativeFlashMHA, flash_attn_wrapper\n",
        "from ncobench.utils.lightning import get_lightning_device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AttentionModelBase\n",
        "\n",
        "Here we declare the `AttentionModelBase`, which is the `nn.Module`:\n",
        "- Given initial states, it returns the solutions and rewards for them\n",
        "- We then wrap the main model with REINFORCE baselines and epoch callbacks to train it (full `AttentionModel`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModelBase(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 env: EnvBase,\n",
        "                 embedding_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 encoder: nn.Module = None,\n",
        "                 decoder: nn.Module = None,\n",
        "                 n_encode_layers: int = 3,\n",
        "                 normalization: str = 'batch',\n",
        "                 n_heads: int = 8,\n",
        "                 checkpoint_encoder: bool = False,\n",
        "                 mask_inner: bool = True,\n",
        "                 force_flash_attn: bool = False,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super(AttentionModelBase, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_encode_layers = n_encode_layers\n",
        "        self.env = env\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.checkpoint_encoder = checkpoint_encoder\n",
        "\n",
        "        self.init_embedding = env_init_embedding(self.env.name, {\"embedding_dim\": embedding_dim})\n",
        "\n",
        "        self.encoder = GraphAttentionEncoder(\n",
        "            num_heads=n_heads,\n",
        "            embed_dim=embedding_dim,\n",
        "            num_layers=self.n_encode_layers,\n",
        "            normalization=normalization,\n",
        "            force_flash_attn=force_flash_attn,\n",
        "        ) if encoder is None else encoder\n",
        "        \n",
        "        self.decoder = Decoder(env, embedding_dim, n_heads, mask_inner=mask_inner, force_flash_attn=force_flash_attn) if decoder is None else decoder\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str = \"train\", decode_type: str = \"sampling\") -> TensorDict:\n",
        "        \"\"\"Given observation, precompute embeddings and rollout\"\"\"\n",
        "\n",
        "        # Set decoding type for policy, can be also greedy\n",
        "        embedding = self.init_embedding(td)\n",
        "        encoded_inputs, _ = self.encoder(embedding)\n",
        "\n",
        "        # Main rollout\n",
        "        _log_p, actions, td = self.decoder(td, encoded_inputs, decode_type)\n",
        "        # reward = self.env.get_reward(td['observation'], actions)\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        ll = self._calc_log_likelihood(_log_p, actions, td.get('mask', None))\n",
        "        out = {\"reward\": td[\"reward\"], \"log_likelihood\": ll, \"actions\": actions}\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _calc_log_likelihood(self, _log_p, a, mask):\n",
        "\n",
        "        # Get log_p corresponding to selected actions\n",
        "        log_p = _log_p.gather(2, a.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Optional: mask out actions irrelevant to objective so they do not get reinforced\n",
        "        if mask is not None:\n",
        "            log_p[mask] = 0\n",
        "\n",
        "        assert (log_p > -1000).data.all(), \"Logprobs should not be -inf, check sampling procedure!\"\n",
        "\n",
        "        # Calculate log_likelihood\n",
        "        return log_p.sum(1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test `AttentionModelBase`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from rich.traceback import install\n",
        "# install()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ncobench.envs.dpp import DPPEnv\n",
        "\n",
        "# num_loc = 15\n",
        "# env = TSPEnv(num_loc=num_loc).transform()\n",
        "device = \"cpu\"\n",
        "\n",
        "base_folder = '../../'\n",
        "chip_fpath = base_folder+\"data/dpp/10x10_pkg_chip.npy\"\n",
        "decap_fpath = base_folder+\"data/dpp/01nF_decap.npy\"\n",
        "freq_fpath = base_folder+\"data/dpp/freq_201.npy\"\n",
        "\n",
        "env = DPPEnv(chip_fpath=chip_fpath, decap_fpath=decap_fpath, freq_fpath=freq_fpath)\n",
        "# env = TSPEnv(num_loc=15).transform()\n",
        "dataset = env.dataset(batch_size=[1000])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=32,\n",
        "                shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "                num_workers=0,\n",
        "                collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            )\n",
        "\n",
        "model = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    # num_pomo=num_loc,\n",
        "    # force_flash_attn=True,\n",
        ").to(device)\n",
        "\n",
        "# model = torch.compile(model)\n",
        "\n",
        "x = next(iter(dataloader)).to(device)\n",
        "x = env.reset(init_obs=x)\n",
        "\n",
        "\n",
        "out = model(x, decode_type=\"sampling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lightning_device(lit_module: L.LightningModule) -> torch.device:\n",
        "    \"\"\"Get the device of the lightning module\n",
        "    See device setting issue in setup https://github.com/Lightning-AI/lightning/issues/2638\n",
        "    \"\"\"\n",
        "    if lit_module.trainer.strategy.root_device != lit_module.device:\n",
        "        return lit_module.trainer.strategy.root_device\n",
        "    return lit_module.device\n",
        "\n",
        "\n",
        "class AttentionModel(nn.Module):\n",
        "    def __init__(self, env, policy):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        # self.policy = instantiate(cfg.policy)\n",
        "        # self.baseline = instantiate(cfg.baseline) TODO\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str=\"train\", decode_type: str=None) -> TensorDict:\n",
        "        \"\"\"Evaluate model, get costs and log probabilities and compare with baseline\"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "        out_policy = self.policy(td)\n",
        "        bl_val, bl_loss = self.baseline.eval(td, -out_policy['reward'])\n",
        "\n",
        "        # print(bl_val, bl_loss)\n",
        "        # Calculate loss\n",
        "        advantage = -out_policy['reward'] - bl_val\n",
        "        reinforce_loss = (advantage * out_policy['log_likelihood']).mean()\n",
        "        loss = reinforce_loss + bl_loss\n",
        "\n",
        "        return {'loss': loss, 'reinforce_loss': reinforce_loss, 'bl_loss': bl_loss, 'bl_val': bl_val, **out_policy}\n",
        "    \n",
        "    def setup(self, lit_module):\n",
        "        # Make baseline taking model itself and train_dataloader from model as input\n",
        "        # TODO make this as taken from config\n",
        "        self.baseline = instantiate({\"_target_\": \"__main__.WarmupBaseline\",\n",
        "                                    \"baseline\": {\"_target_\": \"__main__.RolloutBaseline\",                                             }\n",
        "                                    })  \n",
        "\n",
        "        self.baseline.setup(self.policy, lit_module.val_dataloader(), self.env, device=get_lightning_device(lit_module))         \n",
        "        # self.baseline = NoBaseline()\n",
        "\n",
        "    def on_train_epoch_end(self, lit_module):\n",
        "        # self.baseline.epoch_callback(self.policy, self.env, pl_module)\n",
        "        self.baseline.epoch_callback(self.policy, lit_module.val_dataloader(), lit_module.current_epoch, self.env, device=get_lightning_device(lit_module))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCOLightningModule(L.LightningModule):\n",
        "    def __init__(self, env, model, lr=1e-4, batch_size=128, train_size=1000, val_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def setup(self, stage=\"fit\"):\n",
        "        self.train_dataset = self.get_observation_dataset(self.train_size)\n",
        "        self.val_dataset = self.get_observation_dataset(self.val_size)\n",
        "        if hasattr(self.model, \"setup\"):\n",
        "            self.model.setup(self)\n",
        "\n",
        "    def shared_step(self, batch: Any, batch_idx: int, phase: str):\n",
        "        td = self.env.reset(init_obs=batch)\n",
        "        output = self.model(td, phase)\n",
        "        \n",
        "        # output = self.model(batch, phase)\n",
        "        self.log(f\"{phase}/cost\", -output[\"reward\"].mean(), prog_bar=True)\n",
        "        return {\"loss\": output['loss']}\n",
        "\n",
        "    def training_step(self, batch: Any, batch_idx: int):   \n",
        "        return self.shared_step(batch, batch_idx, phase='train')\n",
        "\n",
        "    def validation_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='val')\n",
        "\n",
        "    def test_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        # optim = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "        # TODO: scheduler\n",
        "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_steps)\n",
        "        return [optim] #, [scheduler]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_dataset)\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if hasattr(self.model, \"on_train_epoch_end\"):\n",
        "            self.model.on_train_epoch_end(self)\n",
        "        self.train_dataset = self.get_observation_dataset(self.train_size) \n",
        "\n",
        "    # def get_observation_dataset(self, size):\n",
        "    #     # online data generation: we generate a new batch online\n",
        "    #     data = self.env.gen_params(batch_size=size)\n",
        "    #     return TorchDictDataset(self.env.reset(data))\n",
        "\n",
        "    def get_observation_dataset(self, size):\n",
        "        # online data generation: we generate a new batch online\n",
        "        # data = self.env.gen_params(batch_size=size)\n",
        "        return TensorDictDataset(self.env.reset(batch_size=[size])['observation'])\n",
        "       \n",
        "    def _dataloader(self, dataset):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "            num_workers=0,\n",
        "            collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            pin_memory=self.on_gpu,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable profiling executor. This reduces memory and increases speed.\n",
        "try:\n",
        "    torch._C._jit_set_profiling_executor(False)\n",
        "    torch._C._jit_set_profiling_mode(False)\n",
        "except AttributeError:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Flash Attention does not support masking, force_flash_attn will only be used for fp16\n"
          ]
        }
      ],
      "source": [
        "env = TSPEnv(num_loc=20).transform()\n",
        "\n",
        "# env = env.transform()\n",
        "policy = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    force_flash_attn=True,\n",
        ")\n",
        "\n",
        "\n",
        "model = instantiate({\"_target_\": \"__main__.AttentionModel\", \"env\": env}, policy=policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Flash Attention does not support masking, force_flash_attn will only be used for fp16\n",
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/lightning/fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
            "  rank_zero_warn(\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | env   | TSPEnv         | 0     \n",
            "1 | model | AttentionModel | 1.4 M \n",
            "-----------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.681     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:  76%|███████▌  | 1898/2500 [01:56<00:37, 16.24it/s, v_num=179, train/cost=4.020]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "env = TSPEnv(num_loc=20).transform()\n",
        "\n",
        "# env = env.transform()\n",
        "policy = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    force_flash_attn=True,\n",
        ")\n",
        "\n",
        "model_final = AttentionModel(env, policy)\n",
        "\n",
        "# # TODO CHANGE THIS\n",
        "batch_size = 512 #1024 #512\n",
        "\n",
        "model = NCOLightningModule(env, model_final, batch_size=batch_size, train_size=1280000, lr=1e-4)\n",
        "\n",
        "# Trick to make calculations faster\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# Wandb Logger - we can use others as well as simply `None`\n",
        "# logger = pl.loggers.WandbLogger(project=\"torchrl\", name=\"pendulum\")\n",
        "# logger = L.loggers.CSVLogger(\"logs\", name=\"tsp\")\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "\n",
        "# from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
        "# callbacks = [DeviceStatsMonitor()]\n",
        "\n",
        "from lightning.pytorch.profilers import AdvancedProfiler\n",
        "\n",
        "profiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logsv2\")\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[1],\n",
        "    # callbacks=callbacks,\n",
        "    # profiler=profiler,\n",
        "    # strategy=\"deepspeed_stage_3_offload\",\n",
        "    precision=16,\n",
        "    log_every_n_steps=100,   \n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
