{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention Model Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # rich tracebacks\n",
        "# import rich\n",
        "# import rich.traceback\n",
        "\n",
        "# rich.traceback.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/torchrl/__init__.py:26: UserWarning: failed to set start method to spawn, and current start method for mp is fork.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "\n",
        "\n",
        "from ncobench.models.am import AttentionModel\n",
        "from ncobench.models.common.am_base import AttentionModelBase\n",
        "from ncobench.models.rl.reinforce import *\n",
        "# from ncobench.envs.tsp import TSPEnv\n",
        "# from ncobench.models.nn.graph import GraphAttentionEncoder\n",
        "from ncobench.models.nn.attention import CrossAttention\n",
        "\n",
        "from cleaner_v2.tsp_refactor import TSPEnv\n",
        "from ncobench.data.dataset import TorchDictDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from am_src.context import env_context\n",
        "from am_src.embeddings import env_init_embedding, env_dynamic_embedding\n",
        "from am_src.encoder import GraphAttentionEncoder"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AttentionModelBase\n",
        "\n",
        "Here we declare the `AttentionModelBase`, which is the `nn.Module`:\n",
        "- Given initial states, it returns the solutions and rewards for them\n",
        "- We then wrap the main model with REINFORCE baselines and epoch callbacks to train it (full `AttentionModel`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PrecomputedCache:\n",
        "    node_embeddings: torch.Tensor\n",
        "    graph_context: torch.Tensor\n",
        "    glimpse_key: torch.Tensor\n",
        "    glimpse_val: torch.Tensor\n",
        "    logit_key: torch.Tensor\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, env, embedding_dim, n_heads, **logit_attn_kwargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert embedding_dim % n_heads == 0\n",
        "\n",
        "        step_context_dim = 2 * embedding_dim  # Embedding of first and last node\n",
        "        self.context = env_context(self.env.name, {\"context_dim\": step_context_dim})\n",
        "        self.dynamic_embedding = env_dynamic_embedding(\n",
        "            self.env.name, {\"embedding_dim\": embedding_dim}\n",
        "        )\n",
        "        \n",
        "        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n",
        "        self.project_node_embeddings = nn.Linear(embedding_dim, 3 * embedding_dim, bias=False)\n",
        "        self.project_fixed_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.project_step_context = nn.Linear(step_context_dim, embedding_dim, bias=False)\n",
        "\n",
        "        # MHA\n",
        "        self.logit_attention = LogitAttention(embedding_dim, n_heads, **logit_attn_kwargs)\n",
        "        \n",
        "\n",
        "    def forward(self, td, embeddings, decode_type=\"sampling\"):\n",
        "\n",
        "        outputs = []\n",
        "        actions = []\n",
        "\n",
        "        # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n",
        "        cached_embeds = self._precompute(embeddings)\n",
        "\n",
        "        while not td[\"done\"].any(): # NOTE: here we suppose all the batch is done at the same time\n",
        "            \n",
        "            log_p, mask = self._get_log_p(cached_embeds, td)\n",
        "\n",
        "            # Select the indices of the next nodes in the sequences, result (batch_size) long\n",
        "            action = self.decode(log_p.exp().squeeze(1), mask.squeeze(1), decode_type=decode_type)\n",
        "           \n",
        "            td.set(\"action\", action[:, None])\n",
        "            td = self.env.step(td)['next']\n",
        "\n",
        "            # Collect output of step\n",
        "            outputs.append(log_p.squeeze(1))\n",
        "            actions.append(action)\n",
        "\n",
        "        outputs, actions = torch.stack(outputs, 1), torch.stack(actions, 1)\n",
        "        td.set(\"reward\", self.env.get_reward(td['observation'], actions))\n",
        "        return outputs, actions, td\n",
        "    \n",
        "    def decode(self, probs, mask, decode_type=\"sampling\"):\n",
        "\n",
        "        assert (probs == probs).all(), \"Probs should not contain any nans\"\n",
        "\n",
        "        if decode_type == \"greedy\":\n",
        "            _, selected = probs.max(1)\n",
        "            assert not mask.gather(1, selected.unsqueeze(\n",
        "                -1)).data.any(), \"Decode greedy: infeasible action has maximum probability\"\n",
        "\n",
        "        elif decode_type == \"sampling\":\n",
        "            selected = probs.multinomial(1).squeeze(1)\n",
        "\n",
        "            while mask.gather(1, selected.unsqueeze(-1)).data.any():\n",
        "                print('Sampled bad values, resampling!')\n",
        "                selected = probs.multinomial(1).squeeze(1)\n",
        "\n",
        "        else:\n",
        "            assert False, \"Unknown decode type\"\n",
        "        return selected\n",
        "    \n",
        "    def _precompute(self, embeddings):\n",
        "        # The fixed context projection of the graph embedding is calculated only once for efficiency\n",
        "        graph_embed = embeddings.mean(1)\n",
        "        \n",
        "        # The projection of the node embeddings for the attention is calculated once up front\n",
        "        glimpse_key_fixed, glimpse_val_fixed, logit_key_fixed = \\\n",
        "            self.project_node_embeddings(embeddings[:, None, :, :]).chunk(3, dim=-1)\n",
        "        \n",
        "        # Organize in a TensorDict for easy access\n",
        "        cached_embeds = PrecomputedCache(\n",
        "            node_embeddings=embeddings,\n",
        "            graph_context=self.project_fixed_context(graph_embed)[:, None, :],\n",
        "            glimpse_key=self.logit_attention._make_heads(glimpse_key_fixed),\n",
        "            glimpse_val=self.logit_attention._make_heads(glimpse_val_fixed),\n",
        "            logit_key=logit_key_fixed\n",
        "        )\n",
        "\n",
        "        return cached_embeds\n",
        "        \n",
        "    def _get_log_p(self, cached, td, normalize=True):\n",
        "        \n",
        "        context = self.context(cached.node_embeddings, td)\n",
        "        step_context = self.project_step_context(context)  # [batch, 1, embed_dim]\n",
        "\n",
        "        query = cached.graph_context + step_context  # [batch, 1, embed_dim]\n",
        "\n",
        "        # Compute keys and values for the nodes\n",
        "        # glimpse_K, glimpse_V, logit_K = self._get_attention_node_data(cached, td['observation'])\n",
        "        glimpse_key_dynamic, glimpse_val_dynamic, logit_key_dynamic = self.dynamic_embedding(td['observation'])\n",
        "        glimpse_key = cached.glimpse_key + glimpse_key_dynamic\n",
        "        glimpse_key = cached.glimpse_val + glimpse_val_dynamic\n",
        "        logit_key = cached.logit_key + logit_key_dynamic\n",
        "\n",
        "        # Compute the mask\n",
        "        mask = ~td['action_mask']\n",
        "\n",
        "        # Compute logits\n",
        "        log_p = self.logit_attention(query, glimpse_key, glimpse_key, logit_key, mask)\n",
        "\n",
        "        return log_p, mask   \n",
        "    \n",
        "\n",
        "cross_attention = CrossAttention()\n",
        "\n",
        "\n",
        "class LogitAttention(nn.Module):\n",
        "    \"\"\"Calculate logits given query, key and value and logit key \n",
        "    If we use Flash Attention, then we automatically move to fp16 for inner computations\n",
        "    Note: with Flash Attention, masking is not supported\n",
        "\n",
        "    Perform the following:\n",
        "        1. Apply cross attention to get the heads\n",
        "        2. Project heads to get glimpse\n",
        "        3. Compute attention score between glimpse and logit key\n",
        "        4. Normalize and mask\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, n_heads, tanh_clipping=10.0, mask_inner=True, mask_logits=True, normalize=True, force_flash_attn=False):\n",
        "        super(LogitAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.mask_logits = mask_logits\n",
        "        self.mask_inner = mask_inner\n",
        "        self.tanh_clipping = tanh_clipping\n",
        "        self.temp = 1.0\n",
        "        self.normalize = normalize\n",
        "        self.force_flash_attn = force_flash_attn\n",
        "\n",
        "        if force_flash_attn and mask_inner:\n",
        "            print(\"WARNING: Flash Attention does not support masking, setting force_flash_attn to False\")\n",
        "            self.force_flash_attn = False\n",
        "\n",
        "        # Projection - query, key, value already include projections\n",
        "        self.project_out = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "    def forward(self, query, key, value, logit_key, mask):\n",
        "\n",
        "        # Compute inner multi-head attention with no projections\n",
        "        heads = self._inner_mha(query, key, value, mask)\n",
        "        glimpse = self.project_out(heads)\n",
        "\n",
        "        # Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)\n",
        "        # bmm is slightly faster than einsum and matmul       \n",
        "        logits = torch.bmm(glimpse.squeeze(1), logit_key.squeeze(1).transpose(-2, -1))/ math.sqrt(glimpse.size(-1))\n",
        "\n",
        "        # From the logits compute the probabilities by clipping, masking and softmax\n",
        "        if self.tanh_clipping > 0:\n",
        "            logits = torch.tanh(logits) * self.tanh_clipping\n",
        "\n",
        "        if self.mask_logits:\n",
        "            logits[mask] = float('-inf')\n",
        "\n",
        "        if self.normalize:\n",
        "            logits = torch.log_softmax(logits / self.temp, dim=-1)\n",
        "\n",
        "        assert not torch.isnan(logits).any()\n",
        "\n",
        "        return logits\n",
        "    \n",
        "    def _inner_mha(self, query, key, value, mask):\n",
        "\n",
        "        # Flash Attention: move to fp16 for inner computations\n",
        "        if self.force_flash_attn:\n",
        "            src_dtype = query.dtype\n",
        "            query = rearrange(query, 'b 1 (h s) -> b h 1 s', h=self.n_heads)\n",
        "            query, key, value = query.half(), key.half(), value.half()\n",
        "            heads = F.scaled_dot_product_attention(query, key, value)\n",
        "            heads = rearrange(heads, 'b h 1 g -> b 1 1 (h g)', h=self.n_heads).to(src_dtype)\n",
        "\n",
        "        # Otherwise, get mask and use cross attention (faster than even original to converge)\n",
        "        else:\n",
        "            kv = torch.cat([key, value], dim=2)\n",
        "            q = rearrange(query, 'b 1 (h s) -> b 1 h s', h=self.n_heads)\n",
        "            key_padding_mask = ~mask.squeeze() if self.mask_inner else None\n",
        "            heads = cross_attention(q, kv, key_padding_mask=key_padding_mask)\n",
        "            heads = rearrange(heads, 'b 1 h g -> b 1 1 (h g)', h=self.n_heads)\n",
        "\n",
        "        #### NEW\n",
        "        # query = rearrange(query, 'b 1 (h s) -> b h 1 s', h=self.n_heads)\n",
        "        # src_dtype = query.dtype\n",
        "        # query, key, value = query.half(), key.half(), value.half()\n",
        "\n",
        "        # mask = ~mask.unsqueeze(1) if self.mask_inner else None\n",
        "\n",
        "        # heads = F.scaled_dot_product_attention(query, key, value, attn_mask=mask)\n",
        "        # heads = rearrange(heads, 'b h 1 g -> b 1 1 (h g)', h=self.n_heads).to(src_dtype)\n",
        "\n",
        "        return heads\n",
        "\n",
        "    def _make_heads(self, v):\n",
        "        if self.force_flash_attn:\n",
        "            v = rearrange(v, 'b 1 g (h s) -> b h g s', h=self.n_heads)\n",
        "        else:\n",
        "            v = rearrange(v, 'b 1 g (h s) -> b g 1 h s', h=self.n_heads)\n",
        "        return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 8, 1, 16])\n"
          ]
        }
      ],
      "source": [
        "# torch.Size([128, 1, 128])\n",
        "# torch.Size([128, 10, 1, 8, 16])\n",
        "# torch.Size([128, 10, 1, 8, 16])\n",
        "import torch.nn.functional as F\n",
        "\n",
        "query = torch.rand(128, 1, 128, dtype=torch.float16, device=\"cuda\")\n",
        "key = torch.rand(128, 10, 1, 8, 16, dtype=torch.float16, device=\"cuda\")\n",
        "value = torch.rand(128, 10, 1, 8, 16, dtype=torch.float16, device=\"cuda\")\n",
        "\n",
        "\n",
        "n_heads = 8\n",
        "query = rearrange(query, 'b 1 (h s) -> b h 1 s', h=n_heads)\n",
        "key = rearrange(key, 'b g 1 h s -> b h g s', h=n_heads)\n",
        "value = rearrange(value, 'b g 1 h s -> b h g s', h=n_heads)\n",
        "# mask \n",
        "mask = torch.rand(128, 1, 1, 10, dtype=torch.float16, device=\"cuda\") > 0.5\n",
        "\n",
        "heads = F.scaled_dot_product_attention(query, key, value, attn_mask=mask) #, attn_mask=mask)\n",
        "\n",
        "# heads = rearrange(heads, 'b h 1 g -> b 1 1 (h g)', h=n_heads)\n",
        "\n",
        "print(heads.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModelBase(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 env: EnvBase,\n",
        "                 embedding_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 *,\n",
        "                 n_encode_layers: int = 2,\n",
        "                 normalization: str = 'batch',\n",
        "                 n_heads: int = 8,\n",
        "                 checkpoint_encoder: bool = False,\n",
        "                 mask_inner: bool = True,\n",
        "                 force_flash_attn: bool = False,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super(AttentionModelBase, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_encode_layers = n_encode_layers\n",
        "        self.env = env\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.checkpoint_encoder = checkpoint_encoder\n",
        "\n",
        "        self.init_embedding = env_init_embedding(self.env.name, {\"embedding_dim\": embedding_dim})\n",
        "\n",
        "        self.encoder = GraphAttentionEncoder(\n",
        "            n_heads=n_heads,\n",
        "            embed_dim=embedding_dim,\n",
        "            n_layers=self.n_encode_layers,\n",
        "            normalization=normalization,\n",
        "            force_flash_attn=force_flash_attn,\n",
        "        )\n",
        "        \n",
        "        self.decoder = Decoder(env, embedding_dim, n_heads, mask_inner=mask_inner, force_flash_attn=force_flash_attn)\n",
        "\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str = \"train\", decode_type: str = \"sampling\") -> TensorDict:\n",
        "        \"\"\"Given observation, precompute embeddings and rollout\"\"\"\n",
        "\n",
        "        # Set decoding type for policy, can be also greedy\n",
        "        embedding = self.init_embedding(td)\n",
        "        encoded_inputs, _ = self.encoder(embedding)\n",
        "\n",
        "        # Main rollout\n",
        "        _log_p, actions, td = self.decoder(td, encoded_inputs, decode_type)\n",
        "        # reward = self.env.get_reward(td['observation'], actions)\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        ll = self._calc_log_likelihood(_log_p, actions, td.get('mask', None))\n",
        "        out = {\"reward\": td[\"reward\"], \"log_likelihood\": ll, \"actions\": actions}\n",
        "\n",
        "        return out\n",
        "\n",
        "    def _calc_log_likelihood(self, _log_p, a, mask):\n",
        "\n",
        "        # Get log_p corresponding to selected actions\n",
        "        log_p = _log_p.gather(2, a.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Optional: mask out actions irrelevant to objective so they do not get reinforced\n",
        "        if mask is not None:\n",
        "            log_p[mask] = 0\n",
        "\n",
        "        assert (log_p > -1000).data.all(), \"Logprobs should not be -inf, check sampling procedure!\"\n",
        "\n",
        "        # Calculate log_likelihood\n",
        "        return log_p.sum(1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test `AttentionModelBase`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = TSPEnv(num_loc=10).transform()\n",
        "\n",
        "# data = env.gen_params(batch_size=[10000]) # NOTE: need to put batch_size in a list!!\n",
        "# init_td = env.reset(data)\n",
        "# env.batch_size = [10000]\n",
        "init_td = env.reset(batch_size=[10000])\n",
        "dataset = TorchDictDataset(init_td)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=128,\n",
        "                shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "                num_workers=0,\n",
        "                collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            )\n",
        "\n",
        "model = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    # force_flash_attn=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model, backend=\"cuda\")\n",
        "\n",
        "x = next(iter(dataloader)).to(\"cuda\")\n",
        "\n",
        "out = model(x, decode_type=\"sampling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lightning_device(lit_module: L.LightningModule) -> torch.device:\n",
        "    \"\"\"Get the device of the lightning module\n",
        "    See device setting issue in setup https://github.com/Lightning-AI/lightning/issues/2638\n",
        "    \"\"\"\n",
        "    if lit_module.trainer.strategy.root_device != lit_module.device:\n",
        "        return lit_module.trainer.strategy.root_device\n",
        "    return lit_module.device\n",
        "\n",
        "\n",
        "class AttentionModel(nn.Module):\n",
        "    def __init__(self, env, policy):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        # self.policy = instantiate(cfg.policy)\n",
        "        # self.baseline = instantiate(cfg.baseline) TODO\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str=\"train\", decode_type: str=None) -> TensorDict:\n",
        "        \"\"\"Evaluate model, get costs and log probabilities and compare with baseline\"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "        out_policy = self.policy(td)\n",
        "        bl_val, bl_loss = self.baseline.eval(td, -out_policy['reward'])\n",
        "\n",
        "        # print(bl_val, bl_loss)\n",
        "        # Calculate loss\n",
        "        advantage = -out_policy['reward'] - bl_val\n",
        "        reinforce_loss = (advantage * out_policy['log_likelihood']).mean()\n",
        "        loss = reinforce_loss + bl_loss\n",
        "\n",
        "        return {'loss': loss, 'reinforce_loss': reinforce_loss, 'bl_loss': bl_loss, 'bl_val': bl_val, **out_policy}\n",
        "    \n",
        "    def setup(self, lit_module):\n",
        "        # Make baseline taking model itself and train_dataloader from model as input\n",
        "        # TODO make this as taken from config\n",
        "        self.baseline = instantiate({\"_target_\": \"__main__.WarmupBaseline\",\n",
        "                                    \"baseline\": {\"_target_\": \"__main__.RolloutBaseline\",                                             }\n",
        "                                    })  \n",
        "\n",
        "        self.baseline.setup(self.policy, lit_module.val_dataloader(), self.env, device=get_lightning_device(lit_module))         \n",
        "        # self.baseline = NoBaseline()\n",
        "\n",
        "    def on_train_epoch_end(self, lit_module):\n",
        "        # self.baseline.epoch_callback(self.policy, self.env, pl_module)\n",
        "        self.baseline.epoch_callback(self.policy, lit_module.val_dataloader(), lit_module.current_epoch, self.env, device=get_lightning_device(lit_module))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCOLightningModule(L.LightningModule):\n",
        "    def __init__(self, env, model, lr=1e-4, batch_size=128, train_size=1000, val_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def setup(self, stage=\"fit\"):\n",
        "        self.train_dataset = self.get_observation_dataset(self.train_size)\n",
        "        self.val_dataset = self.get_observation_dataset(self.val_size)\n",
        "        if hasattr(self.model, \"setup\"):\n",
        "            self.model.setup(self)\n",
        "\n",
        "    def shared_step(self, batch: Any, batch_idx: int, phase: str):\n",
        "        td = self.env.reset(init_observation=batch)\n",
        "        output = self.model(td, phase)\n",
        "        \n",
        "        # output = self.model(batch, phase)\n",
        "        self.log(f\"{phase}/cost\", -output[\"reward\"].mean(), prog_bar=True)\n",
        "        return {\"loss\": output['loss']}\n",
        "\n",
        "    def training_step(self, batch: Any, batch_idx: int):   \n",
        "        return self.shared_step(batch, batch_idx, phase='train')\n",
        "\n",
        "    def validation_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='val')\n",
        "\n",
        "    def test_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        # TODO: scheduler\n",
        "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_steps)\n",
        "        return [optim] #, [scheduler]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_dataset)\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if hasattr(self.model, \"on_train_epoch_end\"):\n",
        "            self.model.on_train_epoch_end(self)\n",
        "        self.train_dataset = self.get_observation_dataset(self.train_size) \n",
        "\n",
        "    # def get_observation_dataset(self, size):\n",
        "    #     # online data generation: we generate a new batch online\n",
        "    #     data = self.env.gen_params(batch_size=size)\n",
        "    #     return TorchDictDataset(self.env.reset(data))\n",
        "\n",
        "    def get_observation_dataset(self, size):\n",
        "        # online data generation: we generate a new batch online\n",
        "        # data = self.env.gen_params(batch_size=size)\n",
        "        return TorchDictDataset(self.env.reset(batch_size=[size])['observation'])\n",
        "       \n",
        "    def _dataloader(self, dataset):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "            num_workers=0,\n",
        "            collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            pin_memory=self.on_gpu,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Disable profiling executor. This reduces memory and increases speed.\n",
        "try:\n",
        "    torch._C._jit_set_profiling_executor(False)\n",
        "    torch._C._jit_set_profiling_mode(False)\n",
        "except AttributeError:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: Flash Attention does not support masking, setting force_flash_attn to False\n",
            "Evaluating baseline model on evaluation dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | env   | TSPEnv         | 0     \n",
            "1 | model | AttentionModel | 1.4 M \n",
            "-----------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.681     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:  16%|█▌        | 398/2500 [00:27<02:23, 14.67it/s, v_num=164, train/cost=4.120]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "env = TSPEnv(num_loc=20).transform()\n",
        "\n",
        "# env = env.transform()\n",
        "policy = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    force_flash_attn=True,\n",
        ")\n",
        "\n",
        "model_final = AttentionModel(env, policy)\n",
        "\n",
        "# # TODO CHANGE THIS\n",
        "batch_size = 512 #1024 #512\n",
        "\n",
        "model = NCOLightningModule(env, model_final, batch_size=batch_size, train_size=1280000, lr=1e-4)\n",
        "\n",
        "# Trick to make calculations faster\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# Wandb Logger - we can use others as well as simply `None`\n",
        "# logger = pl.loggers.WandbLogger(project=\"torchrl\", name=\"pendulum\")\n",
        "# logger = L.loggers.CSVLogger(\"logs\", name=\"tsp\")\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "# from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
        "# callbacks = [DeviceStatsMonitor()]\n",
        "\n",
        "from lightning.pytorch.profilers import AdvancedProfiler\n",
        "\n",
        "profiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logsv2\")\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[1],\n",
        "    # callbacks=callbacks,\n",
        "    # profiler=profiler,\n",
        "    # strategy=\"deepspeed_stage_3_offload\",\n",
        "    # precision=16,\n",
        "    log_every_n_steps=100,   \n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
