{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Env + Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "\n",
        "from rl4co.data.dataset import TensorDictDataset, TensorDictCollate\n",
        "from rl4co.models.rl.reinforce import *\n",
        "from rl4co.models.zoo.am.context import env_context\n",
        "from rl4co.models.zoo.am.embeddings import env_init_embedding, env_dynamic_embedding\n",
        "from rl4co.models.zoo.am.encoder import GraphAttentionEncoder\n",
        "from rl4co.models.zoo.am.decoder import Decoder, decode_probs, PrecomputedCache, LogitAttention\n",
        "from rl4co.models.zoo.am.policy import get_log_likelihood\n",
        "from rl4co.models.zoo.am import AttentionModel, AttentionModelPolicy\n",
        "from rl4co.models.nn.attention import NativeFlashMHA, flash_attn_wrapper\n",
        "from rl4co.utils.lightning import get_lightning_device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.rand(10000, 50, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 50, 2])\n"
          ]
        }
      ],
      "source": [
        "data = TensorDict({'a': a}, batch_size=a.shape[0])\n",
        "\n",
        "dataset = TensorDictDataset(data)\n",
        "dl = DataLoader(dataset, batch_size=512, shuffle=False, num_workers=0, collate_fn=TensorDictCollate())#, collate_fn=torch.stack)\n",
        "\n",
        "\n",
        "batch = next(iter(dl))\n",
        "print(batch['a'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.21 ms ± 2.85 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit for batch in dl: pass"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Old implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorDictDataset2(Dataset):\n",
        "    \"\"\"Simple dataset compatible with TensorDicts\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]  # note: use torch.stack to get batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 50, 2])\n"
          ]
        }
      ],
      "source": [
        "data = TensorDict({'a': a}, batch_size=a.shape[0])\n",
        "\n",
        "dataset = TensorDictDataset2(data)\n",
        "dl = DataLoader(dataset, batch_size=512, shuffle=False, num_workers=0, collate_fn=torch.stack)# collate_fn=TensorDictCollate())#, collate_fn=torch.stack)\n",
        "\n",
        "\n",
        "batch = next(iter(dl))\n",
        "print(batch['a'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65.1 ms ± 170 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit for batch in dl: pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([512, 50, 2])\n"
          ]
        }
      ],
      "source": [
        "data = TensorDict({'a': a}, batch_size=a.shape[0])#.memmap_()\n",
        "\n",
        "# split the batch inside the dictionary as list of dictionaries\n",
        "data = [{key: value[i] for key, value in data.items()} for i in range(data.shape[0])]\n",
        "\n",
        "\n",
        "dataset = TensorDictDataset2(data)\n",
        "\n",
        "dl = DataLoader(dataset, batch_size=512, shuffle=False, num_workers=0)#, collate_fn=#custom_collate)\n",
        "\n",
        "\n",
        "batch = next(iter(dl))\n",
        "print(batch['a'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.11 ms ± 3.49 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit for batch in dl: pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
