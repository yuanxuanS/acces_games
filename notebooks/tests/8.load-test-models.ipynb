{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/torchrl/__init__.py:26: UserWarning: failed to set start method to spawn, and current start method for mp is fork.\n",
      "  warn(\n",
      "/home/botu/botu/Dev/rl4co/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append(2*'../')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from omegaconf import DictConfig\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "from rl4co.tasks.rl4co import RL4COLitModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'_target_': 'rl4co.models.AttentionModel', 'baseline': {'_target_': 'rl4co.models.rl.reinforce.baselines.WarmupBaseline', 'baseline': {'_target_': 'rl4co.models.rl.reinforce.baselines.RolloutBaseline'}}, 'params': {'total': 708608, 'trainable': 708608, 'non_trainable': 0}}, 'data': {'batch_size': 512, 'train_size': 1280000, 'val_size': 10000}, 'trainer': {'_target_': 'lightning.pytorch.trainer.Trainer', 'default_root_dir': '/mnt/HDD/botu/botu/Dev/rl4co/logs/train/runs/tsp50/am-tsp50/2023-06-01_04-37-26', 'gradient_clip_val': 1.0, 'accelerator': 'gpu', 'precision': '16-mixed', 'strategy': {'_target_': 'lightning.pytorch.strategies.DDPStrategy', 'find_unused_parameters': True, 'gradient_as_bucket_view': True}, 'check_val_every_n_epoch': 1, 'deterministic': False, 'reload_dataloaders_every_n_epochs': 1, 'max_epochs': 100, 'devices': [1]}, 'callbacks': {'model_checkpoint': {'_target_': 'lightning.pytorch.callbacks.ModelCheckpoint', 'dirpath': '/mnt/HDD/botu/botu/Dev/rl4co/logs/train/runs/tsp50/am-tsp50/2023-06-01_04-37-26/checkpoints', 'filename': 'epoch_{epoch:03d}', 'monitor': 'val/reward', 'verbose': False, 'save_last': True, 'save_top_k': -1, 'mode': 'max', 'auto_insert_metric_name': False, 'save_weights_only': False, 'every_n_train_steps': None, 'train_time_interval': None, 'every_n_epochs': None, 'save_on_train_epoch_end': None}, 'model_summary': {'_target_': 'lightning.pytorch.callbacks.RichModelSummary', 'max_depth': 5}, 'rich_progress_bar': {'_target_': 'lightning.pytorch.callbacks.RichProgressBar'}, 'speed_monitor': {'_target_': 'rl4co.utils.callbacks.speed_monitor.SpeedMonitor', 'intra_step_time': True, 'inter_step_time': True, 'epoch_time': True}, 'learning_rate_monitor': {'_target_': 'lightning.pytorch.callbacks.LearningRateMonitor', 'logging_interval': 'epoch'}}, 'extras': {'ignore_warnings': False, 'enforce_tags': True, 'print_config': True}, 'task_name': None, 'tags': ['am', 'tsp'], 'ckpt_path': None, 'seed': 12345, 'mode': 'train', 'train': {'optimizer': {'_target_': 'torch.optim.Adam', 'lr': 0.0001, 'weight_decay': 0}, 'scheduler': {'_target_': 'torch.optim.lr_scheduler.MultiStepLR', 'milestones': [80, 95], 'gamma': 0.1}, 'scheduler_interval': 'epoch'}, 'test': True, 'compile': False, 'matmul_precision': 'medium', 'metrics': {'train': ['loss', 'reward'], 'val': ['reward'], 'test': ['reward'], 'log_on_step': True}, 'task': {'_target_': 'rl4co.tasks.rl4co.RL4COLitModule'}, 'paths': {'root_dir': '/mnt/HDD/botu/botu/Dev/rl4co', 'data_dir': '/mnt/HDD/botu/botu/Dev/rl4co/data/', 'log_dir': '/mnt/HDD/botu/botu/Dev/rl4co/logs/', 'output_dir': '/mnt/HDD/botu/botu/Dev/rl4co/logs/train/runs/tsp50/am-tsp50/2023-06-01_04-37-26', 'work_dir': '/mnt/HDD/botu/botu/Dev/rl4co'}, 'env': {'_target_': 'rl4co.envs.tsp.TSPEnv', 'num_loc': 50, 'min_loc': 0, 'max_loc': 1, 'data_dir': '/mnt/HDD/botu/botu/Dev/rl4co/data/tsp', 'val_file': 'tsp50_val_seed4321.npz', 'test_file': 'tsp50_test_seed1234.npz'}}\n"
     ]
    }
   ],
   "source": [
    "checkpoints_path = Path('../../saved_checkpoints/')\n",
    "\n",
    "exp_name = 'tsp50'\n",
    "model_name = 'am-tsp50'\n",
    "checkpoints_path = checkpoints_path / exp_name / model_name\n",
    "\n",
    "hydra_config_path = checkpoints_path / 'config.yaml'\n",
    "\n",
    "with open(hydra_config_path, 'r') as stream:\n",
    "    hydra_config_yaml = yaml.safe_load(stream)\n",
    "\n",
    "\n",
    "# for each key in hydr_config_yaml, replace / by .\n",
    "def clean_hydra_config(config, keep_value_only=True):\n",
    "    \"\"\"Clean hydra config by nesting dictionary and cleaning values\"\"\"\n",
    "    new_config = {}\n",
    "    # Iterate over config dictionary\n",
    "    for key, value in config.items():\n",
    "        # If key contains slash, split it and create nested dictionary recursively\n",
    "        if '/' in key:\n",
    "            keys = key.split('/')\n",
    "            d = new_config\n",
    "            for k in keys[:-1]:\n",
    "                d = d.setdefault(k, {})\n",
    "            d[keys[-1]] = value['value'] if keep_value_only else value\n",
    "        else:\n",
    "            new_config[key] = value['value'] if keep_value_only else value\n",
    "    return DictConfig(new_config)\n",
    "\n",
    "\n",
    "# Remove keys containing 'wandb' \n",
    "def remove_wandb_keys(config):\n",
    "    \"\"\"Remove keys containing 'wandb'\"\"\"\n",
    "    new_config = {}\n",
    "    for key, value in config.items():\n",
    "        if 'wandb' in key:\n",
    "            continue\n",
    "        else:\n",
    "            new_config[key] = value\n",
    "    return new_config\n",
    "\n",
    "hydra_config_yaml = remove_wandb_keys(hydra_config_yaml)\n",
    "\n",
    "hydra_config = clean_hydra_config(hydra_config_yaml)\n",
    "print(hydra_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: {'params': {'total': 708608, 'trainable': 708608, 'non_trainable': 0}}\n"
     ]
    }
   ],
   "source": [
    "lit_module = RL4COLitModule(hydra_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: {'params': {'total': 708608, 'trainable': 708608, 'non_trainable': 0}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No setup function for model required during testing!\n",
      "No wrap_dataset function for model required during testing!\n"
     ]
    }
   ],
   "source": [
    "lit_module = RL4COLitModule(hydra_config)\n",
    "\n",
    "\n",
    "# Remove setup function from lit_module.model if hasattr(lit_module.model, 'setup')\n",
    "if hasattr(lit_module.model, 'setup'):\n",
    "    print(\"No setup function for model required during testing!\")\n",
    "    lit_module.model.setup = lambda *args, **kwargs: None\n",
    "if hasattr(lit_module.model, \"wrap_dataset\"):\n",
    "    print(\"No wrap_dataset function for model required during testing!\")\n",
    "    lit_module.model.wrap_dataset = lambda *args, **kwargs: None\n",
    "\n",
    "# Load from checkpoint. We do not want to load the baseline weights, so we set strict=False\n",
    "# lit_module.load_from_checkpoint(checkpoints_path / 'epoch_099.ckpt', strict=False)\n",
    "\n",
    "def load_policy_state_dict(lit_module, path, device='cpu'):\n",
    "    state_dict = torch.load(path, map_location=device)['state_dict']\n",
    "    # get only policy parameters\n",
    "    policy_state_dict = {k: v for k, v in state_dict.items() if 'policy' in k}\n",
    "    # remove leading 'policy.' from keys\n",
    "    policy_state_dict = {k.replace('model.policy.', ''): v for k, v in policy_state_dict.items()}\n",
    "    \n",
    "    lit_module.model.policy.load_state_dict(policy_state_dict)\n",
    "    return lit_module\n",
    "\n",
    "\n",
    "# Generate few training data during setup for fast loading, since not needed\n",
    "lit_module.train_size = 100 \n",
    "lit_module = load_policy_state_dict(lit_module, checkpoints_path / 'epoch_099.ckpt')\n",
    "\n",
    "lit_module.setup('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "policy = lit_module.model.policy.to(device)\n",
    "policy.eval()\n",
    "env = lit_module.model.env\n",
    "\n",
    "# dataloader = lit_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.7785, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_dataset = lit_module.test_dataset\n",
    "dataloader = lit_module._dataloader(test_dataset, batch_size=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        td = batch.to(device).clone()\n",
    "        td = env.reset(td)\n",
    "        td_out = policy(td, decode_type=\"greedy\")\n",
    "        rewards.append(td_out['reward'])\n",
    "\n",
    "    rewards = torch.cat(rewards)\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy multi-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4co.utils.ops import unbatchify, batchify, gather_by_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.7668, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_dataset = lit_module.test_dataset\n",
    "dataloader = lit_module._dataloader(test_dataset, batch_size=2048)\n",
    "\n",
    "\n",
    "num_starts = env.num_loc\n",
    "# num_starts\n",
    "with torch.no_grad():\n",
    "\n",
    "    rewards_list = []\n",
    "    actions_list = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        td = batch.to(device).clone()\n",
    "        td = env.reset(td)\n",
    "        td_out = policy(td, decode_type=\"greedy_multistart\", \n",
    "                        num_starts=num_starts, return_actions=True)\n",
    "        \n",
    "        # [batch_size, num_starts]\n",
    "        rewards = unbatchify(td_out['reward'], num_starts)\n",
    "        actions = unbatchify(td_out['actions'], num_starts)\n",
    "\n",
    "        max_rewards, max_idxs = rewards.max(dim=1)\n",
    "        best_actions = gather_by_index(actions, max_idxs, dim=1)\n",
    "  \n",
    "        rewards_list.append(max_rewards)\n",
    "        actions_list.append(best_actions)\n",
    "\n",
    "    rewards = torch.cat(rewards_list)\n",
    "    actions = torch.cat(actions_list)\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4co.models.zoo.symnco.augmentations import StateAugmentation as StateAugmentationN\n",
    "from rl4co.models.zoo.pomo.augmentations import StateAugmentation as StateAugmentation8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.7185, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_dataset = lit_module.test_dataset\n",
    "dataloader = lit_module._dataloader(test_dataset, batch_size=128)\n",
    "\n",
    "# POMO\n",
    "num_augment = 8\n",
    "augmentation = StateAugmentation8(env.name, num_augment=num_augment)\n",
    "\n",
    "# SymNCO\n",
    "# num_augment = 8\n",
    "# augmentation = StateAugmentationN(env.name, num_augment=num_augment)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    rewards_list = []\n",
    "    actions_list = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        td = batch.to(device)\n",
    "        td_orig = td.clone()\n",
    "\n",
    "        td = augmentation(td)\n",
    "\n",
    "        td = env.reset(td).clone()\n",
    "        td_out = policy(td, return_actions=True)\n",
    "        \n",
    "        \n",
    "        rewards = env.get_reward(batchify(td_orig, num_augment), td_out['actions'])\n",
    "        rewards = unbatchify(rewards, num_augment)\n",
    "        actions = unbatchify(td_out['actions'], num_augment)\n",
    "        \n",
    "        max_rewards, max_idxs = rewards.max(dim=1)\n",
    "        best_actions = gather_by_index(actions, max_idxs, dim=1)\n",
    "  \n",
    "        rewards_list.append(max_rewards)\n",
    "        actions_list.append(best_actions)\n",
    "\n",
    "    rewards = torch.cat(rewards_list)\n",
    "    actions = torch.cat(actions_list)\n",
    "    print(rewards.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# get param device policy\n",
    "print(next(policy.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EvalBase:\n",
    "    name = \"base\"\n",
    "    def __init__(self, env, policy, **kwargs):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "\n",
    "    def __call__(self, dataloader, **kwargs):\n",
    "\n",
    "        # Collect timings for evaluation (more accurate than timeit)\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        start_event.record()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            rewards_list = []\n",
    "            actions_list = []\n",
    "\n",
    "            for batch in dataloader:\n",
    "                td = batch.to(next(policy.parameters()).device)\n",
    "                td = env.reset(td)\n",
    "                actions, rewards = self._inner(td, **kwargs)\n",
    "                rewards_list.append(rewards)\n",
    "                actions_list.append(actions)\n",
    "\n",
    "            rewards = torch.cat(rewards_list)\n",
    "            actions = torch.cat(actions_list)\n",
    "\n",
    "\n",
    "        end_event.record()\n",
    "        torch.cuda.synchronize()\n",
    "        inference_time = start_event.elapsed_time(end_event)\n",
    "        # inference_time = 0\n",
    "\n",
    "        print(\"Mean reward for {}: {:.4f}\".format(self.name, rewards.mean()))\n",
    "        print(\"Time: {:.4f}s\".format(inference_time/1000))\n",
    "    \n",
    "        return {\"actions\": actions, \"rewards\": rewards, \"inference_time\": inference_time, \n",
    "                \"name\": self.name, \"avg_reward\": rewards.mean()}\n",
    "\n",
    "    def _inner(self, td):\n",
    "        raise NotImplementedError(\"Implement in subclass\")\n",
    "\n",
    "\n",
    "class GreedyEval(EvalBase):\n",
    "    \"\"\"Evaluates the policy using greedy decoding and single trajectory\"\"\"\n",
    "\n",
    "    name = \"greedy\"\n",
    "    def __init__(self, env, policy):\n",
    "        super().__init__(env, policy)\n",
    "\n",
    "    def _inner(self, td):\n",
    "        td_out = self.policy(td.clone(), decode_type=\"greedy\", num_starts=0, return_actions=True)\n",
    "        rewards = self.env.get_reward(td, td_out['actions'])\n",
    "        return td_out['actions'], rewards\n",
    "\n",
    "\n",
    "class AugmentationEval(EvalBase):\n",
    "    \"\"\"Evaluates the policy via N state augmentations\"\"\"\n",
    "\n",
    "    name = \"augmentation\"\n",
    "    def __init__(self, env, policy, num_augment=8):\n",
    "        super().__init__(env, policy)\n",
    "        if num_augment == 8:\n",
    "            self.augmentation = StateAugmentation8(env.name, num_augment=num_augment)\n",
    "        else:\n",
    "            self.augmentation = StateAugmentationN(env.name, num_augment=num_augment, normalize=True)\n",
    "\n",
    "    def _inner(self, td, num_augment=None):\n",
    "        if num_augment is None:\n",
    "            num_augment = self.augmentation.num_augment\n",
    "        td_init = td.clone()\n",
    "        td = self.augmentation(td, num_augment=num_augment)\n",
    "        td_out = self.policy(td.clone(), decode_type=\"greedy\", num_starts=0, return_actions=True)\n",
    "        \n",
    "        rewards = self.env.get_reward(batchify(td_init, num_augment), td_out['actions'])\n",
    "        rewards = unbatchify(rewards, num_augment)\n",
    "        actions = unbatchify(td_out['actions'], num_augment)\n",
    "        \n",
    "        rewards, max_idxs = rewards.max(dim=1)\n",
    "        actions = gather_by_index(actions, max_idxs, dim=1)\n",
    "        return actions, rewards\n",
    "    \n",
    "\n",
    "class SamplingEval(EvalBase):\n",
    "    \"\"\"Evaluates the policy via N samples from the policy\"\"\"\n",
    "\n",
    "    name = \"sampling\"\n",
    "    def __init__(self, env, policy, samples, softmax_temp=None):\n",
    "        super().__init__(env, policy)\n",
    "        self.samples = samples\n",
    "        self.softmax_temp = softmax_temp\n",
    "\n",
    "    def _inner(self, td):\n",
    "        td = batchify(td, self.samples)\n",
    "        td_out = self.policy(td.clone(), decode_type=\"sampling\", \n",
    "                             num_starts=0, return_actions=True, \n",
    "                             softmax_temp=self.softmax_temp)\n",
    "        \n",
    "        rewards = self.env.get_reward(td, td_out['actions'])\n",
    "        rewards = unbatchify(rewards, self.samples)\n",
    "        actions = unbatchify(td_out['actions'], self.samples)\n",
    "        \n",
    "        rewards, max_idxs = rewards.max(dim=1)\n",
    "        actions = gather_by_index(actions, max_idxs, dim=1)\n",
    "        return actions, rewards\n",
    "\n",
    "\n",
    "class GreedyMultiStartEval(EvalBase):\n",
    "    \"\"\"Evaluates the policy via N samples from the policy\"\"\"\n",
    "\n",
    "    name = \"greedy_multistart\"\n",
    "    def __init__(self, env, policy, num_starts):\n",
    "        super().__init__(env, policy)\n",
    "        self.num_starts = num_starts\n",
    "\n",
    "    def _inner(self, td):\n",
    "        td_out = self.policy(td.clone(), decode_type=\"greedy_multistart\",\n",
    "                             num_starts=self.num_starts, return_actions=True)\n",
    "        \n",
    "        rewards = self.env.get_reward(td, td_out['actions'])\n",
    "        rewards = unbatchify(rewards, self.num_starts)\n",
    "        actions = unbatchify(td_out['actions'], self.num_starts)\n",
    "        \n",
    "        rewards, max_idxs = rewards.max(dim=1)\n",
    "        actions = gather_by_index(actions, max_idxs, dim=1)\n",
    "        return actions, rewards\n",
    "    \n",
    "\n",
    "class GreedyMultiStartAugmentEval(EvalBase):\n",
    "    \"\"\"Evaluates the policy via N samples from the policy\"\"\"\n",
    "\n",
    "    name = \"greedy_multistart_augment\"\n",
    "    def __init__(self, env, policy, num_starts, num_augment=8):\n",
    "        super().__init__(env, policy)\n",
    "        self.num_starts = num_starts\n",
    "        if num_augment == 8:\n",
    "            self.augmentation = StateAugmentation8(env.name, num_augment=num_augment)\n",
    "        else:\n",
    "            self.augmentation = StateAugmentationN(env.name, num_augment=num_augment)\n",
    "            \n",
    "    def _inner(self, td, num_augment=None):\n",
    "        if num_augment is None:\n",
    "            num_augment = self.augmentation.num_augment\n",
    "        \n",
    "        td_init = td.clone()\n",
    "\n",
    "        td = self.augmentation(td, num_augment=num_augment)\n",
    "        td_out = self.policy(td.clone(), decode_type=\"greedy_multistart\",\n",
    "                             num_starts=self.num_starts, return_actions=True)\n",
    "        \n",
    "        td = batchify(td_init, (num_augment, self.num_starts))\n",
    "\n",
    "        rewards = self.env.get_reward(td, td_out['actions'])\n",
    "        rewards = unbatchify(rewards, self.num_starts * num_augment)\n",
    "        actions = unbatchify(td_out['actions'], self.num_starts * num_augment)\n",
    "        \n",
    "        rewards, max_idxs = rewards.max(dim=1)\n",
    "        actions = gather_by_index(actions, max_idxs, dim=1)\n",
    "        return actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for greedy: -5.7785\n",
      "Time: 0.6495s\n"
     ]
    }
   ],
   "source": [
    "test_dataset = lit_module.test_dataset\n",
    "dataloader = lit_module._dataloader(test_dataset, batch_size=2048)\n",
    "\n",
    "# POMO\n",
    "num_augment = 8\n",
    "augmentation = StateAugmentation8(env.name, num_augment=num_augment)\n",
    "\n",
    "# SymNCO\n",
    "# num_augment = 8\n",
    "# augmentation = StateAugmentationN(env.name, num_augment=num_augment)\n",
    "\n",
    "\n",
    "greedy_eval = GreedyEval(env, policy)\n",
    "\n",
    "ret_vals_greedy = greedy_eval(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for greedy_multistart_augment: -5.7022\n",
      "Time: 115.0232s\n"
     ]
    }
   ],
   "source": [
    "test_dataset = lit_module.test_dataset\n",
    "dataloader = lit_module._dataloader(test_dataset, batch_size=16)\n",
    "\n",
    "num_augment = 50\n",
    "\n",
    "greedy_eval = GreedyMultiStartAugmentEval(env, policy, num_starts=env.num_loc, num_augment=num_augment)\n",
    "\n",
    "ret_vals_greedy = greedy_eval(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # POMO 8\n",
    "# 5.7184\n",
    "\n",
    "# # 8\n",
    "# 5.7242\n",
    "# 5.7244\n",
    "\n",
    "# # 20\n",
    "# 5.7120\n",
    "# 5.7118\n",
    "\n",
    "# # 50\n",
    "# 5.7050\n",
    "# 5.7050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions.gather(dim=1, index=max_idxs[:, None])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
