{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_neurons: List[int] = [64, 32],\n",
    "                 hidden_act: str = 'ReLU',\n",
    "                 out_act: str = 'Identity',\n",
    "                 input_norm: str = 'None',\n",
    "                 output_norm: str = 'None'):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        assert input_norm in ['Batch', 'Layer', 'None']\n",
    "        assert output_norm in ['Batch', 'Layer', 'None']\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_neurons = num_neurons\n",
    "        self.hidden_act = getattr(nn, hidden_act)()\n",
    "        self.out_act = getattr(nn, out_act)()\n",
    "\n",
    "        input_dims = [input_dim] + num_neurons\n",
    "        output_dims = num_neurons + [output_dim]\n",
    "\n",
    "        self.lins = nn.ModuleList()\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
    "            self.lins.append(nn.Linear(in_dim, out_dim))\n",
    "\n",
    "        self.input_norm = self._get_norm_layer(input_norm, input_dim)\n",
    "        self.output_norm = self._get_norm_layer(output_norm, output_dim)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.input_norm(xs)\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            xs = lin(xs)\n",
    "            xs = self.hidden_act(xs)\n",
    "        xs = self.lins[-1](xs)\n",
    "        xs = self.out_act(xs)\n",
    "        xs = self.output_norm(xs)\n",
    "        return xs\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_norm_layer(norm_method, dim):\n",
    "        if norm_method == 'Batch':\n",
    "            in_norm = nn.BatchNorm1d(dim)\n",
    "        elif norm_method == 'Layer':\n",
    "            in_norm = nn.LayerNorm(dim)\n",
    "        elif norm_method == 'None':\n",
    "            in_norm = nn.Identity()  # kinda placeholder\n",
    "        else:\n",
    "            raise RuntimeError(\"Not implemented normalization layer type {}\".format(norm_method))\n",
    "        return in_norm\n",
    "\n",
    "    def _get_act(self, is_last):\n",
    "        return self.out_act if is_last else self.hidden_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class INLayer(MessagePassing):\n",
    "\n",
    "    def __init__(self,\n",
    "                 edge_indim: int,\n",
    "                 edge_outdim: int,\n",
    "                 node_indim: int,\n",
    "                 node_outdim: int,\n",
    "                 node_aggregator: str = 'add',\n",
    "                 residual: bool = True,\n",
    "                 **mlp_params):\n",
    "        super(INLayer, self).__init__(aggr=node_aggregator)\n",
    "\n",
    "        self.edge_model = MLP(input_dim=edge_indim + 2 * node_indim,\n",
    "                              output_dim=edge_outdim,\n",
    "                              **mlp_params)\n",
    "        self.node_model = MLP(input_dim=edge_outdim + node_indim,\n",
    "                              output_dim=node_outdim,\n",
    "                              **mlp_params)\n",
    "\n",
    "        self.residual = residual\n",
    "\n",
    "    def forward(self,\n",
    "                nf: torch.tensor,\n",
    "                ef: torch.tensor,\n",
    "                edge_idx: torch.tensor):\n",
    "        nf_residual, ef_residual = nf, ef\n",
    "        uef = self.edge_update(nf, ef, edge_idx)\n",
    "        unf = self.propagate(edge_index=edge_idx, x=nf, edge_features=uef)\n",
    "        if self.residual:\n",
    "            unf, uef = unf + nf_residual, uef + ef_residual\n",
    "        return unf, uef\n",
    "\n",
    "    def edge_update(self, nf, ef, edge_index):\n",
    "        row, col = edge_index\n",
    "        x_i, x_j = nf[row], nf[col]\n",
    "        uef = self.edge_model(torch.cat([x_i, x_j, ef], dim=-1))\n",
    "        return uef\n",
    "\n",
    "    def message(self, edge_features: torch.tensor):\n",
    "        return edge_features\n",
    "\n",
    "    def update(self,\n",
    "               aggr_msg: torch.tensor,\n",
    "               x: torch.tensor):\n",
    "        unf = self.node_model(torch.cat([x, aggr_msg], dim=-1))\n",
    "        return unf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8237,  1.2862,  0.2866,  0.5154, -1.2957],\n",
      "        [ 1.3282, -0.0202, -1.6885, -0.0882,  1.2830],\n",
      "        [-0.1396,  2.2594, -1.4746,  1.2366,  1.2765]]) tensor([[ 1.0289, -0.6426,  0.2979, -0.6827, -0.1849,  2.2004, -0.5025],\n",
      "        [ 0.6252, -0.2165,  1.1382, -1.0760, -0.4326, -2.5266,  0.4149],\n",
      "        [-1.1183, -0.4361, -0.9389, -0.8873, -0.9246,  0.1235, -0.3848],\n",
      "        [-0.7542, -0.9807, -1.0409, -0.1864, -1.1971,  0.6327, -1.5552]])\n",
      "tensor([[ 0.9691,  1.2410,  0.1860,  0.6028, -1.4554],\n",
      "        [ 1.4674, -0.0973, -1.8262,  0.0207,  1.0966],\n",
      "        [ 0.0510,  2.2055, -1.7267,  1.4641,  1.0188]], grad_fn=<AddBackward0>) tensor([[ 1.3906, -0.5732,  0.1154, -0.7796, -0.0753,  2.1897, -0.6814],\n",
      "        [ 0.7467, -0.1715,  1.1247, -0.9585, -0.3087, -2.5348,  0.2288],\n",
      "        [-0.9195, -0.4060, -1.0766, -0.9250, -0.8202,  0.0387, -0.5024],\n",
      "        [-0.6658, -0.8893, -1.0313, -0.1558, -0.9605,  0.6137, -1.6716]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                            [1, 0, 2, 1]], dtype=torch.long)\n",
    "\n",
    "n_nodes, n_edges = 3, 4\n",
    "\n",
    "nf = torch.randn(n_nodes, 5)\n",
    "ef = torch.randn(n_edges, 7)\n",
    "\n",
    "layer = INLayer(7, 7, 5, 5)\n",
    "unf, uef = layer(nf, ef, edge_index)\n",
    "print(nf, ef)\n",
    "print(unf, uef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[96, 5], edge_index=[2, 128], edge_attr=[128, 7], num_nodes=96, batch=[96], ptr=[33])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbhua/miniconda3/envs/rl4co/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "nf = torch.randn(1024, n_nodes, 5)\n",
    "ef = torch.randn(1024, n_edges, 7)\n",
    "temp = DataLoader([Data(x=x, edge_index=edge_index, edge_attr=edge_attr, num_nodes=n_nodes) for x, edge_attr in zip(nf, ef)], batch_size=32)\n",
    "data = next(iter(temp))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[640, 2], edge_index=[2, 128], edge_attr=[12160, 1], batch=[640], ptr=[33])\n",
      "torch.Size([640, 2])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([12160, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Batch\n",
    "nf = torch.randn(32, 20, 2)\n",
    "ef = torch.randn(32, 380, 1)\n",
    "data_list = [Data(x=x, edge_index=edge_index, edge_attr=edge_attr) for x, edge_attr in zip(nf, ef)]\n",
    "data_batch = Batch.from_data_list(data_list)\n",
    "print(data_batch)\n",
    "print(data_batch.x.shape)\n",
    "print(data_batch.edge_index.shape)\n",
    "print(data_batch.edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "print(edge_index.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 5]) torch.Size([128, 7])\n"
     ]
    }
   ],
   "source": [
    "layer = INLayer(7, 7, 5, 5)\n",
    "unf, uef = layer(data.x, data.edge_attr, data.edge_index)\n",
    "print(unf.size(), uef.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([380, 2])\n"
     ]
    }
   ],
   "source": [
    "adj_matrix = torch.ones(20, 20)\n",
    "adj_matrix.fill_diagonal_(0) # No self-loops\n",
    "edge_index = torch.nonzero(adj_matrix)\n",
    "print(edge_index.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 380])\n"
     ]
    }
   ],
   "source": [
    "node_feature = torch.randn((32, 20, 2))\n",
    "edge_feature = torch.norm(node_feature[:, edge_index[:, 0], :] - node_feature[:, edge_index[:, 1], :], dim=-1, keepdim=True)\n",
    "print(edge_feature.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4142]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(torch.ones((1, 2)) - torch.zeros((1, 2)), p=2, dim=-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32])\n",
      "torch.Size([32, 32, 20, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((32, 32, 20, 2))\n",
    "print(a.size()[:2])\n",
    "batch_size = a.size()[:-2]\n",
    "shape_size = a.size()[-2:]\n",
    "b = torch.rand((32 * 32, 20, 2))\n",
    "print(b.view(*batch_size, *shape_size).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl4co",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
