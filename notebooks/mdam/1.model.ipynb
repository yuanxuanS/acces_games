{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbhua/miniconda3/envs/rl4co/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.append('../../')\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from tensordict import TensorDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torchrl.envs import EnvBase\n",
    "\n",
    "from rl4co.envs import TSPEnv\n",
    "from rl4co.data.dataset import TensorDictCollate\n",
    "from rl4co.models.nn.env_context import env_context\n",
    "from rl4co.models.nn.env_embedding import env_init_embedding\n",
    "from rl4co.models.nn.attention import LogitAttention\n",
    "from rl4co.models.nn.utils import decode_probs, get_log_likelihood\n",
    "from rl4co.models.zoo.mdam.encoder import GraphAttentionEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***1. Create environment***\n",
    "\n",
    "Test on the TSP environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([5, 20]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_mask: Tensor(shape=torch.Size([5, 20]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        current_node: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        first_node: Tensor(shape=torch.Size([5]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        i: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        locs: Tensor(shape=torch.Size([5, 20, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        reward: Tensor(shape=torch.Size([5, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "    batch_size=torch.Size([5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "def random_policy(td):\n",
    "    \"\"\"Helper function to select a random action from available actions\"\"\"\n",
    "    action = torch.multinomial(td[\"action_mask\"].float(), 1).squeeze(-1)\n",
    "    td.set(\"action\", action)\n",
    "    return td\n",
    "\n",
    "\n",
    "def rollout(env, td, policy):\n",
    "    \"\"\"Helper function to rollout a policy\"\"\"\n",
    "    actions = []\n",
    "    while not td[\"done\"].all():\n",
    "        td = policy(td)\n",
    "        actions.append(td[\"action\"])\n",
    "        td = env.step(td)[\"next\"]\n",
    "    actions = torch.stack(actions, dim=1)\n",
    "    td.set(\"action\", actions)\n",
    "    return td\n",
    "\n",
    "env = TSPEnv()\n",
    "\n",
    "td = env.reset(batch_size=[5])\n",
    "td = rollout(env, td, random_policy)\n",
    "print(td)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***2. Encoder***\n",
    "\n",
    "In MDAM model, the encoder is different with our `GraphAttentionEncoder` which is shared with model models. Now I just ues the MDAM's model and make it works, later we can reconstruct it. \n",
    "\n",
    "Since the code is pretty long, I put it in the `rl4co/models/zoo/mdam/encoder.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***3. Decoder***\n",
    "\n",
    "For now the decoder is inside the policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***4. Policy***\n",
    "\n",
    "This is the whole model combines the encoder and decoder. I didn't modify this policy a lot. For now, let's keep the decoder inside the policy.\n",
    "\n",
    "Discussion points:\n",
    "- Different with other model, the MDAM requiresl a series of tensordict input. Do this inside the model or outside the model\n",
    "- Normalize for the function `get_the_log_p`;\n",
    "\n",
    "TODO list:\n",
    "- [x] init embedding for the MDAM model -> [DONE]: checked it's fine to use ours;\n",
    "- [ ] split the decoder and the policy, the hard part is the decoder will call the encoder, so this may be a problem; actually not, the embedding in the decoder is used repeatly, so theoritically we don't need to call the encoder inside the decoder;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PrecomputedCache:\n",
    "    node_embeddings: torch.Tensor\n",
    "    graph_context: torch.Tensor\n",
    "    glimpse_key: torch.Tensor\n",
    "    glimpse_val: torch.Tensor\n",
    "    logit_key: torch.Tensor\n",
    "\n",
    "class AttentionModelPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: EnvBase,\n",
    "        encoder: nn.Module = None,\n",
    "        decoder: nn.Module = None,\n",
    "        embedding_dim: int = 128,\n",
    "        num_encode_layers: int = 3,\n",
    "        num_heads: int = 8,\n",
    "        num_paths: int = 5,\n",
    "        eg_step_gap: int = 200,\n",
    "        normalization: str = \"batch\",\n",
    "        mask_inner: bool = True,\n",
    "        force_flash_attn: bool = False,\n",
    "        train_decode_type: str = \"sampling\",\n",
    "        val_decode_type: str = \"greedy\",\n",
    "        test_decode_type: str = \"greedy\",\n",
    "        **unused_kw\n",
    "    ):\n",
    "        super(AttentionModelPolicy, self).__init__()\n",
    "        if len(unused_kw) > 0: print(f\"Unused kwargs: {unused_kw}\")\n",
    "\n",
    "        self.env = env\n",
    "        self.init_embedding = env_init_embedding(\n",
    "            self.env.name, {\"embedding_dim\": embedding_dim}\n",
    "        )\n",
    "\n",
    "        self.encoder = (\n",
    "            GraphAttentionEncoder(\n",
    "                num_heads=num_heads,\n",
    "                embed_dim=embedding_dim,\n",
    "                num_layers=num_encode_layers,\n",
    "                normalization=normalization,\n",
    "                force_flash_attn=force_flash_attn,\n",
    "            )\n",
    "            if encoder is None\n",
    "            else encoder\n",
    "        )\n",
    "\n",
    "        self.train_decode_type = train_decode_type\n",
    "        self.val_decode_type = val_decode_type\n",
    "        self.test_decode_type = test_decode_type\n",
    "        \n",
    "        # [Decoder]\n",
    "        self.context = [\n",
    "            env_context(self.env.name, {\"embedding_dim\": embedding_dim}) for _ in range(self.num_path)\n",
    "        ]\n",
    "        self.logit_attention = LogitAttention(\n",
    "            embedding_dim, \n",
    "            num_heads, \n",
    "            mask_inner=mask_inner,\n",
    "            force_flash_attn=force_flash_attn,\n",
    "        )\n",
    "\n",
    "        # TODO: other features\n",
    "        self.num_path = num_paths\n",
    "        self.eg_step_gap = eg_step_gap\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        td: TensorDict,\n",
    "        phase: str = \"train\",\n",
    "        return_actions: bool = False,\n",
    "        **decoder_kwargs,\n",
    "    ) -> TensorDict:\n",
    "        # SECTION: Encode and get embeddings\n",
    "        embedding = self.init_embedding(td)\n",
    "        encoded_inputs, _, attn, V, h_old = self.encoder(embedding)\n",
    "\n",
    "        # Get decode type depending on phase\n",
    "        if decoder_kwargs.get(\"decode_type\", None) is None:\n",
    "            decoder_kwargs[\"decode_type\"] = getattr(self, f\"{phase}_decode_type\")\n",
    "\n",
    "        # SECTION: Decoder first step: calculate for the decoder divergence loss\n",
    "        # Cost list and log likelihood list along with path\n",
    "        output_list = []\n",
    "        td_list = [self.env.reset(td) for i in range(self.num_paths)]\n",
    "        for i in range(self.num_paths):  \n",
    "            # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n",
    "            fixed = self._precompute(encoded_inputs, path_index=i)\n",
    "            log_p, _ = self._get_log_p(fixed, td_list[i], i)\n",
    "\n",
    "            # Collect output of step\n",
    "            output_list.append(log_p[:, 0, :]) # TODO: for vrp, ignore the first one (depot)\n",
    "            output_list[-1] = torch.max(output_list[-1], torch.ones(output_list[-1].shape, dtype=output_list[-1].dtype, device=outputs[-1].device) * (-1e9)) # for the kl loss\n",
    "\n",
    "        if self.num_paths > 1: # TODO: add a check for the baseline\n",
    "            kl_divergences = []\n",
    "            for _i in range(self.num_paths):\n",
    "                for _j in range(self.num_paths):\n",
    "                    if _i==_j:\n",
    "                        continue\n",
    "                    kl_divergence = torch.sum(torch.exp(output_list[_i]) * (output_list[_i] - output_list[_j]), -1)\n",
    "                    kl_divergences.append(kl_divergence)\n",
    "            loss_kl_divergence = torch.stack(kl_divergences, 0).mean()\n",
    "\n",
    "        # SECTION: Decoder rest step: calculate for other decoder divergence loss\n",
    "        # Cost list and log likelihood list along with path\n",
    "        reward_list = []; output_list = []; action_list = []; ll_list = []\n",
    "        td_list = [self.env.reset(td) for _ in range(self.num_paths)]\n",
    "        for i in range(self.num_paths):\n",
    "            outputs, actions = [], []\n",
    "            embeddings, _, attn, V, h_old = self.embedder(self._init_embed(td))\n",
    "            fixed = self._precompute(embeddings, path_index=i)\n",
    "            j = 0\n",
    "            while not (self.shrink_size is None and td_list[i].all_finished()):\n",
    "                if j > 1 and j % self.eg_step_gap == 0:\n",
    "                    if not self.is_vrp:\n",
    "                        mask_attn = mask ^ mask_first\n",
    "                    else:\n",
    "                        mask_attn = mask\n",
    "                    embeddings, _ = self.embedder.change(attn, V, h_old, mask_attn, self.is_tsp)\n",
    "                    fixed = self._precompute(embeddings, path_index=i)\n",
    "                log_p, mask = self._get_log_p(fixed, td_list[i], i)\n",
    "                if j == 0:\n",
    "                    mask_first = mask\n",
    "\n",
    "                # Select the indices of the next nodes in the sequences, result (batch_size) long\n",
    "                action = decode_probs(log_p.exp()[:, 0, :], mask[:, 0, :], decode_type=decoder_kwargs[\"decode_type\"])\n",
    "\n",
    "                td_list[i].set(\"action\", action)\n",
    "                td_list[i] = self.env.step(td_list[i])[\"next\"]\n",
    "\n",
    "                # Collect output of step\n",
    "                outputs.append(log_p[:, 0, :])\n",
    "                actions.append(action)\n",
    "                j += 1\n",
    "\n",
    "            outputs, actions = torch.stack(outputs, 1), torch.stack(actions, 1)\n",
    "            reward = self.env.get_reward(td, actions)\n",
    "            ll = self.get_log_likelihood(outputs, actions, mask)\n",
    "\n",
    "            reward_list.append(reward)\n",
    "            output_list.append(outputs)\n",
    "            action_list.append(actions)\n",
    "            ll_list.append(ll)\n",
    "\n",
    "        # SECTION: Policy output part\n",
    "        out = {\n",
    "            \"reward\": torch.stack(reward_list),\n",
    "            \"log_likelihood\": torch.stack(ll_list),\n",
    "            \"kl_divergence\": loss_kl_divergence,\n",
    "            \"actions\": actions if return_actions else None,\n",
    "        }\n",
    "        return out\n",
    "\n",
    "    def _precompute(self, embeddings, num_steps=1, path_index=None):\n",
    "        ''' Decoder '''\n",
    "        # The fixed context projection of the graph embedding is calculated only once for efficiency\n",
    "        graph_embed = embeddings.mean(1)\n",
    "\n",
    "        # Fixed context = (batch_size, 1, embed_dim) to make broadcastable with parallel timesteps\n",
    "        fixed_context = self.project_fixed_context[path_index](graph_embed)[:, None, :]\n",
    "\n",
    "        # The projection of the node embeddings for the attention is calculated once up front\n",
    "        glimpse_key_fixed, glimpse_val_fixed, logit_key_fixed = \\\n",
    "            self.project_node_embeddings[path_index](embeddings[:, None, :, :]).chunk(3, dim=-1)\n",
    "\n",
    "        fixed = PrecomputedCache(\n",
    "            node_embeddings=embeddings,\n",
    "            graph_context=fixed_context,\n",
    "            glimpse_key=self._make_heads(glimpse_key_fixed, num_steps),\n",
    "            glimpse_val=self._make_heads(glimpse_val_fixed, num_steps),\n",
    "            logit_key=logit_key_fixed.contiguous(),\n",
    "        )\n",
    "        return fixed\n",
    "\n",
    "    def _get_log_p(self, cached, td, path_idx, softmax_temp):\n",
    "        step_context = self.context[path_idx](cached.node_embeddings, td)  # [batch, embed_dim]\n",
    "        glimpse_q = (cached.graph_context + step_context).unsqueeze(1)  # [batch, 1, embed_dim]\n",
    "\n",
    "        # Compute keys and values for the nodes\n",
    "        (\n",
    "            glimpse_key_dynamic,\n",
    "            glimpse_val_dynamic,\n",
    "            logit_key_dynamic,\n",
    "        ) = self.dynamic_embedding(td)\n",
    "        glimpse_k = cached.glimpse_key + glimpse_key_dynamic\n",
    "        glimpse_v = cached.glimpse_val + glimpse_val_dynamic\n",
    "        logit_k = cached.logit_key + logit_key_dynamic\n",
    "\n",
    "        # Get the mask\n",
    "        mask = ~td[\"action_mask\"]\n",
    "        \n",
    "        # Compute log prob: MHA + single-head attention\n",
    "        log_p, _ = self._one_to_many_logits(\n",
    "            glimpse_q,\n",
    "            glimpse_k,\n",
    "            glimpse_v,\n",
    "            logit_k,\n",
    "            mask,\n",
    "            path_idx\n",
    "        )\n",
    "\n",
    "        return log_p, mask\n",
    "\n",
    "    def _one_to_many_logits(self, query, glimpse_K, glimpse_V, logit_K, mask, path_index):\n",
    "        batch_size, num_steps, embed_dim = query.size()\n",
    "        key_size = val_size = embed_dim // self.n_heads\n",
    "\n",
    "        # Compute the glimpse, rearrange dimensions so the dimensions are (n_heads, batch_size, num_steps, 1, key_size)\n",
    "        glimpse_Q = query.view(batch_size, num_steps, self.n_heads, 1, key_size).permute(2, 0, 1, 3, 4)\n",
    "\n",
    "        # Batch matrix multiplication to compute compatibilities (n_heads, batch_size, num_steps, graph_size)\n",
    "        compatibility = torch.matmul(glimpse_Q, glimpse_K.transpose(-2, -1)) / math.sqrt(glimpse_Q.size(-1))\n",
    "        if self.mask_inner:\n",
    "            assert self.mask_logits, \"Cannot mask inner without masking logits\"\n",
    "            compatibility[mask[None, :, :, None, :].expand_as(compatibility)] = -math.inf\n",
    "\n",
    "        # Batch matrix multiplication to compute heads (n_heads, batch_size, num_steps, val_size)\n",
    "        heads = torch.matmul(F.softmax(compatibility, dim=-1), glimpse_V)\n",
    "\n",
    "        # Project to get glimpse/updated context node embedding (batch_size, num_steps, embedding_dim)\n",
    "        glimpse = self.project_out[path_index](\n",
    "            heads.permute(1, 2, 3, 0, 4).contiguous().view(-1, num_steps, 1, self.n_heads * val_size))\n",
    "\n",
    "        # Now projecting the glimpse is not needed since this can be absorbed into project_out\n",
    "        # final_Q = self.project_glimpse(glimpse)\n",
    "        final_Q = glimpse\n",
    "\n",
    "        # Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)\n",
    "        # logits = 'compatibility'\n",
    "        logits = torch.matmul(final_Q, logit_K.transpose(-2, -1)).squeeze(-2) / math.sqrt(final_Q.size(-1))\n",
    "\n",
    "        # From the logits compute the probabilities by clipping, masking and softmax\n",
    "        if self.tanh_clipping > 0:\n",
    "            logits = F.tanh(logits) * self.tanh_clipping\n",
    "        if self.mask_logits:\n",
    "            logits[mask] = -math.inf\n",
    "\n",
    "        return logits, glimpse.squeeze(-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***5. Test: Test the Policy Only***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: TensorDictCollateFn is deprecated. Use tensordict_collate_fn instead.\n",
      "TensorDict(\n",
      "    fields={\n",
      "        action_mask: Tensor(shape=torch.Size([64, 20]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        current_node: Tensor(shape=torch.Size([64, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([64, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        first_node: Tensor(shape=torch.Size([64, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        i: Tensor(shape=torch.Size([64, 1]), device=cuda:0, dtype=torch.int64, is_shared=True),\n",
      "        locs: Tensor(shape=torch.Size([64, 20, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "        reward: Tensor(shape=torch.Size([64, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "    batch_size=torch.Size([64]),\n",
      "    device=cuda,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "# Load the environment with test data\n",
    "env = TSPEnv()\n",
    "\n",
    "dataset = env.dataset(batch_size=[10000])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=TensorDictCollate(),\n",
    ")\n",
    "\n",
    "td = next(iter(dataloader)).to(\"cuda\")\n",
    "td = env.reset(td)\n",
    "print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AttentionModelPolicy' object has no attribute 'num_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m policy \u001b[39m=\u001b[39m AttentionModelPolicy(\n\u001b[1;32m      2\u001b[0m     env,\n\u001b[1;32m      3\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m out \u001b[39m=\u001b[39m policy(td, decode_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msampling\u001b[39m\u001b[39m\"\u001b[39m, return_actions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(out)\n",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m, in \u001b[0;36mAttentionModelPolicy.__init__\u001b[0;34m(self, env, encoder, decoder, embedding_dim, num_encode_layers, num_heads, num_paths, eg_step_gap, normalization, mask_inner, force_flash_attn, train_decode_type, val_decode_type, test_decode_type, **unused_kw)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_decode_type \u001b[39m=\u001b[39m test_decode_type\n\u001b[1;32m     52\u001b[0m \u001b[39m# [Decoder]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext \u001b[39m=\u001b[39m [\n\u001b[0;32m---> 54\u001b[0m     env_context(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mname, {\u001b[39m\"\u001b[39m\u001b[39membedding_dim\u001b[39m\u001b[39m\"\u001b[39m: embedding_dim}) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_path)\n\u001b[1;32m     55\u001b[0m ]\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogit_attention \u001b[39m=\u001b[39m LogitAttention(\n\u001b[1;32m     57\u001b[0m     embedding_dim, \n\u001b[1;32m     58\u001b[0m     num_heads, \n\u001b[1;32m     59\u001b[0m     mask_inner\u001b[39m=\u001b[39mmask_inner,\n\u001b[1;32m     60\u001b[0m     force_flash_attn\u001b[39m=\u001b[39mforce_flash_attn,\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[39m# TODO: other features\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl4co/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AttentionModelPolicy' object has no attribute 'num_path'"
     ]
    }
   ],
   "source": [
    "policy = AttentionModelPolicy(\n",
    "    env,\n",
    ").to(\"cuda\")\n",
    "out = policy(td, decode_type=\"sampling\", return_actions=False)\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***-1. Trash bin***\n",
    "\n",
    "Other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def _get_log_p(self, fixed, td, path_index, normalize=True):\n",
    "    #     ''' Decoder '''\n",
    "    #     # Compute query = context node embedding\n",
    "    #     query = fixed.context_node_projected + \\\n",
    "    #         self.project_step_context[path_index](self._get_parallel_step_context(fixed.node_embeddings, td))\n",
    "\n",
    "    #     # Compute keys and values for the nodes\n",
    "    #     glimpse_K, glimpse_V, logit_K = self._get_attention_node_data(fixed, td)\n",
    "\n",
    "    #     # Compute the mask\n",
    "    #     mask = ~td[\"action_mask\"]\n",
    "\n",
    "    #     # Compute logits (unnormalized log_p)\n",
    "    #     log_p, _ = self._one_to_many_logits(query, glimpse_K, glimpse_V, logit_K, mask, path_index)\n",
    "\n",
    "    #     if normalize:\n",
    "    #         log_p = F.log_softmax(log_p / self.temp, dim=-1)\n",
    "\n",
    "    #     assert not torch.isnan(log_p).any()\n",
    "\n",
    "    #     return log_p, mask\n",
    "    \n",
    "    # def _get_parallel_step_context(self, embeddings, td, from_depot=False):\n",
    "    #     \"\"\"\n",
    "    #     Returns the context per step, optionally for multiple steps at once (for efficient evaluation of the model)\n",
    "        \n",
    "    #     :param embeddings: (batch_size, graph_size, embed_dim)\n",
    "    #     :param prev_a: (batch_size, num_steps)\n",
    "    #     :param first_a: Only used when num_steps = 1, action of first step or None if first step\n",
    "    #     :return: (batch_size, num_steps, context_dim)\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     current_node = td[\"current_node\"]\n",
    "    #     batch_size, num_steps = current_node.size()\n",
    "\n",
    "    #     if self.env.name == 'vrp':\n",
    "    #         # Embedding of previous node + remaining capacity\n",
    "    #         if from_depot:\n",
    "    #             # 1st dimension is node idx, but we do not squeeze it since we want to insert step dimension\n",
    "    #             # i.e. we actually want embeddings[:, 0, :][:, None, :] which is equivalent\n",
    "    #             return torch.cat(\n",
    "    #                 (\n",
    "    #                     embeddings[:, 0:1, :].expand(batch_size, num_steps, embeddings.size(-1)),\n",
    "    #                     # used capacity is 0 after visiting depot\n",
    "    #                     self.problem.VEHICLE_CAPACITY - torch.zeros_like(td[\"used_capacity\"][:, :, None])\n",
    "    #                 ),\n",
    "    #                 -1\n",
    "    #             )\n",
    "    #         else:\n",
    "    #             return torch.cat(\n",
    "    #                 (\n",
    "    #                     torch.gather(\n",
    "    #                         embeddings,\n",
    "    #                         1,\n",
    "    #                         current_node.contiguous()\n",
    "    #                             .view(batch_size, num_steps, 1)\n",
    "    #                             .expand(batch_size, num_steps, embeddings.size(-1))\n",
    "    #                     ).view(batch_size, num_steps, embeddings.size(-1)),\n",
    "    #                     self.problem.VEHICLE_CAPACITY - td[\"used_capacity\"][:, :, None]\n",
    "    #                 ),\n",
    "    #                 -1\n",
    "    #             )\n",
    "    #     elif self.is_orienteering or self.env.name == \"pctsp\":\n",
    "    #         return torch.cat(\n",
    "    #             (\n",
    "    #                 torch.gather(\n",
    "    #                     embeddings,\n",
    "    #                     1,\n",
    "    #                     current_node.contiguous()\n",
    "    #                         .view(batch_size, num_steps, 1)\n",
    "    #                         .expand(batch_size, num_steps, embeddings.size(-1))\n",
    "    #                 ).view(batch_size, num_steps, embeddings.size(-1)),\n",
    "    #                 (\n",
    "    #                     td[\"capacity_length\"][:, :, None]\n",
    "    #                     # TODO\n",
    "    #                     # if self.is_orienteering\n",
    "    #                     # else td[\"\"].get_remaining_prize_to_collect()[:, :, None]\n",
    "    #                 )\n",
    "    #             ),\n",
    "    #             -1\n",
    "    #         )\n",
    "    #     else:  # TSP\n",
    "    #         if num_steps == 1:  # We need to special case if we have only 1 step, may be the first or not\n",
    "    #             if state.i.item() == 0:\n",
    "    #                 # First and only step, ignore prev_a (this is a placeholder)\n",
    "    #                 return self.W_placeholder[None, None, :].expand(batch_size, 1, self.W_placeholder.size(-1))\n",
    "    #             else:\n",
    "    #                 return embeddings.gather(\n",
    "    #                     1,\n",
    "    #                     torch.cat((state.first_a, current_node), 1)[:, :, None].expand(batch_size, 2, embeddings.size(-1))\n",
    "    #                 ).view(batch_size, 1, -1)\n",
    "    #         # More than one step, assume always starting with first\n",
    "    #         embeddings_per_step = embeddings.gather(\n",
    "    #             1,\n",
    "    #             current_node[:, 1:, None].expand(batch_size, num_steps - 1, embeddings.size(-1))\n",
    "    #         )\n",
    "    #         return torch.cat((\n",
    "    #             # First step placeholder, cat in dim 1 (time steps)\n",
    "    #             self.W_placeholder[None, None, :].expand(batch_size, 1, self.W_placeholder.size(-1)),\n",
    "    #             # Second step, concatenate embedding of first with embedding of current/previous (in dim 2, context dim)\n",
    "    #             torch.cat((\n",
    "    #                 embeddings_per_step[:, 0:1, :].expand(batch_size, num_steps - 1, embeddings.size(-1)),\n",
    "    #                 embeddings_per_step\n",
    "    #             ), 2)\n",
    "    #         ), 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
