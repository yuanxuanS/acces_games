{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MatNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "sys.path.append(2*\"../\")\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchrl.modules.models.models import MLP\n",
        "from torchrl.envs import EnvBase\n",
        "from tensordict.tensordict import TensorDict\n",
        "\n",
        "from rl4co.envs import ATSPEnv \n",
        "from rl4co.utils.ops import batchify, unbatchify\n",
        "from rl4co.models.nn.attention import LogitAttention\n",
        "from rl4co.models.nn.utils import decode_probs, get_log_likelihood\n",
        "from rl4co.models.nn.env_context import env_context\n",
        "from rl4co.models.nn.env_embedding import env_dynamic_embedding, env_init_embedding\n",
        "from rl4co.data.dataset import TensorDictCollate"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differences between AM and MatNet\n",
        "\n",
        "1. MatNet uses a dual graph attention layer for processing the  set of source and destination nodes A and B separately\n",
        "2. Mixed-score attention: this should make the network learn the \"best\" recipe\n",
        "3. Initial node representation: zero-vectors for A nodes and one-hot vectors for B nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action_mask: Tensor(shape=torch.Size([10]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "        cost_matrix: Tensor(shape=torch.Size([10, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
              "        current_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
              "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "        first_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
              "        i: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = ATSPEnv(num_loc=10)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test out\n",
        "\n",
        "# col_emb.shape: (batch, col_cnt, embedding)\n",
        "# row_emb.shape: (batch, row_cnt, embedding)\n",
        "# cost_mat.shape: (batch, row_cnt, col_cnt)\n",
        "batch = 64\n",
        "row_cnt = 20\n",
        "col_cnt = 30\n",
        "\n",
        "\n",
        "model_params = {\n",
        "    'embedding_dim': 256,\n",
        "    'sqrt_embedding_dim': 256**(1/2),\n",
        "    'encoder_layer_num': 5,\n",
        "    'qkv_dim': 16,\n",
        "    'sqrt_qkv_dim': 16**(1/2),\n",
        "    'head_num': 16,\n",
        "    'logit_clipping': 10,\n",
        "    'ff_hidden_dim': 512,\n",
        "    'ms_hidden_dim': 16,\n",
        "    'ms_layer1_init': (1/2)**(1/2),\n",
        "    'ms_layer2_init': (1/16)**(1/2),\n",
        "    'eval_type': 'argmax',\n",
        "    'one_hot_seed_cnt': 20,  # must be >= node_cnt\n",
        "}\n",
        "\n",
        "\n",
        "row_emb = torch.randn(batch, row_cnt, model_params['embedding_dim'])\n",
        "col_emb = torch.randn(batch, col_cnt, model_params['embedding_dim'])\n",
        "cost_mat = torch.randn(batch, row_cnt, col_cnt)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ours (all we need)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MixedScoreMHA(nn.Module):\n",
        "    def __init__(self, \n",
        "                    embed_dim,\n",
        "                    num_heads,\n",
        "                    hidden_dim: int = 16,\n",
        "                    qkv_dim: int = 16,\n",
        "                    bias=False,\n",
        "                    layer1_init: float = (1/2)**(1/2),\n",
        "                    layer2_init: float = (1/16)**(1/2),\n",
        "                    device=None,\n",
        "                    dtype=None\n",
        "        ):\n",
        "        super().__init__()\n",
        "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
        "        assert (embed_dim % num_heads == 0), \"embed_dim must be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Project\n",
        "        self.Wq = nn.Linear(embed_dim, num_heads*qkv_dim, bias=bias, **factory_kwargs)\n",
        "        self.Wk = nn.Linear(embed_dim, num_heads*qkv_dim, bias=bias, **factory_kwargs)\n",
        "        self.Wv = nn.Linear(embed_dim, num_heads*qkv_dim, bias=bias, **factory_kwargs)\n",
        "        self.out_proj = nn.Linear(num_heads*qkv_dim, embed_dim, **factory_kwargs)\n",
        "\n",
        "        # Init mix params\n",
        "        self.mix1_weight = nn.Parameter(torch.empty(num_heads, 2, hidden_dim).uniform_(-layer1_init, layer1_init))\n",
        "        self.mix1_bias = nn.Parameter(torch.empty(num_heads, hidden_dim).uniform_(-layer1_init, layer1_init))\n",
        "        self.mix2_weight = nn.Parameter(torch.empty(num_heads, hidden_dim, 1).uniform_(-layer2_init, layer2_init))\n",
        "        self.mix2_bias = nn.Parameter(torch.empty(num_heads, 1).uniform_(-layer2_init, layer2_init))\n",
        "\n",
        "    def forward(self, q, k, v, matrix):\n",
        "        # Project q, k, v and reshape to [batch, head_num, row_cnt, hidden_dim]\n",
        "        q, k, v = self.Wq(q), self.Wk(k), self.Wv(v)\n",
        "        q, k, v = map(lambda t: self._reshape_heads(t), (q, k, v))\n",
        "\n",
        "        # Prepare dot product and matrix score: [batch, head_num, row_cnt, col_cnt]\n",
        "        dot_product = torch.einsum('...rd,...cd->...rc', q, k) / math.sqrt(q.shape[-1])\n",
        "        matrix_score = repeat(matrix, 'b r c -> b h r c', h=self.num_heads)\n",
        "\n",
        "        # Mix the scores. Use einsum for best performance\n",
        "        two_scores = torch.stack((dot_product, matrix_score), dim=-1)\n",
        "        ms1 = torch.einsum('bhrct,htd->brhcd', two_scores, self.mix1_weight)\n",
        "        ms2 = torch.einsum('brhcd,hdt->brhct', F.relu(ms1), self.mix2_weight)\n",
        "        mixed_scores = rearrange(ms2, 'b h r c 1 -> b r h c')\n",
        "\n",
        "        # Softmax and multiply with values\n",
        "        weights = F.softmax(mixed_scores, dim=3)\n",
        "        out = torch.matmul(weights, v)\n",
        "        \n",
        "        # Project out\n",
        "        out = rearrange(out, 'b h r d -> b r (h d)')\n",
        "        return self.out_proj(out)\n",
        "    \n",
        "    def _reshape_heads(self, x):\n",
        "        # same as rearrange(v, 'b r (h d) -> b h r d', h=self.num_heads) but faster\n",
        "        return x.view(x.shape[0], x.shape[1], self.num_heads, -1).transpose(1, 2)\n",
        "         \n",
        "    \n",
        "class AddAndInstanceNormalization(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.InstanceNorm1d(embedding_dim, affine=True, track_running_stats=False)\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # [batch, problem, embedding]\n",
        "        added = input1 + input2\n",
        "        normalized = self.norm(added.transpose(1, 2)).transpose(1, 2)\n",
        "        return normalized\n",
        "    \n",
        "class EncodingBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                    embed_dim,\n",
        "                    num_heads,\n",
        "                    ms_hidden_dim=16,\n",
        "                    ff_hidden_dim=512,\n",
        "                    **mha_kwargs\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.mixed_score_mha = MixedScoreMHA(embed_dim, num_heads, ms_hidden_dim, **mha_kwargs)\n",
        "        self.add_n_normalization_1 = AddAndInstanceNormalization(embed_dim)\n",
        "        self.feed_forward = MLP(embed_dim, embed_dim, 1, ff_hidden_dim, activation_class=nn.ReLU)\n",
        "        self.add_n_normalization_2 = AddAndInstanceNormalization(embed_dim)\n",
        "\n",
        "    def forward(self, row_emb, col_emb, cost_mat):\n",
        "        q, k, v = row_emb, col_emb, col_emb\n",
        "        out_mha = self.mixed_score_mha(q, k, v, cost_mat)\n",
        "        out1 = self.add_n_normalization_1(row_emb, out_mha)\n",
        "        out2 = self.feed_forward(out1)\n",
        "        out3 = self.add_n_normalization_2(out1, out2)\n",
        "        return out3 # shape: (batch, row_cnt, embedding)\n",
        "    \n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, **kw):\n",
        "        super().__init__()\n",
        "        self.row_encoding_block = EncodingBlock(**kw)\n",
        "        self.col_encoding_block = EncodingBlock(**kw)\n",
        "\n",
        "    def forward(self, row_emb, col_emb, cost_mat):\n",
        "        # row_emb.shape: (batch, row_cnt, embedding)\n",
        "        # col_emb.shape: (batch, col_cnt, embedding)\n",
        "        # cost_mat.shape: (batch, row_cnt, col_cnt)\n",
        "        row_emb_out = self.row_encoding_block(row_emb, col_emb, cost_mat)\n",
        "        col_emb_out = self.col_encoding_block(col_emb, row_emb, cost_mat.transpose(1, 2))\n",
        "        return row_emb_out, col_emb_out\n",
        "        \n",
        "\n",
        "class MatNetEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, **kw):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(**kw) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, row_emb, col_emb, cost_mat):\n",
        "        for layer in self.layers:\n",
        "            row_emb, col_emb = layer(row_emb, col_emb, cost_mat)\n",
        "        return row_emb, col_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MatNetEncoder(\n",
            "  (layers): ModuleList(\n",
            "    (0-4): 5 x EncoderLayer(\n",
            "      (row_encoding_block): EncodingBlock(\n",
            "        (mixed_score_mha): MixedScoreMHA(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (add_n_normalization_1): AddAndInstanceNormalization(\n",
            "          (norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "        )\n",
            "        (feed_forward): MLP(\n",
            "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        )\n",
            "        (add_n_normalization_2): AddAndInstanceNormalization(\n",
            "          (norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "        )\n",
            "      )\n",
            "      (col_encoding_block): EncodingBlock(\n",
            "        (mixed_score_mha): MixedScoreMHA(\n",
            "          (Wq): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (Wk): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (Wv): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (add_n_normalization_1): AddAndInstanceNormalization(\n",
            "          (norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "        )\n",
            "        (feed_forward): MLP(\n",
            "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "        )\n",
            "        (add_n_normalization_2): AddAndInstanceNormalization(\n",
            "          (norm): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Number of parameters: 5.27 MB\n",
            "torch.Size([64, 20, 256]) torch.Size([64, 30, 256])\n"
          ]
        }
      ],
      "source": [
        "# encoder = MatNetEncoder(**model_params)\n",
        "\n",
        "encoder = MatNetEncoder(num_layers=5, embed_dim=256, num_heads=16)\n",
        "print(encoder)\n",
        "print('Number of parameters: {:.2f} MB'.format(sum(p.numel() for p in encoder.parameters() if p.requires_grad) / 1e6))\n",
        "out = encoder(row_emb, col_emb, cost_mat)\n",
        "print(out[0].shape, out[1].shape)\n",
        "#print number of parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "129 ms ± 1.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit encoder(row_emb, col_emb, cost_mat)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_start_nodes(batch_size, num_nodes, device=\"cpu\"):\n",
        "    \"\"\"Node selection strategy for POMO\n",
        "    Selects different start nodes for each batch element\n",
        "    \"\"\"\n",
        "    selected = torch.arange(num_nodes, device=device).repeat(batch_size)  # TODO: check\n",
        "    return selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PrecomputedCache:\n",
        "    row_embeddings: torch.Tensor\n",
        "    column_embeddings: torch.Tensor\n",
        "    graph_context: torch.Tensor # TODO: check if used in MatNet\n",
        "    glimpse_key: torch.Tensor\n",
        "    glimpse_val: torch.Tensor\n",
        "    logit_key: torch.Tensor\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, env, embedding_dim, num_heads, num_pomo=20, **logit_attn_kwargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_heads = num_heads\n",
        "\n",
        "        assert embedding_dim % num_heads == 0\n",
        "\n",
        "        self.context = env_context(self.env.name, {\"embedding_dim\": embedding_dim})\n",
        "        self.dynamic_embedding = env_dynamic_embedding(\n",
        "            self.env.name, {\"embedding_dim\": embedding_dim}\n",
        "        )\n",
        "\n",
        "        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n",
        "        self.project_node_embeddings = nn.Linear(\n",
        "            embedding_dim, 3 * embedding_dim, bias=False\n",
        "        )\n",
        "        self.project_fixed_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "        # MHA\n",
        "        self.logit_attention = LogitAttention(\n",
        "            embedding_dim, num_heads, **logit_attn_kwargs\n",
        "        )\n",
        "\n",
        "        # POMO\n",
        "        self.num_pomo = max(num_pomo, 1) # POMO = 1 is just normal REINFORCE\n",
        "\n",
        "    def forward(self, td, embeddings, decode_type=\"sampling\"):\n",
        "        # Collect outputs\n",
        "        outputs = []\n",
        "        actions = []\n",
        "\n",
        "        if self.num_pomo > 1:\n",
        "            # POMO: first action is decided via select_start_nodes\n",
        "            action = select_start_nodes(batch_size=td.shape[0], num_nodes=self.num_pomo, device=td.device)\n",
        "\n",
        "            # # Expand td to batch_size * num_pomo\n",
        "            td = batchify(td, self.num_pomo)\n",
        "\n",
        "            td.set(\"action\", action[:, None])\n",
        "            td = self.env.step(td)[\"next\"]\n",
        "            log_p = torch.zeros_like(td['action_mask'], device=td.device) # first log_p is 0, so p = log_p.exp() = 1\n",
        "\n",
        "            outputs.append(log_p.squeeze(1))\n",
        "            actions.append(action)\n",
        "        \n",
        "        # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n",
        "        cached_embeds = self._precompute(embeddings)        \n",
        "\n",
        "        # Here we suppose all the batch is done at the same time\n",
        "        while not td[\"done\"].all():  \n",
        "            # Compute the logits for the next node\n",
        "            log_p, mask = self._get_log_p(cached_embeds, td)\n",
        "\n",
        "            # Select the indices of the next nodes in the sequences, result (batch_size) long\n",
        "            action = decode_probs(\n",
        "                log_p.exp().squeeze(1), mask.squeeze(1), decode_type=decode_type\n",
        "            )\n",
        "\n",
        "            # Step the environment\n",
        "            td.set(\"action\", action[:, None])\n",
        "            td = self.env.step(td)[\"next\"]\n",
        "\n",
        "            # Collect output of step\n",
        "            outputs.append(log_p.squeeze(1))\n",
        "            actions.append(action)\n",
        "\n",
        "        outputs, actions = torch.stack(outputs, 1), torch.stack(actions, 1)\n",
        "        td.set(\"reward\", self.env.get_reward(td, actions))\n",
        "        return outputs, actions, td\n",
        "    \n",
        "    def _precompute(self, embeddings):       \n",
        "        # The projection of the node embeddings for the attention is calculated once up front\n",
        "        (\n",
        "            glimpse_key_fixed,\n",
        "            glimpse_val_fixed,\n",
        "            logit_key_fixed,\n",
        "        ) = self.project_node_embeddings(embeddings[:, None, :, :]).chunk(3, dim=-1)\n",
        "\n",
        "        # Organize in a dataclass for easy access\n",
        "        cached_embeds = PrecomputedCache(\n",
        "            node_embeddings=batchify(embeddings, self.num_pomo),\n",
        "            glimpse_key=batchify(self.logit_attention._make_heads(glimpse_key_fixed), self.num_pomo),\n",
        "            glimpse_val=batchify(self.logit_attention._make_heads(glimpse_val_fixed), self.num_pomo),\n",
        "            logit_key=batchify(logit_key_fixed, self.num_pomo)\n",
        "        )\n",
        "\n",
        "        return cached_embeds\n",
        "\n",
        "    def _get_log_p(self, cached, td):\n",
        "        # Compute the query based on the context (computes automatically the first and last node context)\n",
        "        step_context = self.context(cached.node_embeddings, td)\n",
        "        query = step_context # in POMO, no graph context (trick for overfit) # [batch, 1, embed_dim] # TODO: check if this is the same as POMO\n",
        "\n",
        "        # Compute keys and values for the nodes\n",
        "        glimpse_key_dynamic, glimpse_val_dynamic, logit_key_dynamic = self.dynamic_embedding(td)\n",
        "        glimpse_key = cached.glimpse_key + glimpse_key_dynamic\n",
        "        glimpse_key = cached.glimpse_val + glimpse_val_dynamic\n",
        "        logit_key = cached.logit_key + logit_key_dynamic\n",
        "\n",
        "        # Get the mask\n",
        "        mask = ~td[\"action_mask\"]\n",
        "        mask = mask.unsqueeze(1) if mask.dim() == 2 else mask\n",
        "\n",
        "        # Compute logits\n",
        "        log_p = self.logit_attention(query, glimpse_key, glimpse_key, logit_key, mask)\n",
        "\n",
        "        return log_p, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# problems = reset_state.problems\n",
        "# # problems.shape: (batch, node, node)\n",
        "\n",
        "# batch_size = problems.size(0)\n",
        "# node_cnt = problems.size(1)\n",
        "# embedding_dim = self.model_params['embedding_dim']\n",
        "\n",
        "# row_emb = torch.zeros(size=(batch_size, node_cnt, embedding_dim))\n",
        "# # emb.shape: (batch, node, embedding)\n",
        "# col_emb = torch.zeros(size=(batch_size, node_cnt, embedding_dim))\n",
        "# # shape: (batch, node, embedding)\n",
        "\n",
        "# seed_cnt = self.model_params['one_hot_seed_cnt']\n",
        "# rand = torch.rand(batch_size, seed_cnt)\n",
        "# batch_rand_perm = rand.argsort(dim=1)\n",
        "# rand_idx = batch_rand_perm[:, :node_cnt]\n",
        "\n",
        "# b_idx = torch.arange(batch_size)[:, None].expand(batch_size, node_cnt)\n",
        "# n_idx = torch.arange(node_cnt)[None, :].expand(batch_size, node_cnt)\n",
        "# col_emb[b_idx, n_idx, rand_idx] = \n",
        "\n",
        "\n",
        "class MatNetInitEmbedding(nn.Module):\n",
        "    def __init__(self, embedding_dim, one_hot_seed_cnt):\n",
        "        super(MatNetInitEmbedding, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.one_hot_seed_cnt = one_hot_seed_cnt\n",
        "\n",
        "    def forward(self, td):\n",
        "        # Generate initial embeddings: [batch, node, node]\n",
        "        cost_mat = td[\"cost_matrix\"]\n",
        "        batch_size = cost_mat.size(0)\n",
        "        node_cnt = cost_mat.size(1)\n",
        "        row_emb = torch.zeros_like(cost_mat, device=cost_mat.device)\n",
        "        col_emb = torch.zeros_like(cost_mat, device=cost_mat.device)\n",
        "        # randomize col_emb: we refactor with topk\n",
        "        rand = torch.rand(batch_size, self.one_hot_seed_cnt, device=cost_mat.device)\n",
        "        _, rand_idx = rand.topk(node_cnt, dim=1, largest=False)\n",
        "        b_idx, n_idx = torch.meshgrid(torch.arange(batch_size, device=cost_mat.device),\n",
        "                                    torch.arange(node_cnt, device=cost_mat.device))\n",
        "        col_emb[b_idx, n_idx, rand_idx] = 1\n",
        "        print(\"col_emb\", col_emb.shape)\n",
        "        print(\"row_emb\", row_emb.shape)\n",
        "        return row_emb, col_emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MatNetPolicy(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: EnvBase,\n",
        "        encoder: nn.Module = None,\n",
        "        decoder: nn.Module = None,\n",
        "        embedding_dim: int = 128,\n",
        "        num_pomo: int = 10,\n",
        "        one_hot_seed_cnt: int = 20,\n",
        "        num_encode_layers: int = 5,\n",
        "        num_heads: int = 16,\n",
        "        mask_inner: bool = True,\n",
        "        train_decode_type: str = \"sampling\",\n",
        "        val_decode_type: str = \"greedy\",\n",
        "        test_decode_type: str = \"greedy\",\n",
        "        **unused_kwargs\n",
        "    ):\n",
        "        super(MatNetPolicy, self).__init__()\n",
        "\n",
        "        if len(unused_kwargs) > 0:\n",
        "            print(\"Unused kwargs found in MatNetPolicy init: \", unused_kwargs)\n",
        "\n",
        "        self.env = env\n",
        "\n",
        "        self.init_embedding = MatNetInitEmbedding(embedding_dim, one_hot_seed_cnt)\n",
        "\n",
        "        self.encoder = (\n",
        "            MatNetEncoder(\n",
        "                num_heads=num_heads,\n",
        "                embed_dim=embedding_dim,\n",
        "                num_layers=num_encode_layers,\n",
        "            )\n",
        "            if encoder is None\n",
        "            else encoder\n",
        "        )\n",
        "\n",
        "        self.decoder = (\n",
        "            Decoder(\n",
        "                env,\n",
        "                embedding_dim,\n",
        "                num_heads,\n",
        "                num_pomo=num_pomo,\n",
        "                mask_inner=mask_inner,\n",
        "            )\n",
        "            if decoder is None\n",
        "            else decoder\n",
        "        )\n",
        "        self.num_pomo = num_pomo\n",
        "        self.train_decode_type = train_decode_type\n",
        "        self.val_decode_type = val_decode_type\n",
        "        self.test_decode_type = test_decode_type\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        td: TensorDict,\n",
        "        phase: str = \"train\",\n",
        "        return_actions: bool = False,\n",
        "        **decoder_kwargs,\n",
        "    ) -> TensorDict:\n",
        "        \"\"\"Given observation, precompute embeddings and rollout\"\"\"\n",
        "\n",
        "        # Set decoding type for policy, can be also greedy\n",
        "        row_emb, col_emb = self.init_embedding(td)\n",
        "        encoded_inputs = self.encoder(row_emb, col_emb, td[\"cost_matrix\"])\n",
        "\n",
        "        # Get decode type depending on phase\n",
        "        if decoder_kwargs.get(\"decode_type\", None) is None:\n",
        "            decoder_kwargs[\"decode_type\"] = getattr(self, f\"{phase}_decode_type\")\n",
        "\n",
        "        # Main rollout\n",
        "        log_p, actions, td = self.decoder(td, encoded_inputs, **decoder_kwargs)\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        ll = get_log_likelihood(log_p, actions, td.get(\"mask\", None))\n",
        "        out = {\n",
        "            \"reward\": td[\"reward\"],\n",
        "            \"log_likelihood\": ll,\n",
        "            \"actions\": actions if return_actions else None,\n",
        "        }\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1280x20 and 128x256)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[74], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m td \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(dataloader))\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m td \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset(td)\n\u001b[0;32m---> 23\u001b[0m out \u001b[39m=\u001b[39m policy(td, decode_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msampling\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(out)\n",
            "File \u001b[0;32m~/Dev/rl4co/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[73], line 64\u001b[0m, in \u001b[0;36mMatNetPolicy.forward\u001b[0;34m(self, td, phase, return_actions, **decoder_kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# Set decoding type for policy, can be also greedy\u001b[39;00m\n\u001b[1;32m     63\u001b[0m row_emb, col_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_embedding(td)\n\u001b[0;32m---> 64\u001b[0m encoded_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(row_emb, col_emb, td[\u001b[39m\"\u001b[39;49m\u001b[39mcost_matrix\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     66\u001b[0m \u001b[39m# Get decode type depending on phase\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m decoder_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdecode_type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Dev/rl4co/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[4], line 115\u001b[0m, in \u001b[0;36mMatNetEncoder.forward\u001b[0;34m(self, row_emb, col_emb, cost_mat)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, row_emb, col_emb, cost_mat):\n\u001b[1;32m    114\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 115\u001b[0m         row_emb, col_emb \u001b[39m=\u001b[39m layer(row_emb, col_emb, cost_mat)\n\u001b[1;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m row_emb, col_emb\n",
            "File \u001b[0;32m~/Dev/rl4co/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[4], line 103\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[0;34m(self, row_emb, col_emb, cost_mat)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, row_emb, col_emb, cost_mat):\n\u001b[1;32m    100\u001b[0m     \u001b[39m# row_emb.shape: (batch, row_cnt, embedding)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# col_emb.shape: (batch, col_cnt, embedding)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39m# cost_mat.shape: (batch, row_cnt, col_cnt)\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     row_emb_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrow_encoding_block(row_emb, col_emb, cost_mat)\n\u001b[1;32m    104\u001b[0m     col_emb_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_encoding_block(col_emb, row_emb, cost_mat\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m row_emb_out, col_emb_out\n",
            "File \u001b[0;32m~/Dev/rl4co/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[4], line 86\u001b[0m, in \u001b[0;36mEncodingBlock.forward\u001b[0;34m(self, row_emb, col_emb, cost_mat)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, row_emb, col_emb, cost_mat):\n\u001b[1;32m     85\u001b[0m     q, k, v \u001b[39m=\u001b[39m row_emb, col_emb, col_emb\n\u001b[0;32m---> 86\u001b[0m     out_mha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmixed_score_mha(q, k, v, cost_mat)\n\u001b[1;32m     87\u001b[0m     out1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_n_normalization_1(row_emb, out_mha)\n\u001b[1;32m     88\u001b[0m     out2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(out1)\n",
            "File \u001b[0;32m~/Dev/rl4co/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mMixedScoreMHA.forward\u001b[0;34m(self, q, k, v, matrix)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, q, k, v, matrix):\n\u001b[1;32m     32\u001b[0m     \u001b[39m# Project q, k, v and reshape to [batch, head_num, row_cnt, hidden_dim]\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     q, k, v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWq(q), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWk(k), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWv(v)\n\u001b[1;32m     34\u001b[0m     q, k, v \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m t: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reshape_heads(t), (q, k, v))\n\u001b[1;32m     36\u001b[0m     \u001b[39m# Prepare dot product and matrix score: [batch, head_num, row_cnt, col_cnt]\u001b[39;00m\n",
            "File \u001b[0;32m~/Dev/rl4co/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Dev/rl4co/env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1280x20 and 128x256)"
          ]
        }
      ],
      "source": [
        "env = ATSPEnv(num_loc=20)\n",
        "env.name = \"tsp\" # TODO: make this automatic when creating env\n",
        "\n",
        "dataset = env.dataset(batch_size=[10000])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,  # no need to shuffle, we're resampling every epoch\n",
        "    num_workers=0,\n",
        "    collate_fn=TensorDictCollate(),\n",
        ")\n",
        "\n",
        "policy = MatNetPolicy(\n",
        "    env,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model)\n",
        "\n",
        "td = next(iter(dataloader)).to(\"cuda\")\n",
        "td = env.reset(td)\n",
        "\n",
        "out = policy(td, decode_type=\"sampling\")\n",
        "\n",
        "print(out)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
