{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MatNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "sys.path.append(2*\"../\")\n",
        "\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "from typing import Optional\n",
        "\n",
        "from rl4co.envs import TSPEnv, ATSPEnv \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Differences between AM and MatNet\n",
        "\n",
        "1. MatNet uses a dual graph attention layer for processing the  set of source and destination nodes A and B separately\n",
        "2. Mixed-score attention: this should make the network learn the \"best\" recipe\n",
        "3. Initial node representation: zero-vectors for A nodes and one-hot vectors for B nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action_mask: Tensor(shape=torch.Size([1, 10]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "        current_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
              "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "        first_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
              "        i: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
              "        observation: Tensor(shape=torch.Size([10, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
              "        reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
              "    batch_size=torch.Size([]),\n",
              "    device=cpu,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = ATSPEnv(num_loc=10)\n",
        "env.reset()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MixedScore MHA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Original\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class AddAndInstanceNormalization(nn.Module):\n",
        "    def __init__(self, **model_params):\n",
        "        super().__init__()\n",
        "        embedding_dim = model_params['embedding_dim']\n",
        "        self.norm = nn.InstanceNorm1d(embedding_dim, affine=True, track_running_stats=False)\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # input.shape: (batch, problem, embedding)\n",
        "\n",
        "        added = input1 + input2\n",
        "        # shape: (batch, problem, embedding)\n",
        "\n",
        "        transposed = added.transpose(1, 2)\n",
        "        # shape: (batch, embedding, problem)\n",
        "\n",
        "        normalized = self.norm(transposed)\n",
        "        # shape: (batch, embedding, problem)\n",
        "\n",
        "        back_trans = normalized.transpose(1, 2)\n",
        "        # shape: (batch, problem, embedding)\n",
        "\n",
        "        return back_trans\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, **model_params):\n",
        "        super().__init__()\n",
        "        embedding_dim = model_params['embedding_dim']\n",
        "        ff_hidden_dim = model_params['ff_hidden_dim']\n",
        "\n",
        "        self.W1 = nn.Linear(embedding_dim, ff_hidden_dim)\n",
        "        self.W2 = nn.Linear(ff_hidden_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, input1):\n",
        "        # input.shape: (batch, problem, embedding)\n",
        "\n",
        "        return self.W2(F.relu(self.W1(input1)))\n",
        "\n",
        "\n",
        "class MixedScore_MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, **model_params):\n",
        "        super().__init__()\n",
        "        self.model_params = model_params\n",
        "\n",
        "        head_num = model_params['head_num']\n",
        "        ms_hidden_dim = model_params['ms_hidden_dim']\n",
        "        mix1_init = model_params['ms_layer1_init']\n",
        "        mix2_init = model_params['ms_layer2_init']\n",
        "\n",
        "        mix1_weight = torch.torch.distributions.Uniform(low=-mix1_init, high=mix1_init).sample((head_num, 2, ms_hidden_dim))\n",
        "        mix1_bias = torch.torch.distributions.Uniform(low=-mix1_init, high=mix1_init).sample((head_num, ms_hidden_dim))\n",
        "        self.mix1_weight = nn.Parameter(mix1_weight)\n",
        "        # shape: (head, 2, ms_hidden)\n",
        "        self.mix1_bias = nn.Parameter(mix1_bias)\n",
        "        # shape: (head, ms_hidden)\n",
        "\n",
        "        mix2_weight = torch.torch.distributions.Uniform(low=-mix2_init, high=mix2_init).sample((head_num, ms_hidden_dim, 1))\n",
        "        mix2_bias = torch.torch.distributions.Uniform(low=-mix2_init, high=mix2_init).sample((head_num, 1))\n",
        "        self.mix2_weight = nn.Parameter(mix2_weight)\n",
        "        # shape: (head, ms_hidden, 1)\n",
        "        self.mix2_bias = nn.Parameter(mix2_bias)\n",
        "        # shape: (head, 1)\n",
        "\n",
        "    def forward(self, q, k, v, cost_mat):\n",
        "        # q shape: (batch, head_num, row_cnt, qkv_dim)\n",
        "        # k,v shape: (batch, head_num, col_cnt, qkv_dim)\n",
        "        # cost_mat.shape: (batch, row_cnt, col_cnt)\n",
        "\n",
        "        batch_size = q.size(0)\n",
        "        row_cnt = q.size(2)\n",
        "        col_cnt = k.size(2)\n",
        "\n",
        "        head_num = self.model_params['head_num']\n",
        "        qkv_dim = self.model_params['qkv_dim']\n",
        "        sqrt_qkv_dim = self.model_params['sqrt_qkv_dim']\n",
        "\n",
        "        dot_product = torch.matmul(q, k.transpose(2, 3))\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        dot_product_score = dot_product / sqrt_qkv_dim\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        cost_mat_score = cost_mat[:, None, :, :].expand(batch_size, head_num, row_cnt, col_cnt)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        two_scores = torch.stack((dot_product_score, cost_mat_score), dim=4)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt, 2)\n",
        "\n",
        "        two_scores_transposed = two_scores.transpose(1,2)\n",
        "        # shape: (batch, row_cnt, head_num, col_cnt, 2)\n",
        "\n",
        "        ms1 = torch.matmul(two_scores_transposed, self.mix1_weight)\n",
        "        # shape: (batch, row_cnt, head_num, col_cnt, ms_hidden_dim)\n",
        "\n",
        "        ms1 = ms1 + self.mix1_bias[None, None, :, None, :]\n",
        "        # shape: (batch, row_cnt, head_num, col_cnt, ms_hidden_dim)\n",
        "\n",
        "        ms1_activated = F.relu(ms1)\n",
        "\n",
        "        ms2 = torch.matmul(ms1_activated, self.mix2_weight)\n",
        "        # shape: (batch, row_cnt, head_num, col_cnt, 1)\n",
        "\n",
        "        ms2 = ms2 + self.mix2_bias[None, None, :, None, :]\n",
        "        # shape: (batch, row_cnt, head_num, col_cnt, 1)\n",
        "\n",
        "        mixed_scores = ms2.transpose(1,2)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt, 1)\n",
        "\n",
        "        mixed_scores = mixed_scores.squeeze(4)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        weights = nn.Softmax(dim=3)(mixed_scores)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        out = torch.matmul(weights, v)\n",
        "        # shape: (batch, head_num, row_cnt, qkv_dim)\n",
        "\n",
        "        out_transposed = out.transpose(1, 2)\n",
        "        # shape: (batch, row_cnt, head_num, qkv_dim)\n",
        "\n",
        "        out_concat = out_transposed.reshape(batch_size, row_cnt, head_num * qkv_dim)\n",
        "        # shape: (batch, row_cnt, head_num*qkv_dim)\n",
        "\n",
        "        return out_concat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 30, 512])\n"
          ]
        }
      ],
      "source": [
        "# make axamples with\n",
        "       # q shape: (batch, head_num, row_cnt, qkv_dim)\n",
        "        # k,v shape: (batch, head_num, col_cnt, qkv_dim)\n",
        "        # cost_mat.shape: (batch, row_cnt, col_cnt)\n",
        "\n",
        "batch_size = 32\n",
        "head_num = 8\n",
        "row_cnt = 30\n",
        "col_cnt = 20\n",
        "qkv_dim = 64\n",
        "\n",
        "q = torch.randn(batch_size, head_num, row_cnt, qkv_dim)\n",
        "k = torch.randn(batch_size, head_num, col_cnt, qkv_dim)\n",
        "v = torch.randn(batch_size, head_num, col_cnt, qkv_dim)\n",
        "cost_mat = torch.randn(batch_size, row_cnt, col_cnt)\n",
        "\n",
        "model_params = {\n",
        "        'head_num': head_num,\n",
        "        'qkv_dim': qkv_dim,\n",
        "        'sqrt_qkv_dim': qkv_dim**(1/2),\n",
        "        'ms_hidden_dim': 16,\n",
        "        'ms_layer1_init': (1/2)**(1/2),\n",
        "        'ms_layer2_init': (1/16)**(1/2)\n",
        "        }\n",
        "\n",
        "ms_mha_old = MixedScore_MultiHeadAttention(**model_params)\n",
        "out = ms_mha_old(q, k, v, cost_mat)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "781 µs ± 2.77 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit ms_mha_old(q, k, v, cost_mat)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.modules.models import MLP\n",
        "\n",
        "\n",
        "class MixedScoreMHA(nn.Module):\n",
        "    def __init__(self, head_num = 8,\n",
        "                    qkv_dim = 64,\n",
        "                    ms_hidden_dim = 16,\n",
        "                    sqrt_qkv_dim = qkv_dim**(1/2),\n",
        "                    **kwargs     \n",
        "            ):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.head_num = head_num\n",
        "        self.qkv_dim = qkv_dim\n",
        "        self.ms_hidden_dim = ms_hidden_dim\n",
        "        self.sqrt_qkv_dim = sqrt_qkv_dim\n",
        "\n",
        "        # NOTE: we refactor the code to use MLP, but we do not use the initialization MatNet used\n",
        "        # this is not mentioned in the paper of course\n",
        "        self.mlp  = MLP(2, 1, depth=1, num_cells=ms_hidden_dim, activation_class=nn.ReLU)\n",
        "\n",
        "    def forward(self, q, k, v, cost_mat):\n",
        "        dot_product = torch.matmul(q, k.transpose(2, 3))\n",
        "        dot_product_score = dot_product / self.sqrt_qkv_dim\n",
        "        cost_mat_score = cost_mat.unsqueeze(1).repeat(1, self.head_num, 1, 1)\n",
        "        two_scores = torch.stack((dot_product_score, cost_mat_score), dim=-1).transpose(1, 2)\n",
        "        # shape: (batch, row_cnt, head_num, col_cnt, 2)\n",
        "        mixed_scores = self.mlp(two_scores).squeeze(-1).transpose(1, 2)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "        weights = nn.Softmax(dim=3)(mixed_scores)\n",
        "        out = torch.matmul(weights, v)\n",
        "        out_concat = out.transpose(1, 2).reshape(q.size(0), q.size(2), self.head_num * self.qkv_dim)\n",
        "        return out_concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torchrl.modules.models.models.MLP"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlp  = MLP(2, 1, depth=1, num_cells=ms_hidden_dim, activation_class=nn.ReLU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MixedScoreMHA2(nn.Module):\n",
        "    def __init__(self, **model_params):\n",
        "        super().__init__()\n",
        "        self.model_params = model_params\n",
        "\n",
        "        head_num = model_params['head_num']\n",
        "        ms_hidden_dim = model_params['ms_hidden_dim']\n",
        "        mix1_init = model_params['ms_layer1_init']\n",
        "        mix2_init = model_params['ms_layer2_init']\n",
        "        self.mlp  = MLP(2, 1, depth=1, num_cells=ms_hidden_dim, activation_class=nn.ReLU)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, cost_mat):\n",
        "        # q shape: (batch, head_num, row_cnt, qkv_dim)\n",
        "        # k,v shape: (batch, head_num, col_cnt, qkv_dim)\n",
        "        # cost_mat.shape: (batch, row_cnt, col_cnt)\n",
        "\n",
        "        batch_size = q.size(0)\n",
        "        row_cnt = q.size(2)\n",
        "        col_cnt = k.size(2)\n",
        "\n",
        "        head_num = self.model_params['head_num']\n",
        "        qkv_dim = self.model_params['qkv_dim']\n",
        "        sqrt_qkv_dim = self.model_params['sqrt_qkv_dim']\n",
        "\n",
        "        dot_product = torch.matmul(q, k.transpose(2, 3))\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        dot_product_score = dot_product / sqrt_qkv_dim\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        cost_mat_score = cost_mat[:, None, :, :].expand(batch_size, head_num, row_cnt, col_cnt)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        two_scores = torch.stack((dot_product_score, cost_mat_score), dim=4)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt, 2)\n",
        "\n",
        "        two_scores_transposed = two_scores.transpose(1,2)\n",
        "        # shape: (batch, row_cnt, head_num, col_cnt, 2)\n",
        "\n",
        "        mixed_scores = self.mlp(two_scores).squeeze(-1)\n",
        "        # print(mixed_scores.shape)\n",
        "\n",
        "        weights = nn.Softmax(dim=3)(mixed_scores)\n",
        "        # shape: (batch, head_num, row_cnt, col_cnt)\n",
        "\n",
        "        out = torch.matmul(weights, v)\n",
        "        # shape: (batch, head_num, row_cnt, qkv_dim)\n",
        "\n",
        "        out_transposed = out.transpose(1, 2)\n",
        "        # shape: (batch, row_cnt, head_num, qkv_dim)\n",
        "\n",
        "        out_concat = out_transposed.reshape(batch_size, row_cnt, head_num * qkv_dim)\n",
        "        # shape: (batch, row_cnt, head_num*qkv_dim)\n",
        "\n",
        "        return out_concat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 30, 512])\n"
          ]
        }
      ],
      "source": [
        "# make axamples with\n",
        "       # q shape: (batch, head_num, row_cnt, qkv_dim)\n",
        "        # k,v shape: (batch, head_num, col_cnt, qkv_dim)\n",
        "        # cost_mat.shape: (batch, row_cnt, col_cnt)\n",
        "\n",
        "batch_size = 32\n",
        "head_num = 8\n",
        "row_cnt = 30\n",
        "col_cnt = 20\n",
        "qkv_dim = 64\n",
        "\n",
        "q = torch.randn(batch_size, head_num, row_cnt, qkv_dim)\n",
        "k = torch.randn(batch_size, head_num, col_cnt, qkv_dim)\n",
        "v = torch.randn(batch_size, head_num, col_cnt, qkv_dim)\n",
        "cost_mat = torch.randn(batch_size, row_cnt, col_cnt)\n",
        "\n",
        "model_params = {\n",
        "        'head_num': head_num,\n",
        "        'qkv_dim': qkv_dim,\n",
        "        'sqrt_qkv_dim': qkv_dim**(1/2),\n",
        "        'ms_hidden_dim': 16,\n",
        "        'ms_layer1_init': (1/2)**(1/2),\n",
        "        'ms_layer2_init': (1/16)**(1/2)\n",
        "        }\n",
        "\n",
        "ms_mha = MixedScoreMHA2(**model_params)\n",
        "out = ms_mha(q, k, v, cost_mat)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.59 ms ± 5.52 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit ms_mha(q, k, v, cost_mat)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
