{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TSP Environment - test import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd79419b310>>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# rich tracebacks\n",
        "import rich\n",
        "import rich.traceback\n",
        "\n",
        "rich.traceback.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "\n",
        "from graph_encoder import GraphAttentionEncoder\n",
        "from attention import CrossAttention\n",
        "from utils import CachedLookup, sample_many\n",
        "from reinforce_baselines import *\n",
        "\n",
        "from ncobench.envs import TSPEnv\n",
        "from sanity.problem_tsp import TSP"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModelFixed(NamedTuple):\n",
        "    \"\"\"\n",
        "    Context for AttentionModel decoder that is fixed during decoding so can be precomputed/cached\n",
        "    This class allows for efficient indexing of multiple Tensors at once\n",
        "    \"\"\"\n",
        "    node_embeddings: torch.Tensor\n",
        "    context_node_projected: torch.Tensor\n",
        "    glimpse_key: torch.Tensor\n",
        "    glimpse_val: torch.Tensor\n",
        "    logit_key: torch.Tensor\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        assert torch.is_tensor(key) or isinstance(key, slice)\n",
        "        return AttentionModelFixed(\n",
        "            node_embeddings=self.node_embeddings[key],\n",
        "            context_node_projected=self.context_node_projected[key],\n",
        "            glimpse_key=self.glimpse_key[:, key],  # dim 0 are the heads\n",
        "            glimpse_val=self.glimpse_val[:, key],  # dim 0 are the heads\n",
        "            logit_key=self.logit_key[key]\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AttentionModelBase\n",
        "\n",
        "Here we declare the `AttentionModelBase`, which is the `nn.Module`:\n",
        "- Given initial states, it returns the solutions and rewards for them\n",
        "- We then wrap the main model with REINFORCE baselines and epoch callbacks to train it (full `AttentionModel`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModelBase(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 *,\n",
        "                 env = TSP(),\n",
        "                 n_encode_layers: int = 2,\n",
        "                 tanh_clipping: float = 10.,\n",
        "                 mask_inner: bool = True,\n",
        "                 mask_logits: bool = True,\n",
        "                 normalization: str = 'batch',\n",
        "                 n_heads: int = 8,\n",
        "                 checkpoint_encoder: bool = False,\n",
        "                 use_flash_attn: bool = False,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super(AttentionModelBase, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_encode_layers = n_encode_layers\n",
        "        self.decode_type = None\n",
        "        self.temp = 1.0\n",
        "\n",
        "        self.env = env\n",
        "\n",
        "        self.tanh_clipping = tanh_clipping\n",
        "\n",
        "        self.mask_inner = mask_inner\n",
        "        self.mask_logits = mask_logits\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.checkpoint_encoder = checkpoint_encoder\n",
        "\n",
        "        # TODO: add extra except TSP\n",
        "        step_context_dim = 2 * embedding_dim  # Embedding of first and last node\n",
        "        node_dim = 2  # x, y\n",
        "        \n",
        "        # Learned input symbols for first action\n",
        "        self.W_placeholder = nn.Parameter(torch.Tensor(2 * embedding_dim))\n",
        "        self.W_placeholder.data.uniform_(-1, 1)  # Placeholder should be in range of activations\n",
        "\n",
        "        self.init_embed = nn.Linear(node_dim, embedding_dim)\n",
        "\n",
        "        self.embedder = GraphAttentionEncoder(\n",
        "            n_heads=n_heads,\n",
        "            embed_dim=embedding_dim,\n",
        "            n_layers=self.n_encode_layers,\n",
        "            normalization=normalization,\n",
        "            use_flash_attn=use_flash_attn,\n",
        "        )\n",
        "        \n",
        "        self.cross_attention = CrossAttention() # NOTE: FlashCrossAttention does not support inner masking!\n",
        "\n",
        "        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n",
        "        self.project_node_embeddings = nn.Linear(embedding_dim, 3 * embedding_dim, bias=False)\n",
        "        self.project_fixed_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.project_step_context = nn.Linear(step_context_dim, embedding_dim, bias=False)\n",
        "        assert embedding_dim % n_heads == 0\n",
        "        # Note n_heads * val_dim == embedding_dim so input to project_out is embedding_dim\n",
        "        self.project_out = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, td, phase: str = \"train\", decode_type: str = \"sampling\"):\n",
        "        \"\"\"Given observation, precompute embeddings and rollout\"\"\"\n",
        "\n",
        "        # Set decoding type for policy, can be also greedy\n",
        "        self.decode_type = decode_type\n",
        "\n",
        "        if self.checkpoint_encoder and self.training:  # Only checkpoint if we need gradients\n",
        "            embeddings, _ = checkpoint(self.embedder, self._init_embed(td))\n",
        "        else:\n",
        "            embeddings, _ = self.embedder(self._init_embed(td))\n",
        "\n",
        "        _log_p, pi, td = self._rollout(td, embeddings)\n",
        "\n",
        "        cost, mask = self.env.get_costs(td, pi)\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        # DataParallel since sequences can be of different lengths\n",
        "        ll = self._calc_log_likelihood(_log_p, pi, mask)\n",
        "        out = {\"reward\": -cost, \"log_likelihood\": ll, \"actions\": pi, \"cost\": cost}\n",
        "        return out\n",
        "    \n",
        "    def _rollout(self, td, embeddings):\n",
        "\n",
        "        outputs = []\n",
        "        sequences = []\n",
        "\n",
        "        state = self.env.make_state(td)\n",
        "\n",
        "        # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n",
        "        fixed = self._precompute(embeddings)\n",
        "\n",
        "        # while not td[\"done\"].any(): # NOTE: here we suppose all the batch is done at the same time\n",
        "        \n",
        "        while not state.all_finished():\n",
        "\n",
        "            log_p, mask = self._get_log_p(fixed, state)\n",
        "\n",
        "            # Select the indices of the next nodes in the sequences, result (batch_size) long\n",
        "            selected = self._select_node(log_p.exp()[:, 0, :], mask[:, 0, :])  # Squeeze out steps dimension\n",
        "           \n",
        "            # Update state\n",
        "            state.update(selected)\n",
        "\n",
        "            # Collect output of step\n",
        "            outputs.append(log_p[:, 0, :])\n",
        "            sequences.append(selected)\n",
        "\n",
        "        # Collected lists, return Tensor\n",
        "        return torch.stack(outputs, 1), torch.stack(sequences, 1), state\n",
        "\n",
        "    def _calc_log_likelihood(self, _log_p, a, mask):\n",
        "\n",
        "        # Get log_p corresponding to selected actions\n",
        "        log_p = _log_p.gather(2, a.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Optional: mask out actions irrelevant to objective so they do not get reinforced\n",
        "        if mask is not None:\n",
        "            log_p[mask] = 0\n",
        "\n",
        "        assert (log_p > -1000).data.all(), \"Logprobs should not be -inf, check sampling procedure!\"\n",
        "\n",
        "        # Calculate log_likelihood\n",
        "        return log_p.sum(1)\n",
        "\n",
        "    def _init_embed(self, x):\n",
        "        # TODO: others except TSP\n",
        "        return self.init_embed(x)\n",
        "    \n",
        "    def _select_node(self, probs, mask):\n",
        "\n",
        "        assert (probs == probs).all(), \"Probs should not contain any nans\"\n",
        "\n",
        "        if self.decode_type == \"greedy\":\n",
        "            _, selected = probs.max(1)\n",
        "            assert not mask.gather(1, selected.unsqueeze(\n",
        "                -1)).data.any(), \"Decode greedy: infeasible action has maximum probability\"\n",
        "\n",
        "        elif self.decode_type == \"sampling\":\n",
        "            selected = probs.multinomial(1).squeeze(1)\n",
        "\n",
        "            while mask.gather(1, selected.unsqueeze(-1)).data.any():\n",
        "                print('Sampled bad values, resampling!')\n",
        "                selected = probs.multinomial(1).squeeze(1)\n",
        "\n",
        "        else:\n",
        "            assert False, \"Unknown decode type\"\n",
        "        return selected\n",
        "\n",
        "    def _precompute(self, embeddings, num_steps=1):\n",
        "\n",
        "        # The fixed context projection of the graph embedding is calculated only once for efficiency\n",
        "        graph_embed = embeddings.mean(1)\n",
        "        # fixed context = (batch_size, 1, embed_dim) to make broadcastable with parallel timesteps\n",
        "        fixed_context = self.project_fixed_context(graph_embed)[:, None, :]\n",
        "\n",
        "        # The projection of the node embeddings for the attention is calculated once up front\n",
        "        glimpse_key_fixed, glimpse_val_fixed, logit_key_fixed = \\\n",
        "            self.project_node_embeddings(embeddings[:, None, :, :]).chunk(3, dim=-1)\n",
        "        \n",
        "        # No need to rearrange key for logit as there is a single head\n",
        "        fixed_attention_node_data = (\n",
        "            self._make_heads(glimpse_key_fixed, num_steps),\n",
        "            self._make_heads(glimpse_val_fixed, num_steps),\n",
        "            logit_key_fixed.contiguous()\n",
        "        )\n",
        "        return AttentionModelFixed(embeddings, fixed_context, *fixed_attention_node_data)\n",
        "\n",
        "    def _get_log_p(self, fixed, td, normalize=True):\n",
        "        \n",
        "        # Compute query = context node embedding\n",
        "        query = fixed.context_node_projected + \\\n",
        "                self.project_step_context(self._get_parallel_step_context(fixed.node_embeddings, td))\n",
        "\n",
        "        # Compute keys and values for the nodes\n",
        "        glimpse_K, glimpse_V, logit_K = self._get_attention_node_data(fixed, td)\n",
        "        # Compute the mask\n",
        "        mask = td.get_mask()\n",
        "        # Compute logits (unnormalized log_p)\n",
        "        log_p, glimpse = self._one_to_many_logits(query, glimpse_K, glimpse_V, logit_K, mask)\n",
        "\n",
        "\n",
        "        if normalize:\n",
        "            log_p = torch.log_softmax(log_p / self.temp, dim=-1)\n",
        "\n",
        "        assert not torch.isnan(log_p).any()\n",
        "\n",
        "        return log_p, mask\n",
        "\n",
        "    def _get_parallel_step_context(self, embeddings, state):\n",
        "        \"\"\"\n",
        "        Returns the context per step, optionally for multiple steps at once (for efficient evaluation of the model)\n",
        "        \n",
        "        :param embeddings: (batch_size, graph_size, embed_dim)\n",
        "        :param prev_a: (batch_size, num_steps)\n",
        "        :param first_a: Only used when num_steps = 1, action of first step or None if first step\n",
        "        :return: (batch_size, num_steps, context_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        current_node= state.get_current_node()\n",
        "        batch_size, num_steps = current_node.size()\n",
        "\n",
        "        # TODO: add others except TSP\n",
        "        if num_steps == 1:  # We need to special case if we have only 1 step, may be the first or not\n",
        "            if state.i.item() == 0:\n",
        "                # First and only step, ignore prev_a (this is a placeholder)\n",
        "                return self.W_placeholder[None, None, :].expand(batch_size, 1, self.W_placeholder.size(-1))\n",
        "            else:\n",
        "                return embeddings.gather(\n",
        "                    1,\n",
        "                    torch.cat((state.first_a, current_node), 1)[:, :, None].expand(batch_size, 2, embeddings.size(-1))\n",
        "                ).view(batch_size, 1, -1)\n",
        "        # More than one step, assume always starting with first\n",
        "        embeddings_per_step = embeddings.gather(\n",
        "            1,\n",
        "            current_node[:, 1:, None].expand(batch_size, num_steps - 1, embeddings.size(-1))\n",
        "        )\n",
        "        return torch.cat((\n",
        "            # First step placeholder, cat in dim 1 (time steps)\n",
        "            self.W_placeholder[None, None, :].expand(batch_size, 1, self.W_placeholder.size(-1)),\n",
        "            # Second step, concatenate embedding of first with embedding of current/previous (in dim 2, context dim)\n",
        "            torch.cat((\n",
        "                embeddings_per_step[:, 0:1, :].expand(batch_size, num_steps - 1, embeddings.size(-1)),\n",
        "                embeddings_per_step\n",
        "            ), 2)\n",
        "        ), 1)\n",
        "\n",
        "    def _one_to_many_logits(self, query, key, value, logit_K, mask):\n",
        "\n",
        "        batch_size, num_steps, embed_dim = query.size()\n",
        "        key_size = val_size = embed_dim // self.n_heads\n",
        "\n",
        "        # TODO: no need to rearrange with refactor\n",
        "        kv = torch.stack([key, value])\n",
        "        q = rearrange(query, 'b 1 (h s) -> b 1 h s', h=self.n_heads)\n",
        "        kv = rearrange(kv, 'two h b 1 g s -> b g two h s', two=2, h=self.n_heads)     \n",
        "\n",
        "        # 1 means to keep, so we invert the mask\n",
        "        key_padding_mask = ~mask.squeeze()\n",
        "        heads = self.cross_attention(q, kv, key_padding_mask=key_padding_mask)\n",
        "\n",
        "        heads = rearrange(heads, 'b 1 h g -> h b 1 1 g')\n",
        "\n",
        "        # Project to get glimpse/updated context node embedding (batch_size, num_steps, embedding_dim)\n",
        "        glimpse = self.project_out(\n",
        "            heads.permute(1, 2, 3, 0, 4).contiguous().view(-1, num_steps, 1, self.n_heads * val_size))\n",
        "        \n",
        "        # Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)\n",
        "        # logits = 'compatibility'\n",
        "        logits = torch.matmul(glimpse, logit_K.transpose(-2, -1)).squeeze(-2) / math.sqrt(glimpse.size(-1))\n",
        "\n",
        "        # From the logits compute the probabilities by clipping, masking and softmax\n",
        "        if self.tanh_clipping > 0:\n",
        "            logits = torch.tanh(logits) * self.tanh_clipping\n",
        "        if self.mask_logits:\n",
        "            logits[mask] = -math.inf\n",
        "\n",
        "        return logits, glimpse.squeeze(-2)\n",
        "\n",
        "    def _get_attention_node_data(self, fixed: AttentionModelFixed, td) -> dict:\n",
        "        # TODO: add others except TSP\n",
        "        # TSP or VRP without split delivery\n",
        "        return fixed.glimpse_key, fixed.glimpse_val, fixed.logit_key\n",
        "\n",
        "    def _make_heads(self, v, num_steps=None):\n",
        "        # TODO: refactor so no need for rearrange\n",
        "        assert num_steps is None or v.size(1) == 1 or v.size(1) == num_steps\n",
        "\n",
        "        return (\n",
        "            v.contiguous().view(v.size(0), v.size(1), v.size(2), self.n_heads, -1)\n",
        "            .expand(v.size(0), v.size(1) if num_steps is None else num_steps, v.size(2), self.n_heads, -1)\n",
        "            .permute(3, 0, 1, 2, 4)  # (n_heads, batch_size, num_steps, graph_size, head_dim)\n",
        "        )\n",
        "    \n",
        "    def precompute_fixed(self, x):\n",
        "        embeddings, _ = self.embedder(self._init_embed(x))\n",
        "        # Use a CachedLookup such that if we repeatedly index this object with the same index we only need to do\n",
        "        # the lookup once... this is the case if all elements in the batch have maximum batch size\n",
        "        return CachedLookup(self._precompute(embeddings))\n",
        "\n",
        "    def sample_many(self, x, batch_rep=1, iter_rep=1):\n",
        "        # Bit ugly but we need to pass the embeddings as well.\n",
        "        # Making a tuple will not work with the problem.get_cost function\n",
        "        return sample_many(\n",
        "            lambda x: self._inner(*x),  # Need to unpack tuple into arguments\n",
        "            lambda x, pi: self.env.get_costs(x[0], pi),  # Don't need embeddings as input to get_costs\n",
        "            (x, self.embedder(self._init_embed(x))[0]),  # Pack input with embeddings (additional input)\n",
        "            batch_rep, iter_rep\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test `AttentionModelBase`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">24</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span>x = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">iter</span>(dataloader)).to(<span style=\"color: #808000; text-decoration-color: #808000\">\"cuda\"</span>)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">22 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">23 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>24 out = model(x, decode_type=<span style=\"color: #808000; text-decoration-color: #808000\">\"greedy\"</span>)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 # print(out['reward'].shape)</span>                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">75</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 72 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 73 │   │   │   </span>embeddings, _ = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embedder(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._init_embed(td))                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 74 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 75 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>_log_p, pi, td = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._rollout(td, embeddings)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 76 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 77 │   │   </span>cost, mask = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.env.get_costs(td, pi)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 78 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_rollout</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">99</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 96 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> state.all_finished():                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 99 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>log_p, mask = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._get_log_p(fixed, state)                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">101 │   │   │   # Select the indices of the next nodes in the sequences, result (batch_size)</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">102 │   │   │   </span>selected = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._select_node(log_p.exp()[:, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, :], mask[:, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, :])  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Squeeze</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_get_log_p</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">182</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">179 │   │   # Compute the mask</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">180 │   │   </span>mask = td.get_mask()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">181 │   │   # Compute logits (unnormalized log_p)</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>182 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>log_p, glimpse = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._one_to_many_logits(query, glimpse_K, glimpse_V, logit_K,    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">185 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> normalize:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_one_to_many_logits</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">242</span>                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">239 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 │   │   # 1 means to keep, so we invert the mask</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">241 │   │   </span>key_padding_mask = ~mask.squeeze()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>242 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>heads = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cross_attention(q, kv, key_padding_mask=key_padding_mask)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">243 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">244 │   │   </span>heads = rearrange(heads, <span style=\"color: #808000; text-decoration-color: #808000\">'b 1 h g -&gt; h b 1 1 g'</span>)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">245 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/botu/Dev/ncobench/notebooks/am/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">attention.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">275</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">272 │   │   │   │   │   │   │   │   │     </span>device=scores.device)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">273 │   │   │   </span>padding_mask.masked_fill_(key_padding_mask, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.0</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">274 │   │   │   # TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better </span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>275 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>scores = scores + rearrange(padding_mask, <span style=\"color: #808000; text-decoration-color: #808000\">'b s -&gt; b 1 1 s'</span>)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">276 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> causal:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">277 │   │   │   # \"triu_tril_cuda_template\" not implemented for 'BFloat16'</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">278 │   │   │   # So we have to construct the mask in float</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m24\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0mx = \u001b[96mnext\u001b[0m(\u001b[96miter\u001b[0m(dataloader)).to(\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m)                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m22 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m23 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m24 out = model(x, decode_type=\u001b[33m\"\u001b[0m\u001b[33mgreedy\u001b[0m\u001b[33m\"\u001b[0m)                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m# print(out['reward'].shape)\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m26 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m75\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 72 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 73 \u001b[0m\u001b[2m│   │   │   \u001b[0membeddings, _ = \u001b[96mself\u001b[0m.embedder(\u001b[96mself\u001b[0m._init_embed(td))                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 74 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 75 \u001b[2m│   │   \u001b[0m_log_p, pi, td = \u001b[96mself\u001b[0m._rollout(td, embeddings)                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 76 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 77 \u001b[0m\u001b[2m│   │   \u001b[0mcost, mask = \u001b[96mself\u001b[0m.env.get_costs(td, pi)                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 78 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m_rollout\u001b[0m:\u001b[94m99\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[95mnot\u001b[0m state.all_finished():                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 99 \u001b[2m│   │   │   \u001b[0mlog_p, mask = \u001b[96mself\u001b[0m._get_log_p(fixed, state)                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Select the indices of the next nodes in the sequences, result (batch_size)\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m102 \u001b[0m\u001b[2m│   │   │   \u001b[0mselected = \u001b[96mself\u001b[0m._select_node(log_p.exp()[:, \u001b[94m0\u001b[0m, :], mask[:, \u001b[94m0\u001b[0m, :])  \u001b[2m# Squeeze\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m_get_log_p\u001b[0m:\u001b[94m182\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m179 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Compute the mask\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   │   \u001b[0mmask = td.get_mask()                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Compute logits (unnormalized log_p)\u001b[0m                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m182 \u001b[2m│   │   \u001b[0mlog_p, glimpse = \u001b[96mself\u001b[0m._one_to_many_logits(query, glimpse_K, glimpse_V, logit_K,    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m185 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m normalize:                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m_one_to_many_logits\u001b[0m:\u001b[94m242\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m239 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m240 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# 1 means to keep, so we invert the mask\u001b[0m                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m241 \u001b[0m\u001b[2m│   │   \u001b[0mkey_padding_mask = ~mask.squeeze()                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m242 \u001b[2m│   │   \u001b[0mheads = \u001b[96mself\u001b[0m.cross_attention(q, kv, key_padding_mask=key_padding_mask)             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m243 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m244 \u001b[0m\u001b[2m│   │   \u001b[0mheads = rearrange(heads, \u001b[33m'\u001b[0m\u001b[33mb 1 h g -> h b 1 1 g\u001b[0m\u001b[33m'\u001b[0m)                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m245 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/botu/Dev/ncobench/notebooks/am/\u001b[0m\u001b[1;33mattention.py\u001b[0m:\u001b[94m275\u001b[0m in \u001b[92mforward\u001b[0m                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m272 \u001b[0m\u001b[2m│   │   │   │   │   │   │   │   │     \u001b[0mdevice=scores.device)                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m273 \u001b[0m\u001b[2m│   │   │   \u001b[0mpadding_mask.masked_fill_(key_padding_mask, \u001b[94m0.0\u001b[0m)                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m274 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better \u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m275 \u001b[2m│   │   │   \u001b[0mscores = scores + rearrange(padding_mask, \u001b[33m'\u001b[0m\u001b[33mb s -> b 1 1 s\u001b[0m\u001b[33m'\u001b[0m)                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m276 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m causal:                                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# \"triu_tril_cuda_template\" not implemented for 'BFloat16'\u001b[0m                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# So we have to construct the mask in float\u001b[0m                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from sanity.\n",
        "dataset = TSP.make_dataset(size=20, num_samples=1000)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=8,\n",
        "                shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "                num_workers=0,\n",
        "                collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            )\n",
        "\n",
        "\n",
        "model = AttentionModelBase(\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model, backend=\"cuda\")\n",
        "\n",
        "x = next(iter(dataloader)).to(\"cuda\")\n",
        "\n",
        "\n",
        "out = model(x, decode_type=\"greedy\")\n",
        "# print(out['reward'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModel(nn.Module):\n",
        "    def __init__(self, env, policy):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        # self.policy = instantiate(cfg.policy)\n",
        "        # self.baseline = instantiate(cfg.baseline) TODO\n",
        "\n",
        "    def forward(self, td, phase: str=\"train\", decode_type: str=None):\n",
        "        \"\"\"Evaluate model, get costs and log probabilities and compare with baseline\"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "        out_policy = self.policy(td)\n",
        "        bl_val, bl_loss = self.baseline.eval(td, out_policy['cost'])\n",
        "\n",
        "        # print(bl_val, bl_loss)\n",
        "        # Calculate loss\n",
        "        advantage = out_policy['cost'] - bl_val\n",
        "        reinforce_loss = (advantage * out_policy['log_likelihood']).mean()\n",
        "        loss = reinforce_loss + bl_loss\n",
        "\n",
        "        return {'loss': loss, 'reinforce_loss': reinforce_loss, 'bl_loss': bl_loss, 'bl_val': bl_val, **out_policy}\n",
        "    \n",
        "    def setup(self, pl_module):\n",
        "        # Make baseline taking model itself and train_dataloader from model as input\n",
        "        # TODO make this as taken from config\n",
        "        self.baseline = instantiate({\"_target_\": \"__main__.WarmupBaseline\",\n",
        "                                    \"baseline\": {\"_target_\": \"__main__.RolloutBaseline\",                                             }\n",
        "                                    })   \n",
        "        self.baseline.setup(self.policy, pl_module.val_dataloader())                          \n",
        "        # self.baseline = NoBaseline()\n",
        "\n",
        "    def on_train_epoch_end(self, pl_module):\n",
        "        self.baseline.epoch_callback(self.policy, pl_module.val_dataloader(), pl_module.current_epoch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCOLightningModule(L.LightningModule):\n",
        "    def __init__(self, env, model, lr=1e-4, batch_size=128, train_size=1000, val_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=\"fit\"):\n",
        "        self.train_dataset = self._get_dataset(self.train_size)\n",
        "        self.val_dataset = self._get_dataset(self.val_size)\n",
        "        if hasattr(self.model, \"setup\"):\n",
        "            self.model.setup(self)\n",
        "\n",
        "    def shared_step(self, batch: Any, batch_idx: int, phase: str):\n",
        "        output = self.model(batch, phase)\n",
        "        self.log(f\"{phase}/cost\", output[\"cost\"].mean(), prog_bar=True)\n",
        "        return {\"loss\": output['loss']}\n",
        "\n",
        "    def training_step(self, batch: Any, batch_idx: int):    \n",
        "        return self.shared_step(batch, batch_idx, phase='train')\n",
        "\n",
        "    def validation_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='val')\n",
        "\n",
        "    def test_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        # TODO: scheduler\n",
        "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_steps)\n",
        "        return [optim] #, [scheduler]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_dataset)\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if hasattr(self.model, \"on_train_epoch_end\"):\n",
        "            self.model.on_train_epoch_end(self)\n",
        "        # sample new dataset\n",
        "        self.train_dataset = self._get_dataset(self.train_size) \n",
        "\n",
        "    def _get_dataset(self, size):\n",
        "        # online data generation: we generate a new batch online\n",
        "        data = self.env.gen_params(batch_size=size)\n",
        "        return TorchDictDataset(self.env.reset(data))\n",
        "           \n",
        "    def _dataloader(self, dataset):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "            num_workers=0,\n",
        "            collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 1 env = TSPEnv(n_loc=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span>)                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>env = env.transform()                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>policy = AttentionModelBase(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 │   </span>env,                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'TSPEnv'</span> is not defined\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 1 env = TSPEnv(n_loc=\u001b[94m20\u001b[0m)                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0menv = env.transform()                                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0mpolicy = AttentionModelBase(                                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0menv,                                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'TSPEnv'\u001b[0m is not defined\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = TSPEnv(n_loc=20)\n",
        "env = env.transform()\n",
        "policy = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        ")\n",
        "\n",
        "model_final = AttentionModel(env, policy)\n",
        "\n",
        "# # TODO CHANGE THIS\n",
        "\n",
        "model = NCOLightningModule(env, model_final, batch_size=512, train_size=1280000, lr=1e-4)\n",
        "\n",
        "# Trick to make calculations faster\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# Wandb Logger - we can use others as well as simply `None`\n",
        "# logger = pl.loggers.WandbLogger(project=\"torchrl\", name=\"pendulum\")\n",
        "# logger = L.loggers.CSVLogger(\"logs\", name=\"tsp\")\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    # precision=\"bf16-mixed\",\n",
        "    accelerator=\"gpu\",\n",
        "    devices=1,\n",
        "    # logger=logger,\n",
        "    log_every_n_steps=1,   \n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
