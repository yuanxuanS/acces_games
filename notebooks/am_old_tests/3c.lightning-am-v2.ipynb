{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TSP Environment - test import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f008ca1b310>>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# rich tracebacks\n",
        "import rich\n",
        "import rich.traceback\n",
        "\n",
        "rich.traceback.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/torchrl/__init__.py:26: UserWarning: failed to set start method to spawn, and current start method for mp is fork.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "\n",
        "from graph_encoder import GraphAttentionEncoder\n",
        "from attention import CrossAttention\n",
        "from utils import CachedLookup, sample_many\n",
        "\n",
        "from reinforce_baselines import *\n",
        "\n",
        "# from ncobench.envs import TSPEnv\n",
        "from notebooks.am.sanity.tsp2 import TSPEnv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from notebooks.am.sanity.tsp2 import TSPEnv\n",
        "\n",
        "\n",
        "env = TSPEnv(n_loc=10)\n",
        "env = env.transform()\n",
        "\n",
        "init_obs = env.reset(env.gen_params(batch_size=100))['observation']\n",
        "\n",
        "new_state = env.reset(init_observation=init_obs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModelFixed(NamedTuple):\n",
        "    \"\"\"\n",
        "    Context for AttentionModel decoder that is fixed during decoding so can be precomputed/cached\n",
        "    This class allows for efficient indexing of multiple Tensors at once\n",
        "    \"\"\"\n",
        "    node_embeddings: torch.Tensor\n",
        "    context_node_projected: torch.Tensor\n",
        "    glimpse_key: torch.Tensor\n",
        "    glimpse_val: torch.Tensor\n",
        "    logit_key: torch.Tensor\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        assert torch.is_tensor(key) or isinstance(key, slice)\n",
        "        return AttentionModelFixed(\n",
        "            node_embeddings=self.node_embeddings[key],\n",
        "            context_node_projected=self.context_node_projected[key],\n",
        "            glimpse_key=self.glimpse_key[:, key],  # dim 0 are the heads\n",
        "            glimpse_val=self.glimpse_val[:, key],  # dim 0 are the heads\n",
        "            logit_key=self.logit_key[key]\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AttentionModelBase\n",
        "\n",
        "Here we declare the `AttentionModelBase`, which is the `nn.Module`:\n",
        "- Given initial states, it returns the solutions and rewards for them\n",
        "- We then wrap the main model with REINFORCE baselines and epoch callbacks to train it (full `AttentionModel`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModelBase(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 env: EnvBase,\n",
        "                 embedding_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 *,\n",
        "                 n_encode_layers: int = 2,\n",
        "                 tanh_clipping: float = 10.,\n",
        "                 mask_inner: bool = True,\n",
        "                 mask_logits: bool = True,\n",
        "                 normalization: str = 'batch',\n",
        "                 n_heads: int = 8,\n",
        "                 checkpoint_encoder: bool = False,\n",
        "                 use_flash_attn: bool = False,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "        super(AttentionModelBase, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_encode_layers = n_encode_layers\n",
        "        self.decode_type = None\n",
        "        self.temp = 1.0\n",
        "        self.env = env\n",
        "\n",
        "        self.tanh_clipping = tanh_clipping\n",
        "\n",
        "        self.mask_inner = mask_inner\n",
        "        self.mask_logits = mask_logits\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.checkpoint_encoder = checkpoint_encoder\n",
        "\n",
        "        # TODO: add extra except TSP\n",
        "        step_context_dim = 2 * embedding_dim  # Embedding of first and last node\n",
        "        node_dim = 2  # x, y\n",
        "        \n",
        "        # Learned input symbols for first action\n",
        "        self.W_placeholder = nn.Parameter(torch.Tensor(2 * embedding_dim))\n",
        "        self.W_placeholder.data.uniform_(-1, 1)  # Placeholder should be in range of activations\n",
        "\n",
        "        self.init_embed = nn.Linear(node_dim, embedding_dim)\n",
        "\n",
        "        self.embedder = GraphAttentionEncoder(\n",
        "            n_heads=n_heads,\n",
        "            embed_dim=embedding_dim,\n",
        "            n_layers=self.n_encode_layers,\n",
        "            normalization=normalization,\n",
        "            use_flash_attn=use_flash_attn,\n",
        "        )\n",
        "        \n",
        "        self.cross_attention = CrossAttention() # NOTE: FlashCrossAttention does not support inner masking!\n",
        "\n",
        "        # For each node we compute (glimpse key, glimpse value, logit key) so 3 * embedding_dim\n",
        "        self.project_node_embeddings = nn.Linear(embedding_dim, 3 * embedding_dim, bias=False)\n",
        "        self.project_fixed_context = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "        self.project_step_context = nn.Linear(step_context_dim, embedding_dim, bias=False)\n",
        "        assert embedding_dim % n_heads == 0\n",
        "        # Note n_heads * val_dim == embedding_dim so input to project_out is embedding_dim\n",
        "        self.project_out = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str = \"train\", decode_type: str = \"sampling\") -> TensorDict:\n",
        "        \"\"\"Given observation, precompute embeddings and rollout\"\"\"\n",
        "\n",
        "        # Set decoding type for policy, can be also greedy\n",
        "        self.decode_type = decode_type\n",
        "\n",
        "        if self.checkpoint_encoder and self.training:  # Only checkpoint if we need gradients\n",
        "            embeddings, _ = checkpoint(self.embedder, self._init_embed(td['observation']))\n",
        "        else:\n",
        "            embeddings, _ = self.embedder(self._init_embed(td['observation']))\n",
        "\n",
        "        # Main rollout\n",
        "        _log_p, actions, td = self._rollout(td, embeddings)\n",
        "        reward = self.env.get_rewards(td['observation'], actions)\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        # DataParallel since sequences can be of different lengths\n",
        "        ll = self._calc_log_likelihood(_log_p, actions, td.get('mask', None))\n",
        "        out = {\"reward\": reward, \"log_likelihood\": ll, \"actions\": actions, \"cost\": -reward}\n",
        "        return out\n",
        "    \n",
        "    def _rollout(self, td, embeddings):\n",
        "\n",
        "        outputs = []\n",
        "        sequences = []\n",
        "\n",
        "        # Compute keys, values for the glimpse and keys for the logits once as they can be reused in every step\n",
        "        fixed = self._precompute(embeddings)\n",
        "\n",
        "        while not td[\"done\"].any(): # NOTE: here we suppose all the batch is done at the same time\n",
        "            \n",
        "            log_p, mask = self._get_log_p(fixed, td)\n",
        "\n",
        "            # Select the indices of the next nodes in the sequences, result (batch_size) long\n",
        "            selected = self._select_node(log_p.exp()[:, 0, :], mask[:, 0, :])  # Squeeze out steps dimension\n",
        "           \n",
        "            td.set(\"action\", selected[:, None])\n",
        "            td = self.env.step(td)['next']\n",
        "\n",
        "            # Collect output of step\n",
        "            outputs.append(log_p[:, 0, :])\n",
        "            sequences.append(selected)\n",
        "\n",
        "        # Collected lists, return Tensor\n",
        "        return torch.stack(outputs, 1), torch.stack(sequences, 1), td\n",
        "\n",
        "    def _calc_log_likelihood(self, _log_p, a, mask):\n",
        "\n",
        "        # Get log_p corresponding to selected actions\n",
        "        log_p = _log_p.gather(2, a.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Optional: mask out actions irrelevant to objective so they do not get reinforced\n",
        "        if mask is not None:\n",
        "            log_p[mask] = 0\n",
        "\n",
        "        assert (log_p > -1000).data.all(), \"Logprobs should not be -inf, check sampling procedure!\"\n",
        "\n",
        "        # Calculate log_likelihood\n",
        "        return log_p.sum(1)\n",
        "\n",
        "    def _init_embed(self, x):\n",
        "        # TODO: others except TSP\n",
        "        return self.init_embed(x)\n",
        "    \n",
        "    def _select_node(self, probs, mask):\n",
        "\n",
        "        assert (probs == probs).all(), \"Probs should not contain any nans\"\n",
        "\n",
        "        if self.decode_type == \"greedy\":\n",
        "            _, selected = probs.max(1)\n",
        "            assert not mask.gather(1, selected.unsqueeze(\n",
        "                -1)).data.any(), \"Decode greedy: infeasible action has maximum probability\"\n",
        "\n",
        "        elif self.decode_type == \"sampling\":\n",
        "            selected = probs.multinomial(1).squeeze(1)\n",
        "\n",
        "            while mask.gather(1, selected.unsqueeze(-1)).data.any():\n",
        "                print('Sampled bad values, resampling!')\n",
        "                selected = probs.multinomial(1).squeeze(1)\n",
        "\n",
        "        else:\n",
        "            assert False, \"Unknown decode type\"\n",
        "        return selected\n",
        "\n",
        "    def _precompute(self, embeddings, num_steps=1):\n",
        "\n",
        "        # The fixed context projection of the graph embedding is calculated only once for efficiency\n",
        "        graph_embed = embeddings.mean(1)\n",
        "        # fixed context = (batch_size, 1, embed_dim) to make broadcastable with parallel timesteps\n",
        "        fixed_context = self.project_fixed_context(graph_embed)[:, None, :]\n",
        "\n",
        "        # The projection of the node embeddings for the attention is calculated once up front\n",
        "        glimpse_key_fixed, glimpse_val_fixed, logit_key_fixed = \\\n",
        "            self.project_node_embeddings(embeddings[:, None, :, :]).chunk(3, dim=-1)\n",
        "        \n",
        "        # No need to rearrange key for logit as there is a single head\n",
        "        fixed_attention_node_data = (\n",
        "            self._make_heads(glimpse_key_fixed, num_steps),\n",
        "            self._make_heads(glimpse_val_fixed, num_steps),\n",
        "            logit_key_fixed.contiguous()\n",
        "        )\n",
        "        return AttentionModelFixed(embeddings, fixed_context, *fixed_attention_node_data)\n",
        "\n",
        "    def _get_log_p(self, fixed, td, normalize=True):\n",
        "        \n",
        "        # Compute query = context node embedding\n",
        "        query = fixed.context_node_projected + \\\n",
        "                self.project_step_context(self._get_parallel_step_context(fixed.node_embeddings, td))\n",
        "\n",
        "        # Compute keys and values for the nodes\n",
        "        glimpse_K, glimpse_V, logit_K = self._get_attention_node_data(fixed, td['observation'])\n",
        "\n",
        "        # Compute the mask\n",
        "        mask = self.env.get_mask(td)\n",
        "\n",
        "        # Compute logits (unnormalized log_p)\n",
        "        log_p, glimpse = self._one_to_many_logits(query, glimpse_K, glimpse_V, logit_K, mask)\n",
        "\n",
        "        if normalize:\n",
        "            log_p = torch.log_softmax(log_p / self.temp, dim=-1)\n",
        "\n",
        "        assert not torch.isnan(log_p).any()\n",
        "\n",
        "        return log_p, mask\n",
        "\n",
        "    def _get_parallel_step_context(self, embeddings, td):\n",
        "        \"\"\"\n",
        "        Returns the context per step, optionally for multiple steps at once (for efficient evaluation of the model)\n",
        "        \n",
        "        :param embeddings: (batch_size, graph_size, embed_dim)\n",
        "        :param prev_a: (batch_size, num_steps)\n",
        "        :param first_a: Only used when num_steps = 1, action of first step or None if first step\n",
        "        :return: (batch_size, num_steps, context_dim)\n",
        "        \"\"\"\n",
        "\n",
        "        current_node = self.env.get_current_node(td)\n",
        "        batch_size, num_steps = current_node.size()\n",
        "\n",
        "        # TODO: add others except TSP\n",
        "        if num_steps == 1:  # We need to special case if we have only 1 step, may be the first or not\n",
        "            if td['i'][0].item() == 0: # TODO check\n",
        "                # First and only step, ignore prev_a (this is a placeholder)\n",
        "                return self.W_placeholder[None, None, :].expand(batch_size, 1, self.W_placeholder.size(-1))\n",
        "            else:\n",
        "                return embeddings.gather(\n",
        "                    1,\n",
        "                    torch.cat((td['first_a'], current_node), 1)[:, :, None].expand(batch_size, 2, embeddings.size(-1))\n",
        "                ).view(batch_size, 1, -1)\n",
        "        # More than one step, assume always starting with first\n",
        "        embeddings_per_step = embeddings.gather(\n",
        "            1,\n",
        "            current_node[:, 1:, None].expand(batch_size, num_steps - 1, embeddings.size(-1))\n",
        "        )\n",
        "        return torch.cat((\n",
        "            # First step placeholder, cat in dim 1 (time steps)\n",
        "            self.W_placeholder[None, None, :].expand(batch_size, 1, self.W_placeholder.size(-1)),\n",
        "            # Second step, concatenate embedding of first with embedding of current/previous (in dim 2, context dim)\n",
        "            torch.cat((\n",
        "                embeddings_per_step[:, 0:1, :].expand(batch_size, num_steps - 1, embeddings.size(-1)),\n",
        "                embeddings_per_step\n",
        "            ), 2)\n",
        "        ), 1)\n",
        "\n",
        "    def _one_to_many_logits(self, query, key, value, logit_K, mask):\n",
        "\n",
        "        batch_size, num_steps, embed_dim = query.size()\n",
        "        key_size = val_size = embed_dim // self.n_heads\n",
        "\n",
        "        # TODO: no need to rearrange with refactor\n",
        "        kv = torch.stack([key, value])\n",
        "        q = rearrange(query, 'b 1 (h s) -> b 1 h s', h=self.n_heads)\n",
        "        kv = rearrange(kv, 'two h b 1 g s -> b g two h s', two=2, h=self.n_heads)     \n",
        "\n",
        "        # 1 means to keep, so we invert the mask\n",
        "        key_padding_mask = ~mask.squeeze()\n",
        "        heads = self.cross_attention(q, kv, key_padding_mask=key_padding_mask)\n",
        "\n",
        "        heads = rearrange(heads, 'b 1 h g -> h b 1 1 g')\n",
        "\n",
        "        # Project to get glimpse/updated context node embedding (batch_size, num_steps, embedding_dim)\n",
        "        glimpse = self.project_out(\n",
        "            heads.permute(1, 2, 3, 0, 4).contiguous().view(-1, num_steps, 1, self.n_heads * val_size))\n",
        "        \n",
        "        # Batch matrix multiplication to compute logits (batch_size, num_steps, graph_size)\n",
        "        # logits = 'compatibility'\n",
        "        logits = torch.matmul(glimpse, logit_K.transpose(-2, -1)).squeeze(-2) / math.sqrt(glimpse.size(-1))\n",
        "\n",
        "        # From the logits compute the probabilities by clipping, masking and softmax\n",
        "        if self.tanh_clipping > 0:\n",
        "            logits = torch.tanh(logits) * self.tanh_clipping\n",
        "        if self.mask_logits:\n",
        "            logits[mask] = -math.inf\n",
        "\n",
        "        return logits, glimpse.squeeze(-2)\n",
        "\n",
        "    def _get_attention_node_data(self, fixed: AttentionModelFixed, td: TensorDict) -> dict:\n",
        "        # TODO: add others except TSP\n",
        "        # TSP or VRP without split delivery\n",
        "        return fixed.glimpse_key, fixed.glimpse_val, fixed.logit_key\n",
        "\n",
        "    def _make_heads(self, v, num_steps=None):\n",
        "        # TODO: refactor so no need for rearrange\n",
        "        assert num_steps is None or v.size(1) == 1 or v.size(1) == num_steps\n",
        "\n",
        "        return (\n",
        "            v.contiguous().view(v.size(0), v.size(1), v.size(2), self.n_heads, -1)\n",
        "            .expand(v.size(0), v.size(1) if num_steps is None else num_steps, v.size(2), self.n_heads, -1)\n",
        "            .permute(3, 0, 1, 2, 4)  # (n_heads, batch_size, num_steps, graph_size, head_dim)\n",
        "        )\n",
        "    \n",
        "    def precompute_fixed(self, x):\n",
        "        embeddings, _ = self.embedder(self._init_embed(x))\n",
        "        # Use a CachedLookup such that if we repeatedly index this object with the same index we only need to do\n",
        "        # the lookup once... this is the case if all elements in the batch have maximum batch size\n",
        "        return CachedLookup(self._precompute(embeddings))\n",
        "\n",
        "    def sample_many(self, x, batch_rep=1, iter_rep=1):\n",
        "        # Bit ugly but we need to pass the embeddings as well.\n",
        "        # Making a tuple will not work with the problem.get_cost function\n",
        "        return sample_many(\n",
        "            lambda x: self._inner(*x),  # Need to unpack tuple into arguments\n",
        "            lambda x, pi: self.problem.get_costs(x[0], pi),  # Don't need embeddings as input to get_costs\n",
        "            (x, self.embedder(self._init_embed(x))[0]),  # Pack input with embeddings (additional input)\n",
        "            batch_rep, iter_rep\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test `AttentionModelBase`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(9.9978, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "class TorchDictDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx] # note: use torch.stack to get batch\n",
        "    \n",
        "\n",
        "env = TSPEnv(n_loc=20)\n",
        "env = env.transform()\n",
        "\n",
        "data = env.gen_params(batch_size=[10000]) # NOTE: need to put batch_size in a list!!\n",
        "init_td = env.reset(data)\n",
        "dataset = TorchDictDataset(init_td)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=128,\n",
        "                shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "                num_workers=0,\n",
        "                collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            )\n",
        "\n",
        "\n",
        "model = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model, backend=\"cuda\")\n",
        "\n",
        "x = next(iter(dataloader)).to(\"cuda\")\n",
        "\n",
        "out = model(x, decode_type=\"sampling\")\n",
        "\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "res = []\n",
        "for x in dataloader:\n",
        "    x = x.to(\"cuda\")\n",
        "    res.append(- model(x, decode_type=\"sampling\")['reward'])\n",
        "\n",
        "\n",
        "print(torch.cat(res).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionModel(nn.Module):\n",
        "    def __init__(self, env, policy):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        # self.policy = instantiate(cfg.policy)\n",
        "        # self.baseline = instantiate(cfg.baseline) TODO\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str=\"train\", decode_type: str=None) -> TensorDict:\n",
        "        \"\"\"Evaluate model, get costs and log probabilities and compare with baseline\"\"\"\n",
        "\n",
        "        # Evaluate model, get costs and log probabilities\n",
        "        out_policy = self.policy(td)\n",
        "        bl_val, bl_loss = self.baseline.eval(td, out_policy['cost'])\n",
        "\n",
        "        # print(bl_val, bl_loss)\n",
        "        # Calculate loss\n",
        "        advantage = out_policy['cost'] - bl_val\n",
        "        reinforce_loss = (advantage * out_policy['log_likelihood']).mean()\n",
        "        loss = reinforce_loss + bl_loss\n",
        "\n",
        "        return {'loss': loss, 'reinforce_loss': reinforce_loss, 'bl_loss': bl_loss, 'bl_val': bl_val, **out_policy}\n",
        "    \n",
        "    def setup(self, pl_module):\n",
        "        # Make baseline taking model itself and train_dataloader from model as input\n",
        "        # TODO make this as taken from config\n",
        "        self.baseline = instantiate({\"_target_\": \"__main__.WarmupBaseline\",\n",
        "                                    \"baseline\": {\"_target_\": \"__main__.RolloutBaseline\",                                             }\n",
        "                                    })   \n",
        "        self.baseline.setup(self.policy, pl_module.val_dataloader(), self.env)                         \n",
        "        # self.baseline = NoBaseline()\n",
        "\n",
        "    def on_train_epoch_end(self, pl_module):\n",
        "        self.baseline.epoch_callback(self.policy, pl_module.val_dataloader(), pl_module.current_epoch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCOLightningModule(L.LightningModule):\n",
        "    def __init__(self, env, model, lr=1e-4, batch_size=128, train_size=1000, val_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self, stage=\"fit\"):\n",
        "        self.train_dataset = self.get_observation_dataset(self.train_size)\n",
        "        self.val_dataset = self.get_observation_dataset(self.val_size)\n",
        "        if hasattr(self.model, \"setup\"):\n",
        "            self.model.setup(self)\n",
        "\n",
        "    def shared_step(self, batch: Any, batch_idx: int, phase: str):\n",
        "        td = self.env.reset(init_observation=batch)\n",
        "        output = self.model(td, phase)\n",
        "        \n",
        "        # output = self.model(batch, phase)\n",
        "        self.log(f\"{phase}/cost\", output[\"cost\"].mean(), prog_bar=True)\n",
        "        return {\"loss\": output['loss']}\n",
        "\n",
        "    def training_step(self, batch: Any, batch_idx: int):    \n",
        "        return self.shared_step(batch, batch_idx, phase='train')\n",
        "\n",
        "    def validation_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='val')\n",
        "\n",
        "    def test_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n",
        "        # TODO: scheduler\n",
        "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_steps)\n",
        "        return [optim] #, [scheduler]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_dataset)\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if hasattr(self.model, \"on_train_epoch_end\"):\n",
        "            self.model.on_train_epoch_end(self)\n",
        "        self.train_dataset = self.get_observation_dataset(self.train_size) \n",
        "\n",
        "    # def get_observation_dataset(self, size):\n",
        "    #     # online data generation: we generate a new batch online\n",
        "    #     data = self.env.gen_params(batch_size=size)\n",
        "    #     return TorchDictDataset(self.env.reset(data))\n",
        "\n",
        "    def get_observation_dataset(self, size):\n",
        "        # online data generation: we generate a new batch online\n",
        "        data = self.env.gen_params(batch_size=size)\n",
        "        return TorchDictDataset(self.env.reset(data)['observation'])\n",
        "       \n",
        "    def _dataloader(self, dataset):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "            num_workers=0,\n",
        "            collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating baseline model on evaluation dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating baseline model on evaluation dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | env   | TransformedEnv | 0     \n",
            "1 | model | AttentionModel | 1.4 M \n",
            "-----------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.681     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:  15%|█▌        | 375/2500 [00:28<02:41, 13.18it/s, v_num=80, train/cost=4.140]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "env = TSPEnv(n_loc=20)\n",
        "env = env.transform()\n",
        "policy = AttentionModelBase(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        ")\n",
        "\n",
        "model_final = AttentionModel(env, policy)\n",
        "\n",
        "# # TODO CHANGE THIS\n",
        "\n",
        "model = NCOLightningModule(env, model_final, batch_size=512, train_size=1280000, lr=1e-4)\n",
        "\n",
        "# Trick to make calculations faster\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# Wandb Logger - we can use others as well as simply `None`\n",
        "# logger = pl.loggers.WandbLogger(project=\"torchrl\", name=\"pendulum\")\n",
        "# logger = L.loggers.CSVLogger(\"logs\", name=\"tsp\")\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    # precision=\"bf16-mixed\",\n",
        "    accelerator=\"gpu\",\n",
        "    devices=1,\n",
        "    # logger=logger,\n",
        "    log_every_n_steps=1,   \n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
