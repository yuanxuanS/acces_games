{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PointerNet Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/torchrl/__init__.py:26: UserWarning: failed to set start method to spawn, and current start method for mp is fork.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "\n",
        "from ncobench.data.dataset import TorchDictDataset\n",
        "\n",
        "from ncobench.envs.tsp import TSPEnv\n",
        "from ncobench.models.rl.reinforce import *\n",
        "from ncobench.models.components.am.context import env_context\n",
        "from ncobench.models.components.am.embeddings import env_init_embedding, env_dynamic_embedding\n",
        "from ncobench.models.components.am.encoder import GraphAttentionEncoder\n",
        "from ncobench.models.components.am.decoder import Decoder, decode_probs, PrecomputedCache, LogitAttention\n",
        "from ncobench.models.components.am.policy import get_log_likelihood\n",
        "from ncobench.models.nn.attention import NativeFlashMHA, flash_attn_wrapper\n",
        "from ncobench.utils.lightning import get_lightning_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Maps a graph represented as an input sequence\n",
        "    to a hidden vector\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "        self.init_hx, self.init_cx = self.init_hidden(hidden_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        output, hidden = self.lstm(x, hidden)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, hidden_dim):\n",
        "        \"\"\"Trainable initial hidden state\"\"\"\n",
        "        std = 1. / math.sqrt(hidden_dim)\n",
        "        enc_init_hx = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
        "        enc_init_hx.data.uniform_(-std, std)\n",
        "\n",
        "        enc_init_cx = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
        "        enc_init_cx.data.uniform_(-std, std)\n",
        "        return enc_init_hx, enc_init_cx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"A generic attention module for a decoder in seq2seq\"\"\"\n",
        "    def __init__(self, dim, use_tanh=False, C=10):\n",
        "        super(Attention, self).__init__()\n",
        "        self.use_tanh = use_tanh\n",
        "        self.project_query = nn.Linear(dim, dim)\n",
        "        self.project_ref = nn.Conv1d(dim, dim, 1, 1)\n",
        "        self.C = C  # tanh exploration\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.v = nn.Parameter(torch.FloatTensor(dim))\n",
        "        self.v.data.uniform_(-(1. / math.sqrt(dim)), 1. / math.sqrt(dim))\n",
        "        \n",
        "    def forward(self, query, ref):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            query: is the hidden state of the decoder at the current\n",
        "                time step. batch x dim\n",
        "            ref: the set of hidden states from the encoder. \n",
        "                sourceL x batch x hidden_dim\n",
        "        \"\"\"\n",
        "        # ref is now [batch_size x hidden_dim x sourceL]\n",
        "        ref = ref.permute(1, 2, 0)\n",
        "        q = self.project_query(query).unsqueeze(2)  # batch x dim x 1\n",
        "        e = self.project_ref(ref)  # batch_size x hidden_dim x sourceL \n",
        "        # expand the query by sourceL\n",
        "        # batch x dim x sourceL\n",
        "        expanded_q = q.repeat(1, 1, e.size(2)) \n",
        "        # batch x 1 x hidden_dim\n",
        "        v_view = self.v.unsqueeze(0).expand(\n",
        "                expanded_q.size(0), len(self.v)).unsqueeze(1)\n",
        "        # [batch_size x 1 x hidden_dim] * [batch_size x hidden_dim x sourceL]\n",
        "        u = torch.bmm(v_view, self.tanh(expanded_q + e)).squeeze(1)\n",
        "        if self.use_tanh:\n",
        "            logits = self.C * self.tanh(u)\n",
        "        else:\n",
        "            logits = u  \n",
        "        return e, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "            embedding_dim: int = 128,\n",
        "            hidden_dim: int = 128,\n",
        "            tanh_exploration: float = 10.0,\n",
        "            use_tanh: bool = True,\n",
        "            n_glimpses=1,\n",
        "            mask_glimpses=True,\n",
        "            mask_logits=True):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_glimpses = n_glimpses\n",
        "        self.mask_glimpses = mask_glimpses\n",
        "        self.mask_logits = mask_logits\n",
        "        self.use_tanh = use_tanh\n",
        "        self.tanh_exploration = tanh_exploration\n",
        "\n",
        "        self.lstm = nn.LSTMCell(embedding_dim, hidden_dim)\n",
        "        self.pointer = Attention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration)\n",
        "        self.glimpse = Attention(hidden_dim, use_tanh=False)\n",
        "        self.sm = nn.Softmax(dim=1)\n",
        "\n",
        "    def update_mask(self, mask, selected):\n",
        "        return mask.clone().scatter_(1, selected.unsqueeze(-1), True)\n",
        "\n",
        "    def recurrence(self, x, h_in, prev_mask, prev_idxs, step, context):\n",
        "\n",
        "        logit_mask = self.update_mask(prev_mask, prev_idxs) if prev_idxs is not None else prev_mask\n",
        "\n",
        "        logits, h_out = self.calc_logits(x, h_in, logit_mask, context, self.mask_glimpses, self.mask_logits)\n",
        "\n",
        "        # Calculate log_softmax for better numerical stability\n",
        "        log_p = torch.log_softmax(logits, dim=1)\n",
        "        probs = log_p.exp()\n",
        "\n",
        "        if not self.mask_logits:\n",
        "            # If self.mask_logits, this would be redundant, otherwise we must mask to make sure we don't resample\n",
        "            # Note that as a result the vector of probs may not sum to one (this is OK for .multinomial sampling)\n",
        "            # But practically by not masking the logits, a model is learned over all sequences (also infeasible)\n",
        "            # while only during sampling feasibility is enforced (a.k.a. by setting to 0. here)\n",
        "            probs[logit_mask] = 0.\n",
        "            # For consistency we should also mask out in log_p, but the values set to 0 will not be sampled and\n",
        "            # Therefore not be used by the reinforce estimator\n",
        "\n",
        "        return h_out, log_p, probs, logit_mask\n",
        "\n",
        "    def calc_logits(self, x, h_in, logit_mask, context, mask_glimpses=None, mask_logits=None):\n",
        "\n",
        "        if mask_glimpses is None:\n",
        "            mask_glimpses = self.mask_glimpses\n",
        "\n",
        "        if mask_logits is None:\n",
        "            mask_logits = self.mask_logits\n",
        "\n",
        "        hy, cy = self.lstm(x, h_in)\n",
        "        g_l, h_out = hy, (hy, cy)\n",
        "\n",
        "        for i in range(self.n_glimpses):\n",
        "            ref, logits = self.glimpse(g_l, context)\n",
        "            # For the glimpses, only mask before softmax so we have always an L1 norm 1 readout vector\n",
        "            if mask_glimpses:\n",
        "                logits[logit_mask] = float('-inf')\n",
        "            # [batch_size x h_dim x sourceL] * [batch_size x sourceL x 1] =\n",
        "            # [batch_size x h_dim x 1]\n",
        "            g_l = torch.bmm(ref, self.sm(logits).unsqueeze(2)).squeeze(2)\n",
        "        _, logits = self.pointer(g_l, context)\n",
        "\n",
        "        # Masking before softmax makes probs sum to one\n",
        "        if mask_logits:\n",
        "            logits[logit_mask] = float('-inf')\n",
        "\n",
        "        return logits, h_out\n",
        "\n",
        "    def forward(self, decoder_input, embedded_inputs, hidden, context, decode_type=\"sampling\", eval_tours=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            decoder_input: The initial input to the decoder\n",
        "                size is [batch_size x embedding_dim]. Trainable parameter.\n",
        "            embedded_inputs: [sourceL x batch_size x embedding_dim]\n",
        "            hidden: the prev hidden state, size is [batch_size x hidden_dim]. \n",
        "                Initially this is set to (enc_h[-1], enc_c[-1])\n",
        "            context: encoder outputs, [sourceL x batch_size x hidden_dim] \n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = context.size(1)\n",
        "        outputs = []\n",
        "        selections = []\n",
        "        steps = range(embedded_inputs.size(0))\n",
        "        idxs = None\n",
        "        mask = Variable(\n",
        "            embedded_inputs.data.new().byte().new(embedded_inputs.size(1), embedded_inputs.size(0)).zero_(),\n",
        "            requires_grad=False\n",
        "        )\n",
        "\n",
        "        for i in steps:\n",
        "            hidden, log_p, probs, mask = self.recurrence(decoder_input, hidden, mask, idxs, i, context)\n",
        "            # select the next inputs for the decoder [batch_size x hidden_dim]\n",
        "            idxs = decode_probs(\n",
        "                probs,\n",
        "                mask,\n",
        "                decode_type=decode_type\n",
        "            ) if eval_tours is None else eval_tours[:, i]\n",
        "\n",
        "            idxs = idxs.detach()  # Otherwise pytorch complains it want's a reward, todo implement this more properly?\n",
        "\n",
        "            # Gather input embedding of selected\n",
        "            decoder_input = torch.gather(\n",
        "                embedded_inputs,\n",
        "                0,\n",
        "                idxs.contiguous().view(1, batch_size, 1).expand(1, batch_size, *embedded_inputs.size()[2:])\n",
        "            ).squeeze(0)\n",
        "\n",
        "            # use outs to point to next object\n",
        "            outputs.append(log_p)\n",
        "            selections.append(idxs)\n",
        "        return (torch.stack(outputs, 1), torch.stack(selections, 1)), hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointerNetworkPolicy(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 embedding_dim: int=128,\n",
        "                 hidden_dim: int=128,\n",
        "                 tanh_clipping=10.,\n",
        "                 mask_inner=True,\n",
        "                 mask_logits=True,\n",
        "                 **kwargs):\n",
        "        super(PointerNetworkPolicy, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        assert self.env.name == \"tsp\", \"Only the Euclidean TSP env supported\"\n",
        "\n",
        "        self.input_dim = 2\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            embedding_dim,\n",
        "            hidden_dim)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            tanh_exploration=tanh_clipping,\n",
        "            use_tanh=tanh_clipping > 0,\n",
        "            n_glimpses=1,\n",
        "            mask_glimpses=mask_inner,\n",
        "            mask_logits=mask_logits\n",
        "        )\n",
        "\n",
        "        # Trainable initial hidden states\n",
        "        std = 1. / math.sqrt(embedding_dim)\n",
        "        self.decoder_in_0 = nn.Parameter(torch.FloatTensor(embedding_dim))\n",
        "        self.decoder_in_0.data.uniform_(-std, std)\n",
        "\n",
        "        self.embedding = nn.Parameter(torch.FloatTensor(self.input_dim, embedding_dim))\n",
        "        self.embedding.data.uniform_(-std, std)\n",
        "\n",
        "    def forward(self, td, phase: str = \"train\", decode_type=\"sampling\", eval_tours=None):\n",
        "        batch_size, graph_size, input_dim = td['observation'].size()\n",
        "\n",
        "        embedded_inputs = torch.mm(\n",
        "            td['observation'].transpose(0, 1).contiguous().view(-1, input_dim),\n",
        "            self.embedding\n",
        "        ).view(graph_size, batch_size, -1)\n",
        "\n",
        "        # query the actor net for the input indices \n",
        "        # making up the output, and the pointer attn \n",
        "        _log_p, actions = self._inner(embedded_inputs, decode_type, eval_tours)\n",
        "\n",
        "        reward = self.env.get_reward(td[\"observation\"], actions)\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        # DataParallel since sequences can be of different lengths\n",
        "        ll = self._calc_log_likelihood(_log_p, actions, td.get(\"mask\", None))\n",
        "\n",
        "        out = {\"reward\": reward, \"log_likelihood\": ll, \"actions\": actions}\n",
        "        return out\n",
        "    \n",
        "    def _calc_log_likelihood(self, _log_p, a, mask):\n",
        "\n",
        "        # Get log_p corresponding to selected actions\n",
        "        log_p = _log_p.gather(2, a.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Optional: mask out actions irrelevant to objective so they do not get reinforced\n",
        "        if mask is not None:\n",
        "            log_p[mask] = 0\n",
        "\n",
        "        assert (log_p > -1000).data.all(), \"Logprobs should not be -inf, check sampling procedure!\"\n",
        "\n",
        "        # Calculate log_likelihood\n",
        "        return log_p.sum(1)\n",
        "\n",
        "    def _inner(self, inputs, decode_type=\"sampling\", eval_tours=None):\n",
        "\n",
        "        encoder_hx = encoder_cx = Variable(\n",
        "            torch.zeros(1, inputs.size(1), self.encoder.hidden_dim, out=inputs.data.new()),\n",
        "            requires_grad=False\n",
        "        )\n",
        "\n",
        "        # encoder forward pass\n",
        "        enc_h, (enc_h_t, enc_c_t) = self.encoder(inputs, (encoder_hx, encoder_cx))\n",
        "\n",
        "        dec_init_state = (enc_h_t[-1], enc_c_t[-1])\n",
        "\n",
        "        # repeat decoder_in_0 across batch\n",
        "        decoder_input = self.decoder_in_0.unsqueeze(0).repeat(inputs.size(1), 1)\n",
        "\n",
        "        (pointer_probs, input_idxs), dec_hidden_t = self.decoder(decoder_input,\n",
        "                                                                 inputs,\n",
        "                                                                 dec_init_state,\n",
        "                                                                 enc_h,\n",
        "                                                                 decode_type,\n",
        "                                                                 eval_tours)\n",
        "\n",
        "        return pointer_probs, input_idxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_819401/639796701.py:64: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/cuda/Indexing.cu:1440.)\n",
            "  logits[logit_mask] = float('-inf')\n",
            "/tmp/ipykernel_819401/639796701.py:72: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/cuda/Indexing.cu:1440.)\n",
            "  logits[logit_mask] = float('-inf')\n"
          ]
        }
      ],
      "source": [
        "num_loc = 15\n",
        "env = TSPEnv(num_loc=num_loc).transform()\n",
        "\n",
        "# data = env.gen_params(batch_size=[10000]) # NOTE: need to put batch_size in a list!!\n",
        "# init_td = env.reset(data)\n",
        "# env.batch_size = [10000]\n",
        "init_td = env.reset(batch_size=[10000])\n",
        "dataset = TorchDictDataset(init_td)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=32,\n",
        "                shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "                num_workers=0,\n",
        "                collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            )\n",
        "\n",
        "model = PointerNetworkPolicy(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model)\n",
        "\n",
        "x = next(iter(dataloader)).to(\"cuda\")\n",
        "\n",
        "out = model(x, decode_type=\"sampling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 15])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.Size([15, 32, 15]) torch.Size([32])\n",
        "# size of a, and idx\n",
        "\n",
        "def gather_by_index(source, index, dim=0):\n",
        "    target = torch.gather(\n",
        "        source, dim, index[:, None, None].expand(-1, -1, source.size(-1))\n",
        "    )\n",
        "    return target\n",
        "\n",
        "a = torch.randn(15, 32, 15)\n",
        "idx = torch.randint(0, 15, (32,))\n",
        "# get idx so that out is 1, 32, 15\n",
        "out = gather_by_index(a, idx, dim=0)\n",
        "\n",
        "out.shape\n",
        "# a.gather(0, idx[None, :, None]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointerNetwork(nn.Module):\n",
        "    def __init__(self, env, policy):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        # self.policy = instantiate(cfg.policy)\n",
        "        # self.baseline = instantiate(cfg.baseline) TODO\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str=\"train\", decode_type: str=None) -> TensorDict:\n",
        "        \"\"\"Evaluate model, get costs and log probabilities and compare with baseline\"\"\"\n",
        "\n",
        "        # Evaluate modelim=0, get costs and log probabilities\n",
        "        out_policy = self.policy(td)\n",
        "        cost = -out_policy['reward']\n",
        "        ll = out_policy['log_likelihood']\n",
        "\n",
        "        # Calculate loss\n",
        "        bl_val, bl_loss = self.baseline.eval(td, cost)\n",
        "\n",
        "        advantage = cost - bl_val\n",
        "        reinforce_loss = (advantage * ll).mean()\n",
        "        loss = reinforce_loss + bl_loss\n",
        "\n",
        "        return {'loss': loss, 'reinforce_loss': reinforce_loss, 'bl_loss': bl_loss, 'bl_val': bl_val, **out_policy}\n",
        "    \n",
        "    # def setup(self, lit_module):\n",
        "    #     # Make baseline taking model itself and train_dataloader from model as input\n",
        "    #     # TODO make this as taken from config\n",
        "    #     self.baseline = instantiate({\"_target_\": \"__main__.SharedBaseline\"})\n",
        "\n",
        "    def setup(self, lit_module):\n",
        "        # Make baseline taking model itself and train_dataloader from model as input\n",
        "        # TODO make this as taken from config\n",
        "        self.baseline = instantiate({\"_target_\": \"__main__.WarmupBaseline\",\n",
        "                                    \"baseline\": {\"_target_\": \"__main__.RolloutBaseline\",                                             }\n",
        "                                    })  \n",
        "        # self.baseline = instantiate({\"_target_\": \"__main__.SharedBaseline\"})\n",
        "\n",
        "        self.baseline.setup(self.policy, lit_module.val_dataloader(), self.env, device=get_lightning_device(lit_module))         \n",
        "        # self.baseline = NoBaseline()\n",
        "\n",
        "    def on_train_epoch_end(self, lit_module):\n",
        "        # self.baseline.epoch_callback(self.policy, self.env, pl_module)\n",
        "        self.baseline.epoch_callback(self.policy, lit_module.val_dataloader(), lit_module.current_epoch, self.env, device=get_lightning_device(lit_module))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCOLightningModule(L.LightningModule):\n",
        "    def __init__(self, env, model, lr=1e-4, batch_size=128, train_size=1000, val_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def setup(self, stage=\"fit\"):\n",
        "        self.train_dataset = self.get_dataset(self.train_size)\n",
        "        self.val_dataset = self.get_dataset(self.val_size)\n",
        "        if hasattr(self.model, \"setup\"):\n",
        "            self.model.setup(self)\n",
        "\n",
        "    def shared_step(self, batch: Any, batch_idx: int, phase: str):\n",
        "        td = self.env.reset(batch)\n",
        "        output = self.model(td, phase)\n",
        "        \n",
        "        # output = self.model(batch, phase)\n",
        "        self.log(f\"{phase}/cost\", -output[\"reward\"].mean(), prog_bar=True)\n",
        "        return {\"loss\": output['loss']}\n",
        "\n",
        "    def training_step(self, batch: Any, batch_idx: int):   \n",
        "        return self.shared_step(batch, batch_idx, phase='train')\n",
        "\n",
        "    def validation_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='val')\n",
        "\n",
        "    def test_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-6)\n",
        "        # optim = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "        # TODO: scheduler\n",
        "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_steps)\n",
        "        return [optim] #, [scheduler]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_dataset)\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if hasattr(self.model, \"on_train_epoch_end\"):\n",
        "            self.model.on_train_epoch_end(self)\n",
        "        self.train_dataset = self.get_dataset(self.train_size) \n",
        "\n",
        "    def get_dataset(self, size):\n",
        "        # online data generation: we generate a new batch online\n",
        "        # data = self.env.gen_params(batch_size=size)\n",
        "        return TorchDictDataset(self.env.generate_data(size))\n",
        "       \n",
        "    def _dataloader(self, dataset):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "            num_workers=0,\n",
        "            collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            pin_memory=self.on_gpu,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fd5a8d59e70>>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rich.traceback import install\n",
        "install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/tmp/ipykernel_819401/639796701.py:64: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/cuda/Indexing.cu:1440.)\n",
            "  logits[logit_mask] = float('-inf')\n",
            "/tmp/ipykernel_819401/639796701.py:72: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/cuda/Indexing.cu:1440.)\n",
            "  logits[logit_mask] = float('-inf')\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | env   | TSPEnv         | 0     \n",
            "1 | model | PointerNetwork | 662 K \n",
            "-----------------------------------------\n",
            "662 K     Trainable params\n",
            "0         Non-trainable params\n",
            "662 K     Total params\n",
            "2.649     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   3%|▎         | 588/20000 [00:39<21:45, 14.87it/s, v_num=3, train/cost=7.650]"
          ]
        }
      ],
      "source": [
        "num_loc = 20\n",
        "env = TSPEnv(num_loc=num_loc).transform()\n",
        "\n",
        "# env = env.transform()\n",
        "policy = PointerNetworkPolicy(\n",
        "    env,\n",
        "    num_pomo=num_loc,\n",
        "    # num_pomo=1,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    # force_flash_attn=True,\n",
        ")\n",
        "\n",
        "model_final = PointerNetwork(env, policy)\n",
        "\n",
        "# # TODO CHANGE THIS\n",
        "batch_size = 64 #1024 #512\n",
        "\n",
        "model = NCOLightningModule(env, model_final, batch_size=batch_size, train_size=1280000, lr=1e-4)\n",
        "\n",
        "# Trick to make calculations faster\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# Wandb Logger - we can use others as well as simply `None`\n",
        "# logger = pl.loggers.WandbLogger(project=\"torchrl\", name=\"pendulum\")\n",
        "# logger = L.loggers.CSVLogger(\"logs\", name=\"tsp\")\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "\n",
        "# from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
        "# callbacks = [DeviceStatsMonitor()]\n",
        "\n",
        "from lightning.pytorch.profilers import AdvancedProfiler\n",
        "\n",
        "profiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logsv2\")\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[1],\n",
        "    # callbacks=callbacks,\n",
        "    # profiler=profiler,\n",
        "    # strategy=\"deepspeed_stage_3_offload\",\n",
        "    # precision=16,\n",
        "    log_every_n_steps=100,   \n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
