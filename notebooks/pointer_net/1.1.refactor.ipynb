{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PointerNet Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/torchrl/__init__.py:26: UserWarning: failed to set start method to spawn, and current start method for mp is fork.\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys; sys.path.append('../../')\n",
        "\n",
        "import math\n",
        "from typing import List, Tuple, Optional, NamedTuple, Dict, Union, Any\n",
        "from einops import rearrange, repeat\n",
        "from hydra.utils import instantiate\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.nn import DataParallel\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import lightning as L\n",
        "\n",
        "from torchrl.envs import EnvBase\n",
        "from torchrl.envs.utils import step_mdp\n",
        "from tensordict import TensorDict\n",
        "from torchrl.modules.models.models import MLP\n",
        "\n",
        "from ncobench.envs.tsp import TSPEnv\n",
        "from ncobench.models.rl.reinforce import *\n",
        "from ncobench.models.co.am.context import env_context\n",
        "from ncobench.models.co.am.embeddings import env_init_embedding, env_dynamic_embedding\n",
        "from ncobench.models.co.am.encoder import GraphAttentionEncoder\n",
        "from ncobench.models.co.am.decoder import Decoder, decode_probs, PrecomputedCache, LogitAttention\n",
        "from ncobench.models.co.am.policy import get_log_likelihood\n",
        "from ncobench.models.nn.attention import NativeFlashMHA, flash_attn_wrapper\n",
        "from ncobench.utils.lightning import get_lightning_device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Maps a graph represented as an input sequence\n",
        "    to a hidden vector\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
        "        self.init_hx, self.init_cx = self.init_hidden(hidden_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        output, hidden = self.lstm(x, hidden)\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, hidden_dim):\n",
        "        \"\"\"Trainable initial hidden state\"\"\"\n",
        "        std = 1. / math.sqrt(hidden_dim)\n",
        "        enc_init_hx = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
        "        enc_init_hx.data.uniform_(-std, std)\n",
        "\n",
        "        enc_init_cx = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
        "        enc_init_cx.data.uniform_(-std, std)\n",
        "        return enc_init_hx, enc_init_cx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleAttention(nn.Module):\n",
        "    \"\"\"A generic attention module for a decoder in seq2seq\"\"\"\n",
        "    def __init__(self, dim, use_tanh=False, C=10):\n",
        "        super(SimpleAttention, self).__init__()\n",
        "        self.use_tanh = use_tanh\n",
        "        self.project_query = nn.Linear(dim, dim)\n",
        "        self.project_ref = nn.Conv1d(dim, dim, 1, 1)\n",
        "        self.C = C  # tanh exploration\n",
        "\n",
        "        self.v = nn.Parameter(torch.FloatTensor(dim))\n",
        "        self.v.data.uniform_(-(1. / math.sqrt(dim)), 1. / math.sqrt(dim))\n",
        "        \n",
        "    def forward(self, query, ref):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            query: is the hidden state of the decoder at the current\n",
        "                time step. batch x dim\n",
        "            ref: the set of hidden states from the encoder. \n",
        "                sourceL x batch x hidden_dim\n",
        "        \"\"\"\n",
        "        # ref is now [batch_size x hidden_dim x sourceL]\n",
        "        ref = ref.permute(1, 2, 0)\n",
        "        q = self.project_query(query).unsqueeze(2)  # batch x dim x 1\n",
        "        e = self.project_ref(ref)  # batch_size x hidden_dim x sourceL \n",
        "        # expand the query by sourceL\n",
        "        # batch x dim x sourceL\n",
        "        expanded_q = q.repeat(1, 1, e.size(2)) \n",
        "        # batch x 1 x hidden_dim\n",
        "        v_view = self.v.unsqueeze(0).expand(\n",
        "                expanded_q.size(0), len(self.v)).unsqueeze(1)\n",
        "        # [batch_size x 1 x hidden_dim] * [batch_size x hidden_dim x sourceL]\n",
        "        u = torch.bmm(v_view, F.tanh(expanded_q + e)).squeeze(1)\n",
        "        if self.use_tanh:\n",
        "            logits = self.C * F.tanh(u)\n",
        "        else:\n",
        "            logits = u  \n",
        "        return e, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "            embedding_dim: int = 128,\n",
        "            hidden_dim: int = 128,\n",
        "            tanh_exploration: float = 10.0,\n",
        "            use_tanh: bool = True,\n",
        "            n_glimpses=1,\n",
        "            mask_glimpses=True,\n",
        "            mask_logits=True):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_glimpses = n_glimpses\n",
        "        self.mask_glimpses = mask_glimpses\n",
        "        self.mask_logits = mask_logits\n",
        "        self.use_tanh = use_tanh\n",
        "        self.tanh_exploration = tanh_exploration\n",
        "\n",
        "        self.lstm = nn.LSTMCell(embedding_dim, hidden_dim)\n",
        "        self.pointer = SimpleAttention(hidden_dim, use_tanh=use_tanh, C=tanh_exploration)\n",
        "        self.glimpse = SimpleAttention(hidden_dim, use_tanh=False)\n",
        "\n",
        "    def update_mask(self, mask, selected):\n",
        "        return mask.clone().scatter_(1, selected.unsqueeze(-1), True)\n",
        "\n",
        "    def recurrence(self, x, h_in, prev_mask, prev_idxs, step, context):\n",
        "\n",
        "        logit_mask = self.update_mask(prev_mask, prev_idxs) if prev_idxs is not None else prev_mask\n",
        "\n",
        "        logits, h_out = self.calc_logits(x, h_in, logit_mask, context, self.mask_glimpses, self.mask_logits)\n",
        "\n",
        "        # Calculate log_softmax for better numerical stability\n",
        "        log_p = torch.log_softmax(logits, dim=1)\n",
        "        probs = log_p.exp()\n",
        "\n",
        "        if not self.mask_logits:\n",
        "            probs[logit_mask] = 0.\n",
        "\n",
        "        return h_out, log_p, probs, logit_mask\n",
        "\n",
        "    def calc_logits(self, x, h_in, logit_mask, context, mask_glimpses=None, mask_logits=None):\n",
        "\n",
        "        if mask_glimpses is None:\n",
        "            mask_glimpses = self.mask_glimpses\n",
        "\n",
        "        if mask_logits is None:\n",
        "            mask_logits = self.mask_logits\n",
        "\n",
        "        hy, cy = self.lstm(x, h_in)\n",
        "        g_l, h_out = hy, (hy, cy)\n",
        "\n",
        "        for i in range(self.n_glimpses):\n",
        "            ref, logits = self.glimpse(g_l, context)\n",
        "            # For the glimpses, only mask before softmax so we have always an L1 norm 1 readout vector\n",
        "            if mask_glimpses:\n",
        "                logits[logit_mask] = float('-inf')\n",
        "            # [batch_size x h_dim x sourceL] * [batch_size x sourceL x 1] =\n",
        "            # [batch_size x h_dim x 1]\n",
        "            g_l = torch.bmm(ref, F.softmax(logits, dim=1).unsqueeze(2)).squeeze(2)\n",
        "        _, logits = self.pointer(g_l, context)\n",
        "\n",
        "        # Masking before softmax makes probs sum to one\n",
        "        if mask_logits:\n",
        "            logits[logit_mask] = float('-inf')\n",
        "\n",
        "        return logits, h_out\n",
        "\n",
        "    def forward(self, decoder_input, embedded_inputs, hidden, context, decode_type=\"sampling\", eval_tours=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            decoder_input: The initial input to the decoder\n",
        "                size is [batch_size x embedding_dim]. Trainable parameter.\n",
        "            embedded_inputs: [sourceL x batch_size x embedding_dim]\n",
        "            hidden: the prev hidden state, size is [batch_size x hidden_dim]. \n",
        "                Initially this is set to (enc_h[-1], enc_c[-1])\n",
        "            context: encoder outputs, [sourceL x batch_size x hidden_dim] \n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = context.size(1)\n",
        "        outputs = []\n",
        "        selections = []\n",
        "        steps = range(embedded_inputs.size(0))\n",
        "        idxs = None\n",
        "        mask = torch.zeros(embedded_inputs.size(1), embedded_inputs.size(0), dtype=torch.bool, device=embedded_inputs.device)\n",
        "\n",
        "        for i in steps:\n",
        "            hidden, log_p, probs, mask = self.recurrence(decoder_input, hidden, mask, idxs, i, context)\n",
        "            # select the next inputs for the decoder [batch_size x hidden_dim]\n",
        "            idxs = decode_probs(\n",
        "                probs,\n",
        "                mask,\n",
        "                decode_type=decode_type\n",
        "            ) if eval_tours is None else eval_tours[:, i]\n",
        "\n",
        "            idxs = idxs.detach()  # Otherwise pytorch complains it want's a reward, todo implement this more properly?\n",
        "\n",
        "            # Gather input embedding of selected\n",
        "            decoder_input = torch.gather(\n",
        "                embedded_inputs,\n",
        "                0,\n",
        "                idxs.contiguous().view(1, batch_size, 1).expand(1, batch_size, *embedded_inputs.size()[2:])\n",
        "            ).squeeze(0)\n",
        "\n",
        "            # use outs to point to next object\n",
        "            outputs.append(log_p)\n",
        "            selections.append(idxs)\n",
        "        return (torch.stack(outputs, 1), torch.stack(selections, 1)), hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointerNetworkPolicy(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 env,\n",
        "                 embedding_dim: int=128,\n",
        "                 hidden_dim: int=128,\n",
        "                 tanh_clipping=10.,\n",
        "                 mask_inner=True,\n",
        "                 mask_logits=True,\n",
        "                 **kwargs):\n",
        "        super(PointerNetworkPolicy, self).__init__()\n",
        "\n",
        "        self.env = env\n",
        "        assert self.env.name == \"tsp\", \"Only the Euclidean TSP env supported\"\n",
        "\n",
        "        self.input_dim = 2\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            embedding_dim,\n",
        "            hidden_dim)\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            tanh_exploration=tanh_clipping,\n",
        "            use_tanh=tanh_clipping > 0,\n",
        "            n_glimpses=1,\n",
        "            mask_glimpses=mask_inner,\n",
        "            mask_logits=mask_logits\n",
        "        )\n",
        "\n",
        "        # Trainable initial hidden states\n",
        "        std = 1. / math.sqrt(embedding_dim)\n",
        "        self.decoder_in_0 = nn.Parameter(torch.FloatTensor(embedding_dim))\n",
        "        self.decoder_in_0.data.uniform_(-std, std)\n",
        "\n",
        "        self.embedding = nn.Parameter(torch.FloatTensor(self.input_dim, embedding_dim))\n",
        "        self.embedding.data.uniform_(-std, std)\n",
        "\n",
        "    def forward(self, td, phase: str = \"train\", decode_type=\"sampling\", eval_tours=None):\n",
        "        batch_size, graph_size, input_dim = td['observation'].size()\n",
        "\n",
        "        embedded_inputs = torch.mm(\n",
        "            td['observation'].transpose(0, 1).contiguous().view(-1, input_dim),\n",
        "            self.embedding\n",
        "        ).view(graph_size, batch_size, -1)\n",
        "\n",
        "        # query the actor net for the input indices \n",
        "        # making up the output, and the pointer attn \n",
        "        _log_p, actions = self._inner(embedded_inputs, decode_type, eval_tours)\n",
        "\n",
        "        reward = self.env.get_reward(td[\"observation\"], actions)\n",
        "\n",
        "        # Log likelyhood is calculated within the model since returning it per action does not work well with\n",
        "        # DataParallel since sequences can be of different lengths\n",
        "        ll = get_log_likelihood(_log_p, actions, td.get(\"mask\", None))\n",
        "\n",
        "        out = {\"reward\": reward, \"log_likelihood\": ll, \"actions\": actions}\n",
        "        return out\n",
        "    \n",
        "\n",
        "    def _inner(self, inputs, decode_type=\"sampling\", eval_tours=None):\n",
        "\n",
        "        encoder_hx = encoder_cx = torch.zeros(1, *inputs.shape[1:], device=inputs.device) #(1, inputs.size(1), self.encoder.hidden_dim, device=inputs.device, out=inputs.data.new(), requires_grad=False)\n",
        "        \n",
        "        # encoder forward pass\n",
        "        enc_h, (enc_h_t, enc_c_t) = self.encoder(inputs, (encoder_hx, encoder_cx))\n",
        "\n",
        "        dec_init_state = (enc_h_t[-1], enc_c_t[-1])\n",
        "\n",
        "        # repeat decoder_in_0 across batch\n",
        "        decoder_input = self.decoder_in_0.unsqueeze(0).repeat(inputs.size(1), 1)\n",
        "\n",
        "        (pointer_probs, input_idxs), dec_hidden_t = self.decoder(decoder_input,\n",
        "                                                                 inputs,\n",
        "                                                                 dec_init_state,\n",
        "                                                                 enc_h,\n",
        "                                                                 decode_type,\n",
        "                                                                 eval_tours)\n",
        "\n",
        "        return pointer_probs, input_idxs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f82dcee3160>>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rich.traceback import install\n",
        "install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_loc = 15\n",
        "env = TSPEnv(num_loc=num_loc).transform()\n",
        "\n",
        "dataset = env.dataset(batch_size=[10000])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "                dataset,\n",
        "                batch_size=32,\n",
        "                shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "                num_workers=0,\n",
        "                collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            )\n",
        "\n",
        "model = PointerNetworkPolicy(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    num_pomo=num_loc,\n",
        "    # force_flash_attn=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "# model = torch.compile(model)\n",
        "\n",
        "x = next(iter(dataloader)).to(\"cuda\")\n",
        "x = env.reset(init_obs=x)\n",
        "\n",
        "out = model(x, decode_type=\"sampling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointerNetwork(nn.Module):\n",
        "    def __init__(self, env, policy, baseline):\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "        self.baseline = baseline\n",
        "\n",
        "    def forward(self, td: TensorDict, phase: str=\"train\", decode_type: str=None) -> TensorDict:\n",
        "        \"\"\"Evaluate model, get costs and log probabilities and compare with baseline\"\"\"\n",
        "\n",
        "        # Evaluate modelim=0, get costs and log probabilities\n",
        "        out_policy = self.policy(td)\n",
        "        cost = -out_policy['reward']\n",
        "        ll = out_policy['log_likelihood']\n",
        "\n",
        "        # Calculate loss\n",
        "        bl_val, bl_loss = self.baseline.eval(td, cost)\n",
        "\n",
        "        advantage = cost - bl_val\n",
        "        reinforce_loss = (advantage * ll).mean()\n",
        "        loss = reinforce_loss + bl_loss\n",
        "\n",
        "        return {'loss': loss, 'reinforce_loss': reinforce_loss, 'bl_loss': bl_loss, 'bl_val': bl_val, **out_policy}\n",
        "    \n",
        "    def setup(self, lit_module):\n",
        "        # Make baseline taking model itself and train_dataloader from model as input\n",
        "        if hasattr(self.baseline, \"setup\"):\n",
        "            self.baseline.setup(self.policy, lit_module.train_dataloader(), self.env, device=get_lightning_device(lit_module))\n",
        "\n",
        "    def on_train_epoch_end(self, lit_module):\n",
        "        # self.baseline.epoch_callback(self.policy, self.env, pl_module)\n",
        "        self.baseline.epoch_callback(self.policy, lit_module.val_dataloader(), lit_module.current_epoch, self.env, device=get_lightning_device(lit_module))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NCOLightningModule(L.LightningModule):\n",
        "    def __init__(self, env, model, lr=1e-4, batch_size=128, train_size=1000, val_size=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: hydra instantiation\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "\n",
        "    def setup(self, stage=\"fit\"):\n",
        "        self.train_dataset = self.env.dataset(self.train_size)\n",
        "        self.val_dataset = self.env.dataset(self.val_size)\n",
        "        if hasattr(self.model, \"setup\"):\n",
        "            self.model.setup(self)\n",
        "\n",
        "    def shared_step(self, batch: Any, batch_idx: int, phase: str):\n",
        "        td = self.env.reset(init_obs=batch)\n",
        "        output = self.model(td, phase)\n",
        "        \n",
        "        # output = self.model(batch, phase)\n",
        "        self.log(f\"{phase}/cost\", -output[\"reward\"].mean(), prog_bar=True)\n",
        "        return {\"loss\": output['loss']}\n",
        "\n",
        "    def training_step(self, batch: Any, batch_idx: int):   \n",
        "        return self.shared_step(batch, batch_idx, phase='train')\n",
        "\n",
        "    def validation_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='val')\n",
        "\n",
        "    def test_step(self, batch: Any, batch_idx: int):\n",
        "        return self.shared_step(batch, batch_idx, phase='test')\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optim = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-6)\n",
        "        # optim = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "        # TODO: scheduler\n",
        "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_steps)\n",
        "        return [optim] #, [scheduler]\n",
        "    \n",
        "    def train_dataloader(self):\n",
        "        return self._dataloader(self.train_dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        return self._dataloader(self.val_dataset)\n",
        "    \n",
        "    def on_train_epoch_end(self):\n",
        "        if hasattr(self.model, \"on_train_epoch_end\"):\n",
        "            self.model.on_train_epoch_end(self)\n",
        "        self.train_dataset = self.env.dataset(self.train_size) \n",
        "       \n",
        "    def _dataloader(self, dataset):\n",
        "        return DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False, # no need to shuffle, we're resampling every epoch\n",
        "            num_workers=0,\n",
        "            collate_fn=torch.stack, # we need this to stack the batches in the dataset\n",
        "            # pin_memory=self.on_gpu,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f82dcee3160>>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from rich.traceback import install\n",
        "install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "epochs = 1\n",
        "batch_size = 512 #1024 #512\n",
        "num_loc = 20\n",
        "train_size = 1280000\n",
        "lr = 1e-4\n",
        "num_pomo = num_loc # TODO: comment to try out = 1\n",
        "# num_pomo = 1 # set to 1: similar to simple AM\n",
        "\n",
        "# Environment\n",
        "env = TSPEnv(num_loc=num_loc).transform()\n",
        "\n",
        "# Policy\n",
        "policy = PointerNetworkPolicy(\n",
        "    env,\n",
        "    embedding_dim=128,\n",
        "    hidden_dim=128,\n",
        "    n_encode_layers=3,\n",
        "    # force_flash_attn=True,\n",
        ")\n",
        "\n",
        "# Baseline\n",
        "baseline = WarmupBaseline(RolloutBaseline())\n",
        "# baseline = SharedBaseline() # TODO: uncomment\n",
        "\n",
        "# Create RL model\n",
        "model = PointerNetwork(env, policy, baseline)\n",
        "\n",
        "# Create Lightning module (for training)\n",
        "lit_model = NCOLightningModule(env, model, batch_size=batch_size, train_size=train_size, lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
            "\n",
            "  | Name  | Type           | Params\n",
            "-----------------------------------------\n",
            "0 | env   | TSPEnv         | 0     \n",
            "1 | model | PointerNetwork | 662 K \n",
            "-----------------------------------------\n",
            "662 K     Trainable params\n",
            "0         Non-trainable params\n",
            "662 K     Total params\n",
            "2.649     Total estimated model params size (MB)\n",
            "2023-04-12 01:02:43.188662: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-12 01:02:43.707475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n",
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:  11%|█         | 275/2500 [00:17<02:25, 15.28it/s, v_num=11, train/cost=7.750]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/botu/Dev/ncobench/env/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "# Trick to make calculations faster\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=epochs,\n",
        "    accelerator=\"gpu\",\n",
        "    devices=[1],\n",
        "    logger=None, # can replace with WandbLogger, TensorBoardLogger, etc.\n",
        "    # precision=16, # uncomment to make faster\n",
        "    log_every_n_steps=100,   \n",
        "    gradient_clip_val=1.0, # clip gradients to avoid exploding gradients!\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "trainer.fit(lit_model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
