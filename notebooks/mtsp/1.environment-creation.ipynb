{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# mTSP Environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Differences with CVRP:\n",
        "\n",
        "- We have a fixed maximum number of vehicles\n",
        "- No capacity constraint\n",
        "- Objective: minimize maximum subtour length (minmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys; sys.path.append(2*\"../\")\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tensordict.tensordict import TensorDict\n",
        "\n",
        "from torchrl.data import (\n",
        "    BoundedTensorSpec,\n",
        "    CompositeSpec,\n",
        "    UnboundedContinuousTensorSpec,\n",
        "    UnboundedDiscreteTensorSpec,\n",
        ")\n",
        "\n",
        "from rl4co.envs.utils import batch_to_scalar\n",
        "from rl4co.envs.base import RL4COEnvBase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _reset(self, td: Optional[TensorDict] = None, batch_size=None) -> TensorDict:\n",
        "    # Initialize data\n",
        "    if batch_size is None:\n",
        "        batch_size = self.batch_size if td is None else td[\"observation\"].shape[:-2]\n",
        "\n",
        "    device = td.device if td is not None else self.device\n",
        "    if td is None or td.is_empty():\n",
        "        td = self.generate_data(batch_size=batch_size)\n",
        "\n",
        "    # Keep track of the agent number to know when to stop\n",
        "    agent_idx = torch.zeros((*batch_size, 1), dtype=torch.int64, device=device)\n",
        "    \n",
        "    # Make variable for max_subtour_length between subtours\n",
        "    max_subtour_length = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "    current_length = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Other variables\n",
        "    current_node = torch.zeros((*batch_size, 1), dtype=torch.int64, device=device)\n",
        "    available = torch.ones(\n",
        "        (*batch_size, self.num_loc), dtype=torch.bool, device=device\n",
        "    )  # 1 means not visited, i.e. action is allowed\n",
        "    available[..., 0] = 0  # Depot is not available as first node\n",
        "    i = torch.zeros((*batch_size, 1), dtype=torch.int64, device=device)\n",
        "    \n",
        "    return TensorDict(\n",
        "        {\n",
        "            \"observation\": td['observation'], # depot is first node\n",
        "            \"num_agents\": td['num_agents'],\n",
        "            \"max_subtour_length\": max_subtour_length,\n",
        "            \"current_length\": current_length,\n",
        "            \"agent_idx\": agent_idx,\n",
        "            \"first_node\": current_node,\n",
        "            \"current_node\": current_node,\n",
        "            \"i\": i,\n",
        "            \"action_mask\": available,\n",
        "        },\n",
        "        batch_size=batch_size,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _step(td: TensorDict) -> TensorDict:\n",
        "    is_first_action = batch_to_scalar(td[\"i\"]) == 0\n",
        "    current_node = td[\"action\"]\n",
        "    first_node = current_node if is_first_action else td[\"first_node\"]\n",
        "    locs = td[\"observation\"]\n",
        "\n",
        "    # If current_node is the depot, then increment agent_idx\n",
        "    cur_agent_idx = td[\"agent_idx\"] + (current_node == 0).long()\n",
        "\n",
        "    # Get cur distance: distance between current_node and td['current_node'] (which is the previous)\n",
        "    cur_dist = torch.norm(locs.gather(-2, current_node.unsqueeze(-1).expand_as(locs)) - locs.gather(-2, td[\"current_node\"].unsqueeze(-1).expand_as(locs)), p=2, dim=-1)\n",
        "\n",
        "    # If current agent is different from previous agent, then we have a new subtour\n",
        "    # We update the max_subtour_length and reset the current_length\n",
        "    max_subtour_length = torch.where(td[\"current_length\"] > td[\"max_subtour_length\"], td[\"current_length\"], td[\"max_subtour_length\"])\n",
        "\n",
        "    # If current agent is different from previous agent, then we have a new subtour, otherwise we add the distance\n",
        "    current_length = torch.where(cur_agent_idx != td[\"agent_idx\"], torch.zeros_like(td[\"current_length\"]), td[\"current_length\"] + cur_dist)\n",
        "\n",
        "\n",
        "    # Set not visited to 0 (i.e., we visited the node)\n",
        "    available = td[\"action_mask\"].scatter(\n",
        "        -1, current_node.unsqueeze(-1).expand_as(td[\"action_mask\"]), 0\n",
        "    )\n",
        "    # Available[..., 0] is the depot, which is always available unless:\n",
        "    # - current_node is the depot\n",
        "    # - agent_idx greater than num_agents -1\n",
        "    available[..., 0] = torch.logical_and(current_node != 0, td['agent_idx'] < td['num_agents'] - 1)\n",
        "\n",
        "    # We are done there are no unvisited locations except the depot\n",
        "    done = torch.count_nonzero(available[..., 1:], dim=-1) == 0\n",
        "\n",
        "    # If done is True, then we make the depot available again, so that it will be selected as the next node with prob 1 (since we finished)\n",
        "    available[..., 0] = done # where done = 1, available = 1\n",
        "\n",
        "    # The reward is the negative of the max_subtour_length (minmax objective)\n",
        "    reward = - max_subtour_length\n",
        "\n",
        "    # The output must be written in a ``\"next\"`` entry\n",
        "    return TensorDict(\n",
        "        {\n",
        "            \"next\": {\n",
        "                \"observation\": td[\"observation\"],\n",
        "                \"num_agents\": td['num_agents'],\n",
        "                \"max_subtour_length\": max_subtour_length,\n",
        "                \"current_length\": current_length,\n",
        "                \"agent_idx\": cur_agent_idx,\n",
        "                \"first_node\": first_node,\n",
        "                \"current_node\": current_node,\n",
        "                \"i\": td[\"i\"] + 1,\n",
        "                \"action_mask\": available,\n",
        "                \"reward\": reward,\n",
        "                \"done\": done,\n",
        "            }\n",
        "        },\n",
        "        td.shape,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _make_spec(self, td_params: TensorDict):\n",
        "    \"\"\"Make the observation and action specs from the parameters.\"\"\"\n",
        "    self.observation_spec = CompositeSpec(\n",
        "        observation=BoundedTensorSpec(\n",
        "            minimum=self.min_loc,\n",
        "            maximum=self.max_loc,\n",
        "            shape=(self.num_loc, 2),\n",
        "            dtype=torch.float32,\n",
        "        ),\n",
        "        first_node=UnboundedDiscreteTensorSpec(\n",
        "            shape=(1),\n",
        "            dtype=torch.int64,\n",
        "        ),\n",
        "        current_node=UnboundedDiscreteTensorSpec(\n",
        "            shape=(1),\n",
        "            dtype=torch.int64,\n",
        "        ),\n",
        "        num_agents=UnboundedDiscreteTensorSpec(\n",
        "            shape=(1),\n",
        "            dtype=torch.int64,\n",
        "        ),\n",
        "        agent_idx=UnboundedDiscreteTensorSpec(\n",
        "            shape=(1),\n",
        "            dtype=torch.int64,\n",
        "        ),\n",
        "        i=UnboundedDiscreteTensorSpec(\n",
        "            shape=(1),\n",
        "            dtype=torch.int64,\n",
        "        ),\n",
        "        action_mask=UnboundedDiscreteTensorSpec(\n",
        "            shape=(self.num_loc),\n",
        "            dtype=torch.bool,\n",
        "        ),\n",
        "        shape=(),\n",
        "    )\n",
        "    self.input_spec = self.observation_spec.clone()\n",
        "    self.action_spec = BoundedTensorSpec(\n",
        "        shape=(1,),\n",
        "        dtype=torch.int64,\n",
        "        minimum=0,\n",
        "        maximum=self.num_loc,\n",
        "    )\n",
        "    self.reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n",
        "    self.done_spec = UnboundedDiscreteTensorSpec(shape=(1,), dtype=torch.bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data(self, batch_size) -> TensorDict:\n",
        "\n",
        "    # Batch size input check\n",
        "    batch_size = [batch_size] if isinstance(batch_size, int) else batch_size\n",
        "\n",
        "    # Initialize the locations (including the depot which is always the first node)\n",
        "    locs = (\n",
        "        torch.FloatTensor(*batch_size, self.num_loc, 2)\n",
        "        .uniform_(self.min_loc, self.max_loc)\n",
        "        .to(self.device)\n",
        "    )\n",
        "    \n",
        "    # Initialize the num_agents\n",
        "    num_agents = torch.randint(\n",
        "        self.min_num_agents, self.max_num_agents, size=batch_size, device=self.device\n",
        "    )\n",
        "\n",
        "    return TensorDict(\n",
        "        {\n",
        "            \"observation\": locs,\n",
        "            \"num_agents\": num_agents,\n",
        "        },\n",
        "        batch_size=batch_size,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_reward(self, td, actions) -> TensorDict:\n",
        "\n",
        "    if self.cost_type == \"minmax\":\n",
        "        # With minmax, get the maximum distance among subtours, calculated in the model\n",
        "        return td[\"reward\"]\n",
        "    \n",
        "    elif self.cost_type == \"distance\":\n",
        "        locs = td[\"observation\"]\n",
        "        # Gather locations in order of tour\n",
        "        locs = locs.gather(1, actions.unsqueeze(-1).expand_as(locs))\n",
        "\n",
        "        # Concatenate as last node the depot\n",
        "        locs = torch.cat([locs, locs[..., 0:1]], dim=-2)\n",
        "        locs_next = torch.roll(locs, 1, dims=1)\n",
        "        return  -((locs_next - locs).norm(p=2, dim=2).sum(1))\n",
        "    else:\n",
        "        raise ValueError(f\"Cost type {self.cost_type} not supported\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MTSPEnv(RL4COEnvBase):\n",
        "    name = \"mtsp\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_loc: int = 10,\n",
        "        min_loc: float = 0,\n",
        "        max_loc: float = 1,\n",
        "        min_num_agents: int = 1,\n",
        "        max_num_agents: int = 10,\n",
        "        cost_type: str = \"minmax\",\n",
        "        td_params: TensorDict = None,\n",
        "        seed: int = None,\n",
        "        device: str = \"cpu\",\n",
        "    ):\n",
        "        self.num_loc = num_loc\n",
        "        self.min_loc = min_loc\n",
        "        self.max_loc = max_loc\n",
        "        self.min_num_agents = min_num_agents\n",
        "        self.max_num_agents = max_num_agents\n",
        "        self.cost_type = cost_type\n",
        "\n",
        "        super().__init__(device=device, batch_size=[])\n",
        "        self._make_spec(td_params)\n",
        "        if seed is None:\n",
        "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
        "        self.set_seed(seed)\n",
        "\n",
        "    # Helpers: _make_step and gen_params\n",
        "    # gen_params = gen_params # we don't use it for this case. See notebook 0\n",
        "    _make_spec = _make_spec\n",
        "\n",
        "    # Mandatory methods: _step, _reset and _set_seed\n",
        "    _reset = _reset\n",
        "    _step = _step\n",
        "\n",
        "    # Get reward\n",
        "    get_reward = get_reward\n",
        "\n",
        "    # Data\n",
        "    generate_data = generate_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[0.1807, 0.7539],\n",
            "         [0.2895, 0.9399],\n",
            "         [0.6412, 0.0321],\n",
            "         [0.7584, 0.1472],\n",
            "         [0.4810, 0.3595],\n",
            "         [0.2817, 0.7075],\n",
            "         [0.2506, 0.0151],\n",
            "         [0.1024, 0.2672],\n",
            "         [0.4661, 0.3446],\n",
            "         [0.4184, 0.2753]],\n",
            "\n",
            "        [[0.9895, 0.6966],\n",
            "         [0.3829, 0.7444],\n",
            "         [0.2576, 0.9897],\n",
            "         [0.3450, 0.6744],\n",
            "         [0.5010, 0.5251],\n",
            "         [0.2894, 0.2769],\n",
            "         [0.2377, 0.8089],\n",
            "         [0.9846, 0.4740],\n",
            "         [0.1922, 0.4611],\n",
            "         [0.0252, 0.5431]]])\n",
            "tensor([8, 8])\n"
          ]
        }
      ],
      "source": [
        "env = MTSPEnv()\n",
        "\n",
        "td = env.generate_data(2)\n",
        "print(td['observation'])\n",
        "print(td['num_agents'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
