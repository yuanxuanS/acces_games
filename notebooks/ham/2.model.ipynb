{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HAAM model\n",
        "\n",
        "Heterogeneous Attentions for Solving Pickup and Delivery Problem via Deep Reinforcement Learning\n",
        "\n",
        "From:\n",
        "https://github.com/Demon0312/Heterogeneous-Attentions-PDP-DRL"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Heterogeneous Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'HeterogenousAttentionModelPolicy' from 'rl4co.models.zoo.ham.policy' (/home/botu/Dev/rl4co/notebooks/haam/../../rl4co/models/zoo/ham/policy.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m; sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m../../\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl4co\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m HeterogeneousAttentionModel\n",
            "File \u001b[0;32m~/Dev/rl4co/notebooks/haam/../../rl4co/models/__init__.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl4co\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mzoo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpomo\u001b[39;00m \u001b[39mimport\u001b[39;00m POMO, POMOPolicy\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl4co\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mzoo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msymnco\u001b[39;00m \u001b[39mimport\u001b[39;00m SymNCO, SymNCOPolicy\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl4co\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mzoo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mham\u001b[39;00m \u001b[39mimport\u001b[39;00m HeterogeneousAttentionModel, HeterogeneousAttentionModelPolicy\n",
            "File \u001b[0;32m~/Dev/rl4co/notebooks/haam/../../rl4co/models/zoo/ham/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m HeterogeneousAttentionModel\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpolicy\u001b[39;00m \u001b[39mimport\u001b[39;00m HeterogeneousAttentionModelPolicy\n",
            "File \u001b[0;32m~/Dev/rl4co/notebooks/haam/../../rl4co/models/zoo/ham/model.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl4co\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreinforce\u001b[39;00m \u001b[39mimport\u001b[39;00m WarmupBaseline, RolloutBaseline\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl4co\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlightning\u001b[39;00m \u001b[39mimport\u001b[39;00m get_lightning_device\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrl4co\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mzoo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mham\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolicy\u001b[39;00m \u001b[39mimport\u001b[39;00m HeterogenousAttentionModelPolicy\n\u001b[1;32m     11\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mHeterogeneousAttentionModel\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     12\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env, policy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, baseline\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'HeterogenousAttentionModelPolicy' from 'rl4co.models.zoo.ham.policy' (/home/botu/Dev/rl4co/notebooks/haam/../../rl4co/models/zoo/ham/policy.py)"
          ]
        }
      ],
      "source": [
        "import sys; sys.path.append(\"../../\")\n",
        "from rl4co.models import HeterogeneousAttentionModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HeterogenousMHA(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_heads,\n",
        "            input_dim,\n",
        "            embed_dim=None,\n",
        "            val_dim=None,\n",
        "            key_dim=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Heterogenous Multi-Head Attention for Pickup and Delivery problems\n",
        "        https://arxiv.org/abs/2110.02634\n",
        "        \"\"\"\n",
        "        super(HeterogenousMHA, self).__init__()\n",
        "\n",
        "        if val_dim is None:\n",
        "            assert embed_dim is not None, \"Provide either embed_dim or val_dim\"\n",
        "            val_dim = embed_dim // num_heads\n",
        "        if key_dim is None:\n",
        "            key_dim = val_dim\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.input_dim = input_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.val_dim = val_dim\n",
        "        self.key_dim = key_dim\n",
        "\n",
        "        self.norm_factor = 1 / math.sqrt(key_dim)  # See Attention is all you need\n",
        "\n",
        "        self.W_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
        "        self.W_key = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
        "        self.W_val = nn.Parameter(torch.Tensor(num_heads, input_dim, val_dim))\n",
        "\n",
        "        # Pickup weights\n",
        "        self.W1_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
        "        self.W2_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
        "        self.W3_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
        "\n",
        "        # Delivery weights\n",
        "        self.W4_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
        "        self.W5_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
        "        self.W6_query = nn.Parameter(torch.Tensor(num_heads, input_dim, key_dim))\n",
        "\n",
        "        if embed_dim is not None:\n",
        "            self.W_out = nn.Parameter(torch.Tensor(num_heads, key_dim, embed_dim))\n",
        "\n",
        "        self.init_parameters()\n",
        "\n",
        "    def init_parameters(self):\n",
        "\n",
        "        for param in self.parameters():\n",
        "            stdv = 1. / math.sqrt(param.size(-1))\n",
        "            param.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, q, h=None, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            q: queries (batch_size, n_query, input_dim)\n",
        "            h: data (batch_size, graph_size, input_dim)\n",
        "            mask: mask (batch_size, n_query, graph_size) or viewable as that (i.e. can be 2 dim if n_query == 1)\n",
        "            Mask should contain 1 if attention is not possible (i.e. mask is negative adjacency)\n",
        "        \"\"\"\n",
        "        if h is None:\n",
        "            h = q  # compute self-attention\n",
        "\n",
        "        # h should be (batch_size, graph_size, input_dim)\n",
        "        batch_size, graph_size, input_dim = h.size()\n",
        "\n",
        "        # Check if graph size is odd number\n",
        "        assert graph_size % 2 == 1, \"Graph size should have odd number of nodes due to pickup-delivery problem  \\\n",
        "                                     (n/2 pickup, n/2 delivery, 1 depot)\"\n",
        "    \n",
        "        n_query = q.size(1)\n",
        "        assert q.size(0) == batch_size\n",
        "        assert q.size(2) == input_dim\n",
        "        assert input_dim == self.input_dim, \"Wrong embedding dimension of input\"\n",
        "\n",
        "        hflat = h.contiguous().view(-1, input_dim)  # [batch_size * graph_size, embed_dim]\n",
        "        qflat = q.contiguous().view(-1, input_dim)  # [batch_size * n_query, embed_dim]\n",
        "\n",
        "        # last dimension can be different for keys and values\n",
        "        shp = (self.num_heads, batch_size, graph_size, -1)\n",
        "        shp_q = (self.num_heads, batch_size, n_query, -1)\n",
        "\n",
        "        # pickup -> its delivery attention\n",
        "        n_pick = (graph_size - 1) // 2\n",
        "        shp_delivery = (self.num_heads, batch_size, n_pick, -1)\n",
        "        shp_q_pick = (self.num_heads, batch_size, n_pick, -1)\n",
        "\n",
        "        # pickup -> all pickups attention\n",
        "        shp_allpick = (self.num_heads, batch_size, n_pick, -1)\n",
        "        shp_q_allpick = (self.num_heads, batch_size, n_pick, -1)\n",
        "\n",
        "        # pickup -> all pickups attention\n",
        "        shp_alldelivery = (self.num_heads, batch_size, n_pick, -1)\n",
        "        shp_q_alldelivery = (self.num_heads, batch_size, n_pick, -1)\n",
        "\n",
        "        # Calculate queries, (num_heads, n_query, graph_size, key/val_size)\n",
        "        Q = torch.matmul(qflat, self.W_query).view(shp_q)\n",
        "        # Calculate keys and values (num_heads, batch_size, graph_size, key/val_size)\n",
        "        K = torch.matmul(hflat, self.W_key).view(shp)\n",
        "        V = torch.matmul(hflat, self.W_val).view(shp)\n",
        "\n",
        "        # pickup -> its delivery\n",
        "        pick_flat = h[:, 1:n_pick + 1, :].contiguous().view(-1, input_dim)  # [batch_size * n_pick, embed_dim]\n",
        "        delivery_flat = h[:, n_pick + 1:, :].contiguous().view(-1, input_dim)  # [batch_size * n_pick, embed_dim]\n",
        "\n",
        "\n",
        "        # pickup -> its delivery attention\n",
        "        Q_pick = torch.matmul(pick_flat, self.W1_query).view(shp_q_pick)  # (self.num_heads, batch_size, n_pick, key_size)\n",
        "        K_delivery = torch.matmul(delivery_flat, self.W_key).view(shp_delivery)  # (self.num_heads, batch_size, n_pick, -1)\n",
        "        V_delivery = torch.matmul(delivery_flat, self.W_val).view(shp_delivery)  # (num_heads, batch_size, n_pick, key/val_size)\n",
        "\n",
        "        # pickup -> all pickups attention\n",
        "        Q_pick_allpick = torch.matmul(pick_flat, self.W2_query).view(shp_q_allpick) # (self.num_heads, batch_size, n_pick, -1)\n",
        "        K_allpick = torch.matmul(pick_flat, self.W_key).view(shp_allpick)  # [self.num_heads, batch_size, n_pick, key_size]\n",
        "        V_allpick = torch.matmul(pick_flat, self.W_val).view(shp_allpick)  # [self.num_heads, batch_size, n_pick, key_size]\n",
        "\n",
        "        # pickup -> all delivery\n",
        "        Q_pick_alldelivery = torch.matmul(pick_flat, self.W3_query).view(shp_q_alldelivery)  # (self.num_heads, batch_size, n_pick, key_size)\n",
        "        K_alldelivery = torch.matmul(delivery_flat, self.W_key).view(shp_alldelivery)  # (self.num_heads, batch_size, n_pick, -1)\n",
        "        V_alldelivery = torch.matmul(delivery_flat, self.W_val).view(shp_alldelivery)  # (num_heads, batch_size, n_pick, key/val_size)\n",
        "\n",
        "        # pickup -> its delivery\n",
        "        V_additional_delivery = torch.cat([  # [num_heads, batch_size, graph_size, key_size]\n",
        "            torch.zeros(self.num_heads, batch_size, 1, self.input_dim // self.num_heads, dtype=V.dtype, device=V.device),\n",
        "            V_delivery,  # [num_heads, batch_size, n_pick, key/val_size]\n",
        "            torch.zeros(self.num_heads, batch_size, n_pick, self.input_dim // self.num_heads, dtype=V.dtype, device=V.device)\n",
        "            ], 2)\n",
        "\n",
        "\n",
        "        # delivery -> its pickup attention\n",
        "        Q_delivery = torch.matmul(delivery_flat, self.W4_query).view(shp_delivery)  # (self.num_heads, batch_size, n_pick, key_size)\n",
        "        K_pick = torch.matmul(pick_flat, self.W_key).view(shp_q_pick)  # (self.num_heads, batch_size, n_pick, -1)\n",
        "        V_pick = torch.matmul(pick_flat, self.W_val).view(shp_q_pick)  # (num_heads, batch_size, n_pick, key/val_size)\n",
        "\n",
        "        # delivery -> all delivery attention\n",
        "        Q_delivery_alldelivery = torch.matmul(delivery_flat, self.W5_query).view(shp_alldelivery) # (self.num_heads, batch_size, n_pick, -1)\n",
        "        K_alldelivery2 = torch.matmul(delivery_flat, self.W_key).view(shp_alldelivery)  # [self.num_heads, batch_size, n_pick, key_size]\n",
        "        V_alldelivery2 = torch.matmul(delivery_flat, self.W_val).view(shp_alldelivery)  # [self.num_heads, batch_size, n_pick, key_size]\n",
        "        \n",
        "        # delivery -> all pickup\n",
        "        Q_delivery_allpickup = torch.matmul(delivery_flat, self.W6_query).view(shp_alldelivery)  # (self.num_heads, batch_size, n_pick, key_size)\n",
        "        K_allpickup2 = torch.matmul(pick_flat, self.W_key).view(shp_q_alldelivery)  # (self.num_heads, batch_size, n_pick, -1)\n",
        "        V_allpickup2 = torch.matmul(pick_flat, self.W_val).view(shp_q_alldelivery)  # (num_heads, batch_size, n_pick, key/val_size)\n",
        "\n",
        "        # delivery -> its pick up     \n",
        "        V_additional_pick = torch.cat([  # [num_heads, batch_size, graph_size, key_size]\n",
        "            torch.zeros(self.num_heads, batch_size, 1, self.input_dim // self.num_heads, dtype=V.dtype, device=V.device),\n",
        "            torch.zeros(self.num_heads, batch_size, n_pick, self.input_dim // self.num_heads, dtype=V.dtype, device=V.device),\n",
        "            V_pick  # [num_heads, batch_size, n_pick, key/val_size]\n",
        "            ], 2)  \n",
        "\n",
        "        # Calculate compatibility (num_heads, batch_size, n_query, graph_size)\n",
        "        compatibility = self.norm_factor * torch.matmul(Q, K.transpose(2, 3))\n",
        "\n",
        "        ##Pick up pair attention\n",
        "        compatibility_pick_delivery = self.norm_factor * torch.sum(Q_pick * K_delivery, -1)  # element_wise, [num_heads, batch_size, n_pick]\n",
        "        # [num_heads, batch_size, n_pick, n_pick]\n",
        "        compatibility_pick_allpick = self.norm_factor * torch.matmul(Q_pick_allpick, K_allpick.transpose(2, 3))# [num_heads, batch_size, n_pick, n_pick]\n",
        "        compatibility_pick_alldelivery = self.norm_factor * torch.matmul(Q_pick_alldelivery, K_alldelivery.transpose(2, 3))  # [num_heads, batch_size, n_pick, n_pick]\n",
        "        \n",
        "        ##Delivery\n",
        "        compatibility_delivery_pick = self.norm_factor * torch.sum(Q_delivery * K_pick, -1)  # element_wise, [num_heads, batch_size, n_pick]\n",
        "        compatibility_delivery_alldelivery = self.norm_factor * torch.matmul(Q_delivery_alldelivery, K_alldelivery2.transpose(2, 3))# [num_heads, batch_size, n_pick, n_pick]\n",
        "        compatibility_delivery_allpick = self.norm_factor * torch.matmul(Q_delivery_allpickup, K_allpickup2.transpose(2, 3))  # [num_heads, batch_size, n_pick, n_pick]\n",
        "        \n",
        "        ##Pick up->\n",
        "        # compatibility_additional?pickup????delivery????attention(size 1),1:n_pick+1??attention,depot?delivery??\n",
        "        compatibility_additional_delivery = torch.cat([  # [num_heads, batch_size, graph_size, 1]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, 1, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            compatibility_pick_delivery,  # [num_heads, batch_size, n_pick]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, n_pick, dtype=compatibility.dtype, device=compatibility.device)\n",
        "            ], -1).view(self.num_heads, batch_size, graph_size, 1)\n",
        "\n",
        "        compatibility_additional_allpick = torch.cat([  # [num_heads, batch_size, graph_size, n_pick]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, 1, n_pick, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            compatibility_pick_allpick,  # [num_heads, batch_size, n_pick, n_pick]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, n_pick, n_pick, dtype=compatibility.dtype, device=compatibility.device)\n",
        "            ], 2).view(self.num_heads, batch_size, graph_size, n_pick)\n",
        "\n",
        "        compatibility_additional_alldelivery = torch.cat([  # [num_heads, batch_size, graph_size, n_pick]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, 1, n_pick, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            compatibility_pick_alldelivery,  # [num_heads, batch_size, n_pick, n_pick]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, n_pick, n_pick, dtype=compatibility.dtype, device=compatibility.device)\n",
        "        ], 2).view(self.num_heads, batch_size, graph_size, n_pick)\n",
        "        # [num_heads, batch_size, n_query, graph_size+1+n_pick+n_pick]\n",
        "\n",
        "        # Delivery\n",
        "        compatibility_additional_pick = torch.cat([  # [num_heads, batch_size, graph_size, 1]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, 1, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, n_pick, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            compatibility_delivery_pick  # [num_heads, batch_size, n_pick]\n",
        "            ], -1).view(self.num_heads, batch_size, graph_size, 1)        \n",
        "        \n",
        "        compatibility_additional_alldelivery2 = torch.cat([  # [num_heads, batch_size, graph_size, n_pick]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, 1, n_pick, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, n_pick, n_pick, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            compatibility_delivery_alldelivery  # [num_heads, batch_size, n_pick, n_pick]\n",
        "            ], 2).view(self.num_heads, batch_size, graph_size, n_pick)    \n",
        "    \n",
        "        compatibility_additional_allpick2 = torch.cat([  # [num_heads, batch_size, graph_size, n_pick]\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, 1, n_pick, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            float('-inf') * torch.ones(self.num_heads, batch_size, n_pick, n_pick, dtype=compatibility.dtype, device=compatibility.device),\n",
        "            compatibility_delivery_allpick  # [num_heads, batch_size, n_pick, n_pick]\n",
        "        ], 2).view(self.num_heads, batch_size, graph_size, n_pick)     \n",
        "    \n",
        "        compatibility = torch.cat([compatibility, compatibility_additional_delivery, compatibility_additional_allpick, compatibility_additional_alldelivery,\n",
        "                                   compatibility_additional_pick, compatibility_additional_alldelivery2, compatibility_additional_allpick2], dim=-1)\n",
        "\n",
        "        # Optionally apply mask to prevent attention\n",
        "        if mask is not None:\n",
        "            mask = mask.view(1, batch_size, n_query, graph_size).expand_as(compatibility)\n",
        "            compatibility[mask] = float('-inf')\n",
        "\n",
        "        attn = torch.softmax(compatibility, dim=-1)  # [num_heads, batch_size, n_query, graph_size+1+n_pick*2] (graph_size include depot)\n",
        "\n",
        "        # If there are nodes with no neighbours then softmax returns nan so we fix them to 0\n",
        "        if mask is not None:\n",
        "            attnc = attn.clone()\n",
        "            attnc[mask] = 0\n",
        "            attn = attnc\n",
        "\n",
        "        # heads: [num_heads, batrch_size, n_query, val_size] pick -> its delivery\n",
        "        heads = torch.matmul(attn[:, :, :, :graph_size], V)  # V: (self.num_heads, batch_size, graph_size, val_size)\n",
        "        heads = heads + attn[:, :, :, graph_size].view(self.num_heads, batch_size, graph_size, 1) * V_additional_delivery  # V_addi:[num_heads, batch_size, graph_size, key_size]\n",
        "\n",
        "        # Heads pick -> otherpick, V_allpick: # [num_heads, batch_size, n_pick, key_size]\n",
        "        heads = heads + torch.matmul(attn[:, :, :, graph_size+1:graph_size+1+n_pick].view(self.num_heads, batch_size, graph_size, n_pick), V_allpick)\n",
        "\n",
        "        # V_alldelivery: # (num_heads, batch_size, n_pick, key/val_size)\n",
        "        heads = heads + torch.matmul(attn[:, :, :, graph_size+1+n_pick :graph_size+1 + 2 * n_pick].view(self.num_heads, batch_size, graph_size, n_pick), V_alldelivery)\n",
        "\n",
        "        # Delivery\n",
        "        heads = heads + attn[:, :, :, graph_size+1 + 2 * n_pick].view(self.num_heads, batch_size, graph_size, 1) * V_additional_pick \n",
        "        heads = heads + torch.matmul(attn[:, :, :, graph_size+1 + 2 * n_pick+1:graph_size+1 + 3 * n_pick+1].view(self.num_heads, batch_size, graph_size, n_pick), V_alldelivery2)\n",
        "        heads = heads + torch.matmul(attn[:, :, :, graph_size+1 + 3 * n_pick+1:].view(self.num_heads, batch_size, graph_size, n_pick), V_allpickup2)\n",
        "\n",
        "        out = torch.mm(\n",
        "            heads.permute(1, 2, 0, 3).contiguous().view(-1, self.num_heads * self.val_dim),\n",
        "            self.W_out.view(-1, self.embed_dim)\n",
        "        ).view(batch_size, n_query, self.embed_dim)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 21, 128])\n"
          ]
        }
      ],
      "source": [
        "#(batch_size, graph_size, embed_dim)\n",
        "batch_size = 64\n",
        "graph_size = 21\n",
        "embed_dim = 128\n",
        "\n",
        "x = torch.randn(batch_size, graph_size, embed_dim)\n",
        "\n",
        "mha = HeterogenousMHA(8, embed_dim, embed_dim)\n",
        "\n",
        "out = mha(x)\n",
        "print(out.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "fbc5b198709957cb10390a2819ca930d3578f48e335d60395e01c5208a66cb86"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
