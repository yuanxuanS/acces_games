# @package _global_

defaults:
  - override /model_adversary: am-ppo.yaml
  - override /env: acsp.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /logger: wandb.yaml

env:
  num_loc: 50

logger:
  wandb:
    project: "rl4co"
    tags: ["am-ppo", "${env.name}"]
    group: ${env.name}${env.num_loc}
    name: ppo-adv-${env.name}${env.num_loc}
    offline: true
prog_pth:  /home/panpan/acces_games/logs/train/runs/acsp50/am-acsp50/2024-05-22_19-57-54/rl4co/c1b28edi/checkpoints/epoch=0-step=20.ckpt
model_adversary:
  opponent_type: "rl"
  batch_size: 512
  val_batch_size: 512
  test_batch_size: 100
  train_data_size: 10_000
  val_data_size: 10_000
  test_data_size: 100
  clip_range: 0.2
  ppo_epochs: 2
  mini_batch_size: 512
  vf_lambda: 0.5
  entropy_lambda: 0.0
  normalize_adv: False
  max_grad_norm: 0.5
  optimizer_kwargs:
    lr: 5e-5
    weight_decay: 0
  lr_scheduler:
    "MultiStepLR"
  lr_scheduler_kwargs:
    milestones: [150, 175]
    gamma: 0.1
  data_dir: "data${paths.stoch_idx}/"

model:
  batch_size: ${model_adversary.batch_size}
  val_batch_size: ${model_adversary.val_batch_size}
  test_batch_size: ${model_adversary.test_batch_size}
  train_data_size: ${model_adversary.train_data_size}
  val_data_size: ${model_adversary.val_data_size}
  test_data_size: ${model_adversary.test_data_size}

trainer:
  max_epochs: 1
  gradient_clip_val: Null # not supported in manual optimization
  precision: "32-true" # NOTE: this seems to be important during manual optimization

seed: 1234

metrics:
  train: ["loss", "reward", "surrogate_loss", "value_loss", "entropy_bonus"]
