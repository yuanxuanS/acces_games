# @package _global_

defaults:
  
  - override /env: svrp.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /logger: wandb.yaml  # csv.yaml  #
  

# psro params
iters: 20
epsilon: 0.01
prog_epoch1: 5
prog_epoch2: 5
prog_epoch3: 5
adver_epoch: 5

eval_baseline: True
baseline_method: "cw"


env:
  num_loc: 20

logger:
  wandb:
    project: "rl4co-psro"
    tags: ["adv-am", "${env.name}"]
    group: ${env.name}${env.num_loc}
    name: am-${env.name}${env.num_loc}
    offline: True


model_psro:
  batch_size:  10 #
  val_batch_size: 100  #10 #
  test_batch_size: 100 #10   #
  train_data_size: 100  #
  val_data_size: 10_000  #
  test_data_size: 100  #100 #
  data_dir: "data${paths.stoch_idx}/"

  # policy_kwargs:
  #   test_decode_type: "multistart_sampling"

model:
  optimizer_kwargs:
    lr: 1e-4
    weight_decay: 0
  lr_scheduler:
    "MultiStepLR"
  lr_scheduler_kwargs:
    milestones: [1] # 200相当于不降低
    gamma: 0.1
  data_dir: "data${paths.stoch_idx}/"
  # policy_kwargs:
  #   test_decode_type: "multistart_sampling"

load_prog_from_path: "/home/panpan/rl4co/logs/train/runs/svrp20/am-svrp20/2024-05-16_11-37-16/rl4co/x80zwa1a/checkpoints/epoch=99-step=62500.ckpt"

train_with_pretrain: null
  # "/home/panpan/rl4co/logs/train_rarl/runs/svrp20/am-svrp20/2024-01-11_22-28-55/csv/version_0/checkpoints/epoch=106-step=627500.ckpt"

model_adversary:
  opponent: null
  optimizer_kwargs:
    lr: 1e-3
    weight_decay: 0
  lr_scheduler:
    "MultiStepLR"
  lr_scheduler_kwargs:
    milestones: [1]
    gamma: 0.1
  data_dir: "data${paths.stoch_idx}/"
  # policy_kwargs:
  #   test_decode_type: "multistart_sampling"

# trainer:
#   max_epochs:  1 #

seed: 1234

evaluate_method: "greedy"   #["greedy", "sampling", "greedy_multistart", "augment_dihedral_8", "augment", "greedy_multistart_augment_dihedral_8", "greedy_multistart_augment"]